{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Содержание**<a id='toc0_'></a>    \n",
    "- [Преобразование последовательностей (seq2seq)](#toc1_)    \n",
    "  - [Основные архитектуры](#toc1_1_)    \n",
    "    - [Сверточные архитектуры](#toc1_1_1_)    \n",
    "    - [Трансформеры](#toc1_1_2_)    \n",
    "    - [ RNN based Neural Machine Translation / RNMT+](#toc1_1_3_)    \n",
    "    - [Что лучше](#toc1_1_4_)    \n",
    "  - [Декодированые выходной последовательности](#toc1_2_)    \n",
    "      - [Полностью жадное решение задачи поиска максимума вероятности](#toc1_2_1_1_)    \n",
    "  - [Проблема низкого развнообразия генерации](#toc1_3_)    \n",
    "      - [**Замена функции потерь**](#toc1_3_1_1_)    \n",
    "- [Теоретические вопросы](#toc2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Преобразование последовательностей (seq2seq)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что значит \"метод [seq2seq](https://en.wikipedia.org/wiki/Seq2seq)\" (сокращение \"sequence to sequence\"). \n",
    "\n",
    "**На вход** подают список номеров токенов (результат токенизации исходного текста): $[t_1, ..., t_l]$ \n",
    "\n",
    "**На выходе** генерируем новый список номеров токенов, по которым можно будет декодировать новый результирующий текст. Таким образом, задача \"sequence to sequence\" — это задача генерации одного текста на основе другого текста: $[y_1, ..., y_{l_y}]$\n",
    "\n",
    "К таким задачам, например, относится:\n",
    "- машинный перевод\n",
    "- диалоговые системы (генерация реплик)\n",
    "- суммаизация (генерация аннотаций)\n",
    "- анализ структуры (можно на seq2seq сделать POS-tagging, но не нужно!)\n",
    "\n",
    "**Обучающая выборка** \n",
    "\n",
    "- Cостоит из пар (входной - выходной тексты):\n",
    "\n",
    "    Мама мыла раму -> Mom washed the windows  \n",
    "    Потом мы пошли гулять -> Then we went for a walk\n",
    "\n",
    "- **Токенизация** (BPE, wordpieces):\n",
    "\n",
    "$$x_{i_1,1}, x_{i_1,2}, x_{i_1,3}, ... \\rightarrow y_{i_1,1},y_{i_1,2},y_{i_1,3},y_{i_1,4},.... \\\\ \n",
    "x_{i_2,1}, x_{i_2,2}, x_{i_2,3}, ... \\rightarrow y_{i_2,1},y_{i_2,2},y_{i_2,3},y_{i_2,4},....$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общий алгоритм**\n",
    "\n",
    "В каждой seq2seq модели есть две самые главные части — это энкодер (encoder) и декодер (decoder)\n",
    "\n",
    "1. Закодировать входную последовательность (энкодер)\n",
    "- преобразует последовательность в один или несколько векторов\n",
    "2. Генерировать выходную последовательность слово за словом (декодер)\n",
    "- принимает в качестве входа результат работы энкодера и, на каждом шаге, выдаёт распределение вероятностей очередного токена при условии всех предыдущих\n",
    "- распределение первого токена обусловлено только на входную последовательность, а все последующие — уже и на первый токен\n",
    "  - $P(y_1|x_1, ...x_n)$\n",
    "  - $P(y_2|x_1, ...x_n, y_1)$\n",
    "  - ...\n",
    "- процесс генерации продолжается до тех пор, пока декодер не сгенерирует специальный токен, обозначающий конец последовательности `<EOS>`\n",
    "\n",
    "Такой процесс генерации называется \"**авторегрессионным**\".\n",
    "\n",
    "[1] Sutskever I., Vinyals O., Le Q. V. Sequence to sequence learning with neural networks //Advances in neural information processing systems. – 2014. – С. 3104-3112."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Основные архитектуры](#toc0_)\n",
    "\n",
    "### <a id='toc1_1_1_'></a>[Сверточные архитектуры](#toc0_)\n",
    "\n",
    "- В фейсбуке предложили использовать архитектуру, состоящую целиком из свёрток[1,2] — свёрточные модули хорошо распараллеливаются, так как зависимости по данным — локальны и укладываются в ширину ядра свёртки. \n",
    "- Свертки инварианты к порядку в последовательности, поэтому применяется позиционное кодирование последовательностей (эмбеддинг позиций)\n",
    "- Учет зависимости между токенами длины $n$ свертками с ядром $k$ требуется $O(n/k)$ сверточных слоев \n",
    "  - Если с прореживанием/dilation, то $O(\\log n)$, т.е. все равно по памяти значительно хуже RNN\n",
    "\n",
    "  **Преимущества**\n",
    "\n",
    "  1. Такие архитектуры обладают всеми преимуществами архитектуры [Google Machine Translation](https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation) (она примерно так же устроена):\n",
    "    - они достаточно выразительные, мощные, и на ряде задач они дают лучшее качество, чем seq2seq модели, основанные на рекуррентках\n",
    "  2. В энкодере используется двунаправленный контекст, то есть, для вычисления вектора слова на каком-то уровне, используются как соседи слева, так и соседи справа \n",
    "  3.  Свёрточные нейросети гораздо эффективнее с вычислительной точки зрения, то есть, они лучше заполняют вычислительные мощности видеокарт.\n",
    "\n",
    "  **Недостатки**\n",
    "\n",
    "  Дорого учитывать длинные зависимости\n",
    "  - а в RNN не дорого n ячеек памяти?\n",
    "\n",
    "  Низкое разнообразие результирующих текстов\n",
    "\n",
    "### <a id='toc1_1_2_'></a>[Трансформеры](#toc0_)\n",
    "Следующее логичное развитие — это использовать трансформер, состоящий целиком из **механизмов внутреннего внимания**.[3] \n",
    "\n",
    "Эта архитектура также состоит из двух частей — энкодера и декодера. \n",
    "- В энкодере используется полный механизм внимания, учитывающий все пары элементов последовательности. \n",
    "- В декодере у нас ситуация особая — когда мы генерируем очередное слово, мы не видим ничего справа. \n",
    "  - Мы можем опираться только на часть сгенерированной последовательности слева от текущего слова. \n",
    "  - Для того, чтобы в режиме обучения игнорировать часть последовательности справа от текущего слова, используется так называемый \"механизм внутреннего внимания с маской\". \n",
    "  - Маска применяется перед софтмаксами, внутри механизма внимания, и приводит к тому, что некоторые позиции получают нулевой вес и не участвуют в вычислении результирующих векторов. \n",
    "- Так же, как и в полносвёрточных архитектурах, здесь необходимо позиционное кодирование для того, чтобы сообщить нейросети информацию о том, где, относительно начала текста, находится каждое входное слово. \n",
    "  - На практике, часто используется синусоидальный сигнал, который складывается, добавляется к эмбеддингу токена. \n",
    "- Как мы уже говорили в лекции про трансформер и про механизмы внимания, стоимость обработки зависимостей любой длины — константна $O(1)$ и не зависит от расстояния между элементами последовательностей, между которыми зависимость существует.\n",
    "\n",
    "Т.о. в задачах seq2seq механизм внимания:\n",
    "- усиливает поток информации между энкодером и декодером (больше информации о входном тексте доходит до декодера и генерируемый текст сильнее зависит от входного текста).\n",
    "- позволяет учитывать далёкие зависимости.\n",
    "- позволяет избежать \"забывания\" начала входного текста при использовании рекуррентного энкодера.\n",
    "\n",
    "**Преимущества**\n",
    "\n",
    "- отлично распараллеливается\n",
    "- учитывается сразу весь контекст любой длины\n",
    "\n",
    "**Недостатки**\n",
    "- сложный алгоритм обучения (?)\n",
    "\n",
    "### <a id='toc1_1_3_'></a>[ RNN based Neural Machine Translation / RNMT+](#toc0_)\n",
    "\n",
    "Тоже на RNN, по сути - многослойная LSTM. Тоже норм.\n",
    "\n",
    "### <a id='toc1_1_4_'></a>[Что лучше](#toc0_)\n",
    "\n",
    "Как обычно, где как. Например, в энторнете есть таблички где кто-то сравнивают разные варианты, и у них получилось, что лучшее качество достигается когда энкодер это трансформер, а декодер RNMT+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] [Gehring J. et al. Convolutional sequence to sequence learning //Proceedings of the 34th International Conference on Machine Learning-Volume 70. – JMLR. org, 2017. – С. 1243-1252.](https://arxiv.org/pdf/1705.03122.pdf)  \n",
    "[2] Gehring J. et al. A convolutional encoder model for neural machine translation //arXiv preprint arXiv:1611.02344. – 2016.  \n",
    "[3] [Vaswani, Ashish, et al. Attention is all you need. Advances in neural information processing systems. 2017.](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Декодированые выходной последовательности](#toc0_)\n",
    "\n",
    "В чем проблема?\n",
    "\n",
    "- модель предсказывается вероятности токенов\n",
    "  - $P(y_1|x_1, ...x_n)$\n",
    "  - $P(y_2|x_1, ...x_n, y_1)$\n",
    "  - ...\n",
    "- результатом работы декодера должно стать совместное распределение результирующих токенов (для выбор максимума вероятности)\n",
    "$$P(y_{1}, y_{2}, ..., y_{m} | x_{1}, x_{2}, ..., x_{n}) = \\prod_{j=1}^m P(y_{j} | x_{1}, x_{2}, ..., x_{n}, y_{1}, ..., y_{(j-1)})$$\n",
    "- поэтому возможны варианты, как из этого распределения получать последовательности токенов (непосредственно решать **задачу генерации**):\n",
    "  - семплирование из полученного распределения: $y_{1},y_{2},...,y_{m} \\sim P(y_{1},y_{2},...,y_{m}|x_{1},x_{2},...,x_{n})$\n",
    "  - оптимизация, поиск максимума вероятности: $y_{1},y_{2},...,y_{m} = \\argmax_{y_{i}} P(y_{1}, y_{2}, ..., y_{m} | x_{1}, x_{2}, ..., x_{n})$\n",
    "\n",
    "Т.е. имеем проблемы \n",
    "1. **баланс правдоподобия и разнообразия** (максимум вероятности / семпл из распределения)\n",
    "2. задача поиска максимума по всем токенам — **[NP-полная](https://ru.wikipedia.org/wiki/NP-%D0%BF%D0%BE%D0%BB%D0%BD%D0%B0%D1%8F_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B0)**, т.е. нерешаема без полного перебора, поэтому её не представляется возможным решить за разумное время\n",
    "\n",
    "#### <a id='toc1_2_1_1_'></a>[Полностью жадное решение задачи поиска максимума вероятности](#toc0_)\n",
    "\n",
    "Можно применять полностью жадное решение[2] - это такое упрощение исходной задачи, когда на каждом шаге всегда выбирается наиболее правдоподобный токен, идя последовательно слева направо:\n",
    "\n",
    "Выбираеются моды последовательности распределений:\n",
    "$$ \\hat y_{1} = \\argmax_{y_{1}} P(y_{1}| x_{1}, x_{2}, ..., x_{m}) \\\\\n",
    "\\hat y_{2} = \\argmax_{y_{2}} P(y_{2}| x_{1}, x_{2}, ..., x_{m}, \\hat y_1) \\\\\n",
    "\\hat y_{3} = \\argmax_{y_{3}} P(y_{3}| x_{1}, x_{2}, ..., x_{m}, \\hat y_1, \\hat y_2) \\\\\n",
    "...$$\n",
    "\n",
    "Полностью жадное решение требует $O(m - |Vocal|)$ операций.\n",
    "\n",
    "Если модель хорошо обучилась, даже скажем прямо, переучилась, то такой алгоритм дает хорошие результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "\n",
    "Умеренно жадный алгоритм[2] категории \"best first алгоритмов\" поиска в графах. Направлен на поиск не оптимальных, но достаточно хороших наборов решений, и обычно дает баланс правдоподобия и разнообразия.\n",
    "\n",
    "**Вход** - оценка условного распределния: $P(y_{i}| x_{1}, x_{2}, ..., x_{m}, y_{1}, ..., y_{i-1})$.\n",
    "\n",
    "Нам нужна некоторая функция, которая будет оценивать качество наших последовательностей. Мы можем использовать (как раз) наш декодер для того, чтобы оценивать правдоподобие очередного токена, при условии наблюдения какой-то части последовательности.\n",
    "\n",
    "**Выход** - последовательность токенов: $[\\hat y_1, \\hat y_2, ...\\hat y_m]$\n",
    "\n",
    "Это не будет конечно наилучшей оценку в смысле построенной модели, но уж точно не хуже полностью жадного алгоритма.\n",
    "\n",
    "**Гиперпараметры алгоритма**\n",
    "\n",
    "b - ширина луча (beamsize), порядка 10  \n",
    "M - максимальная длина решения\n",
    "\n",
    "**Алгоритм**\n",
    "\n",
    "1. Инициализация списка частных решений (пучок лучей) и списка полных решений (в луче $b$ токенов, с которых вообще может начаться последовательность)\n",
    "\n",
    "$$ Beam = \\{ |\\hat y_{i, 1}|\\}, i \\in \\overline{1,b} \\\\ \n",
    "\\hat y_{i,1} = \\argmax_{y_{1}} P(y_{1}| x_{1}, x_{2}, ..., x_{m}) \\\\\n",
    "Result = \\emptyset $$\n",
    "\n",
    "2. Для каждого частного решения $[\\hat y_{i, 1},  .. \\hat y_{i, m_i}] \\in Beam$\n",
    "   - для каждого токена $y_k \\in Vocab$\n",
    "     - построить продолжение частного решения $[\\hat y_{i, 1},  .. \\hat y_{i, m_i}, y_k]$\n",
    "     - оценить его правдоподобие $P(\\hat y_{i, 1},  .. \\hat y_{i, m_i}, y_k| x_{1}, x_{2}, ..., x_{m})$\n",
    "     - добавить его в луч\n",
    "     - если $y_k = <EOS>$, то добавить это частное решение в $Result$\n",
    "\n",
    "3. Удалить из луча все частные решения, кроме $b$ самых правдоподобных\n",
    "4. Повтор 2-3 пока $Bean \\neq \\emptyset$ или в луче не останется частных решений, короче или равные по длине $M$\n",
    "5. Выбрать из $Result$ наилучшее решение или несколько лучших\n",
    "\n",
    "**Недостаток**\n",
    "\n",
    "- предпочитает короткие решения, т.к. чем длинее последовательность, тем больше множителей (вероятностей) в оценке правдоподобия, и оценка снижается\n",
    "- нужно добавлять \"антиштраф\" за длинные последовательности, занижать правдоподобие коротких решений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] [Chen, Mia Xu, et al. The best of both worlds: Combining recent advances in neural machine translation. arXiv preprint arXiv:1804.09849 (2018).](https://arxiv.org/abs/1804.09849)  \n",
    "[2] Жадное декодирование и декодирование через лучевой поиск (beam-search) рассматривалось в семинаре про трансформер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Проблема низкого развнообразия генерации](#toc0_)\n",
    "\n",
    "Низкое разнообразие — это когда в разных ситуациях генерируется один и тот же текст. Есть множество гипотез касательно того, почему это может происходить. К основным можно отнести две:\n",
    "\n",
    "- **слабая обусловленность на вход**, то есть связь между энкодером и декодером — слабая\n",
    "  - т.е. **генератор текста работает в режиме языковой модели**\n",
    "  - считается, что использование **механизма внимания** частично решает проблему\n",
    "  \n",
    "- **чрезмерная уверенность декодера** (английский термин: \"over-confidence\").[1]\n",
    "  - т.е. предсказываемые вероятности токенов последовательности либо близки к 0, либо близки к 1\n",
    "  - тогда попадание 0 токенов в луч обнуляет правдоподобие и последовательность отбрасывается\n",
    "  - возможный способ борьбы с чрезмерной уверенностью — это **замена функции потерь**\n",
    "\n",
    "#### <a id='toc1_3_1_1_'></a>[**Замена функции потерь**](#toc0_)\n",
    "\n",
    "Исходная функция потерь для токена $y_i$ - категориальная кросс-энтропия:\n",
    "$$CE(y_i, y_y^{GT}) = -\\log NNet (x_1, ..., x_m, y_1, ...,y_{i-1})_{y_i^{GT}} = \\\\\n",
    "= - \\sum_{k=1}^{|Vocab|} [k = y_i^{GT}] \\log NNet(x_1, ..., x_m, y_1, ...,y_{i-1})$$\n",
    "\n",
    "$[k = y_i^{GT}]$ - индикаторная функция, равна 1, если k совпадает с номером истинного токена (таргета), иначе 0\n",
    "\n",
    "1. **Сглаживание меток классов** (label smoothing[2])\n",
    "\n",
    "$$SmoothedCE(y_i, y_y^{GT}) = - \\sum_{k=1}^{|Vocab|} W(k, y_i^{GT}) \\log NNet(x_1, ..., x_m, y_1, ...,y_{i-1}) \\\\ \n",
    "W(k, y_i^{GT}) = \\begin{cases} 1 - \\beta & k = y^{GT} \\\\ \\beta \\cdot P(y^{GT}) & k \\neq y^{GT} \\end{cases}$$\n",
    "\n",
    "$W(k, y_i^{GT})$ - мягкая индикаторная функция, $\\beta \\sim 0.1$. Т.е. для правильного токена вес будет браться \"почти\" 1, а для остальных - пропорционально априорной вероятности правильного токена в тексте.\n",
    "\n",
    "2. **Энтропийная регуляризация** - штраф за контрастность\n",
    "\n",
    "$$ loss(y_i, y_y^{GT}) = CE(y_i, y_y^{GT}) - \\beta H (NNet (x_1, ..., x_m, y_1, ...,y_{i-1})_{y_i^{GT}}) \\\\\n",
    "H(P) = -\\sum_{i=1}^K p_i \\log p_i$$\n",
    "\n",
    "- энтропия максимальна, когда распределение равномерное, а минимальна — когда распределение вырождено\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Семинар в директории `./stepik-dl-nlp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Теоретические вопросы](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Алгоритм машинного перевода переводит текст с английского языка на русский. Размер английского словаря равен 5000 токенов, русского - 8000. Оцените, сколько последовательностей рассмотрит декодер при генерации перевода наибольшей длины 5 при условии, что при декодировании используется алгоритм полного перебора?\n",
    "\n",
    "- Число размещений **с повторениями** из 8000 по 5: $A_n^k = n^k = 8000^5 = 32768000000000000000$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Алгоритм машинного перевода работает с переводом текста с английского языка на русский. Размер английского словаря равен 5000 токенов, русского - 8000. Оцените, сколько последовательностей сгенерирует декодер при переводе предложения длины 5 при условии, что используется лучевой поиск (beam search) при b=3 (b - ширина луча)? \n",
    "\n",
    "- Дерево высоты 5 и по 3 ветви в узлах, сколько элементов на последнем уровне? $3^0, 3^1, ... , 3^4, 3^5$, причем нулевой уровень, это еше не дерево по данному алгоритму"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Пусть у нас есть предложение, состоящее из произвольных цифр. Например, '1576429830'. Посчитайте **перплексию** этого предложения относительно модели, которая считает, что вероятность встретить каждую цифру равна 0.1.\n",
    "Формула перплексии:\n",
    "\n",
    "$$PP(W) = P(w_1, w_2, .., w_N)^{-\\frac{1}{N}} = \\sqrt[N]{\\dfrac{1}{P(w_1, w_2, .., w_N)}} = \\sqrt[N]{\\dfrac{1}{\\prod_{i=1}^N P(w_i | w_1, .., w_{i-1})}} = \\sqrt[10] {1 / 0.1^{10}}= 10^1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **BLEU (bilingual evaluation understudy)** - метрика для оценивания качества машинного перевода, основанная на сравнении перевода, предложенного алгоритмом, и референсного перевода (ground truth). Сравнение производится на основе подсчета n-грамм (n меняется от 1 до некоторого порога, например, 4), которые встретились и в предложенном переводе, и в референсном (ground truth). После подсчета совстречаемости n-грамм полученная метрика умножается на так называемый **brevity penalty** - штраф за слишком короткие варианты перевода. Brevity penalty считается как <количество слов в переводе, предложенном алгоритмом> / <количество слов в референсном переводе>.\n",
    "\n",
    "Формула:\n",
    "\n",
    "$$BLEU = \\text{brevity penalty} \\cdot \\left (\\prod_{i=1}^n \\text{precision}_i \\right)^{1/n} \\cdot 100\\% = (6/8*4/7*2/6)**(1/3) * 100$$\n",
    "где $\\text{brevity penalty} = min \\left(1, \\dfrac{\\text{output length}}{\\text{reference length}} \\right)$\n",
    "\n",
    "- Перевод, предложенный алгоритмом: \"Кошка вышла из дома и села на крыльцо\"\n",
    "- Референсный перевод (ground truth): \"Кошка вышла из комнаты и села на ступеньки\"\n",
    "- $BLEU =  = 1 * (6/8*4/7*2/6)^{1/3} * 100 \\% = 52 \\%$\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75291dc0307ea48294888123147845d2e15abd18d38848ca6ac05a6fe8c88425"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
