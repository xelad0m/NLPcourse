{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Содержание**<a id='toc0_'></a>    \n",
    "- [Векторная модель текста и классификация длинных текстов](#toc1_)    \n",
    "  - [Разреженные векторные модели](#toc1_1_)    \n",
    "  - [Закон Ципфа](#toc1_2_)    \n",
    "  - [TF-IDF](#toc1_3_)    \n",
    "  - [Точечная взаимная информация (pointwise mutual information, PMI)](#toc1_4_)    \n",
    "- [Создаём нейросеть для работы с текстом](#toc2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Векторная модель текста и классификация длинных текстов](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ве́кторная моде́ль (англ. vector space model) — в информационном поиске представление коллекции документов векторами из одного общего для всей коллекции векторного пространства.\n",
    "\n",
    "Документ в векторной модели рассматривается как неупорядоченное множество термов. Термами в информационном поиске называют слова, из которых состоит текст, а также такие элементы текста, как, например, *2010, II-5* или *Тянь-Шань*.\n",
    "\n",
    "Более формально\n",
    "\n",
    "$$d_j = (w_{1j}, w_{2j}, …, w_{nj})$$\n",
    "где $d_j$ — векторное представление j-го документа, $w_{ij}$ — вес i-го терма в j-м документе, n — общее количество различных термов во всех документах коллекции.\n",
    "\n",
    "*Косинусное сходство* — это мера сходства между двумя векторами предгильбертового пространства, которая используется для измерения косинуса угла между ними.\n",
    "\n",
    "Если даны два вектора признаков, A и B, то косинусное сходство, $cos(θ)$, может быть представлено используя скалярное произведение и норму:\n",
    "\n",
    "$${\\displaystyle {\\text{similarity}}=\\cos(\\theta )={A\\cdot B \\over \\|A\\|\\|B\\|}={\\frac {\\sum \\limits _{i=1}^{n}{A_{i}\\times B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{(A_{i})^{2}}}}\\times {\\sqrt {\\sum \\limits _{i=1}^{n}{(B_{i})^{2}}}}}}}$$\n",
    "\n",
    "В случае информационного поиска, косинусное сходство двух документов изменяется в диапазоне от 0 до 1, поскольку частота терма (веса tf-idf) не может быть отрицательной. Угол между двумя векторами частоты терма не может быть больше, чем 90°."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Разреженные векторные модели](#toc0_)\n",
    "Ещё их называют \"методом мешка слов\". \n",
    "\n",
    "Они хорошо работают, когда класс документа соответствует его тематике. Как правило, тематика документа хорошо описывается составом словаря, который используется в этом документе, а также частотами слов, а не тем, как именно они употребляются в документе, не структурой фраз. \n",
    "\n",
    "Тогда каждый документ описывается длинным вектором размерности порядка десятков или сотен тысяч элементов. Большая часть — нули. Для каждого слова в документе мы имеем какое-то вещественное число. \n",
    "\n",
    "Ранее мы уже выяснили, что модели должны работать лучше, когда веса слов отличаются — чем значимее слово, тем больше его вес. \n",
    "\n",
    "Однако, как именно считать эти веса? \n",
    "\n",
    "**Простейший вариант**\n",
    "- взвешивать слова по количеству их употреблений в документе.[1,2] \n",
    "\n",
    "Элементарно — здесь мы видим несколько наиболее частотных слов из статьи Википедии про машинное обучение. Веса слов — это просто целые числа. \n",
    "\n",
    "Естественно, данный подход имеет недостатки: \n",
    "- вес слова зависит от длины документа. В длинных документах слова имеют больший вес, как будто бы они более значимы, но это не так. \n",
    "- самые частотные слова — это союзы, предлоги, местоимения... Они встречаются везде, но абсолютно неинформативны и редко бывают полезны для каких-либо задач классификации. \n",
    "\n",
    "Преодолеть эти недостатки призвана **нормировка**:\n",
    "\n",
    "$$nw_i = \\frac{w_i}{\\sqrt{\\sum_i w_j^2}}$$\n",
    "\n",
    "- отнормируем вектор документа на его длину (например по L2-норме (евклидовой)). \n",
    "\n",
    "Тогда веса слов будут зависеть от длины документа гораздо слабее, но они всё равно **будут зависеть**, так как с увеличением длины документа расширяется используемый словарный запас. Однако, по-прежнему, предлоги и союзы — это самые значимые слова. \n",
    "\n",
    "Нас это не совсем устраивает. *Хотя иногда в задачах классификации предлоги, союзы и местоимения всё-таки могут быть полезны - особенно когда важен стиль написания текста*.\n",
    "\n",
    "[1] Количество уникальных слов в документе - закон Ципфа https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%BA%D0%BE%D0%BD_%D0%A5%D0%B8%D0%BF%D1%81%D0%B0  \n",
    "[2] Векторизация текстов через подсчёт количества словоупотреблений https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Закон Ципфа](#toc0_)\n",
    "- один из фундаментальных эмпирических законов лингвистики (и не только лингвистики, на самом деле). \n",
    "\n",
    "Возьмём большую коллекцию документов, посчитаем для каждого слова в этой коллекции частоту его встречаемости, то есть количество документов, в которых это слово используется, потом отсортируем полученный список по убыванию частот и получим примерно (гиперболически убывающий) график. \n",
    "\n",
    "Плотность распределения Ципфа (класс степенных распределений):\n",
    "\n",
    "$$f(rank, s, N) = \\frac{1}{Z(s, N) \\cdot rank^s}$$\n",
    "\n",
    "$rank$ - исследуемая сл.вел. - порядковый номер слова после сортировки по убыванию частоты  \n",
    "$s$ - коэфф-т скорости убывания вероятности  \n",
    "$N$ - количество слов  \n",
    "$Z(s, N)=\\sum_{i=1}^N j^{(-s)}$ - нормализационная функция, чтобы распределение стало распределением.\n",
    "\n",
    "Закон Ципфа - это эмпирическая закономерность распределения частот слов естественных языков (или достаточно длинных тектов): частотность n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n. Второе по используемости слово встречается примерно в два раза реже, чем первое, третье — в три раза реже, чем первое, и так далее.\n",
    "\n",
    "- Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008. https://nlp.stanford.edu/IR-book/\n",
    "- Закон Ципфа https://en.wikipedia.org/wiki/Zipf%27s_law\n",
    "\n",
    "PS: Американский специалист по биоинформатике Вэньтянь Ли[en] предложил статистическое объяснение закона Ципфа, доказав, что случайная последовательность символов также подчиняется этому закону[12]. Автор делает вывод, что закон Ципфа, по-видимому, является чисто статистическим феноменом, который не имеет отношения к семантике текста и имеет поверхностное отношение к лингвистике. \n",
    "\n",
    "В общих чертах доказательство этой теории состоит в следующем. Вероятность случайного появления какого-либо слова длиной n в цепочке случайных символов уменьшается с ростом n в той же пропорции, в какой растёт при этом ранг этого слова в частотном списке (порядковой шкале). Потому произведение ранга слова на его частотность есть константа.\n",
    "\n",
    "Этот \"закон\" также работает в отношении распределения городской системы: город с самым большим населением в любой стране в два раза больше, чем следующий по размеру город, и так далее.\n",
    "\n",
    "В 1999 году экономист Ксавье Габэ описал закон Ципфа как пример степенного закона: если города будут расти случайным образом с одинаковым среднеквадратичным отклонением, то в пределе распределение будет сводиться к закону Ципфа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример:**\n",
    "\n",
    "Пусть в некотором языке есть $N=3$ слова - А, Б и В. Их ранги - 1, 2 и 3 (нумерация рангов начинается с 1).   \n",
    "Найдите вероятности встретить каждое из этих слов в тексте при условии, что относительные частоты слов распределены по Ципфу с $s=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.73469, 0.18367, 0.08163]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [1 / (1 + 1/4 + 1/9), 1 / 4 / (1 + 1/4 + 1/9), 1 / 9 / (1 + 1/4 + 1/9)]\n",
    "list(map(lambda x: round(x, 5), p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.73469388 0.18367347 0.08163265\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "v = c(1, 2, 3)^(-2)\n",
    "res = 1 / c(1, 2, 3)^2 / sum(v)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите количество слов, которые встречаются менее, чем в 10 из 10000 документов, если предполагать, что вероятность встретить слово в документе распределена по Ципфу с параметром $s = 2$, количество слов в словаре $N = 1000$. Ранги нумеруются с 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 976\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "p <- 10 / 10000                 # пороговая вероятность\n",
    "s <- 2                          # к-т убывания вероятности\n",
    "N <- 1000                       # размер словаря\n",
    "ranks <- (1:N) ^ s\n",
    "Z <- sum((1:N) ^ (-s))\n",
    "f <- 1 / Z / ranks              # вероятности распределения Ципфа\n",
    "sum(f < p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "\n",
    "- частотных слов очень мало. Они слабо информативны, так как встречаются практически во всех документах. \n",
    "- редких слов очень много — если мы какое-нибудь редкое слово встречаем в документе, то мы с большой уверенностью можем сказать, к какой тематике он относится.\n",
    "\n",
    "Нужно как-то свернуть эти два фактора в одну метрику."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[TF-IDF](#toc0_)\n",
    "\n",
    "TF - term frequency - значимость слова w в рамках документа d (количество употреблений слова в документе к длине документа)\n",
    "\n",
    "$$TF(w, d) = \\frac{WordCount(w,d)}{Length(d)}$$\n",
    "\n",
    "IDF - inverse document frequency - обратная частота слова в документах, специфичность слова (размер коллекции делим на количество документов, в которых слово употребляется)\n",
    "\n",
    "$$IDF(w, c) = \\frac{Size(c)}{DocCount(w, c)}$$\n",
    "\n",
    "Тогда итоговый вес слова в документе (TFIDF):\n",
    "\n",
    "$$TFIDF(w, d, c) = TF(w, d) \\cdot IDF(w, c)$$\n",
    "\n",
    "На практике, TF и/или IDF часто логарифмируют, например, так: \n",
    "\n",
    "$TF_l(w, d) = \\log(TF(w, d) + 1)$ - полезно, когда в датасете документы сильно отличаются по длине. Это позволяет сделать распределение весов слов менее контрастным и уменьшить его дисперсию.\n",
    "\n",
    "$IDF_l(w, d) = \\log(IDF(w, c) + 1)$ - полезно, когда документы сильно отличаются по лексике/набору слов (?) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Алгоритм:**\n",
    "\n",
    "1. Нормализация текста (стемминг/лемматизация), выделение базовых элементов (символы, токены, n-граммы)\n",
    "2. Создание частотного словаря Doccount(w, c) для всех w\n",
    "3. Убрать из словаря выбросы по частоте\n",
    "4. Для каждого документа d:\n",
    "    - для каждого слова w из d найти WordCount(w, d)\n",
    "        \n",
    "        записать в результирующий вектор в позицию w значение TFIDF(w, d, c)\n",
    "    - записать вектор документа в таблицу признаков документов коллекции\n",
    "\n",
    "Особенности TFIDF:\n",
    "\n",
    "- это способ отбора категориальных признаков\n",
    "- не использует информацию о метках документов (теряем информацию, если метки есть, но преимущество, если меток нет)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Точечная взаимная информация (pointwise mutual information, PMI)](#toc0_)\n",
    "\n",
    "Альтернативный способ взвешивания признаков [1,2,3]:\n",
    "\n",
    "- измеряется между двумя случайными событиями или реализациями двух случайных величин. \n",
    "\n",
    "- характеризует, насколько сильнее мы будем ожидать первое событие, если перед этим пронаблюдаем второе[4] (по сравнению с нашими априорными ожиданиями). \n",
    "\n",
    "Для задачи классификации текстов:\n",
    "\n",
    "$$pmi(l, w) = \\log \\frac{p(w,l)}{p(w)\\cdot p(l)} = \\log \\frac{p(l|w)}{p(l)} = \\log \\frac{p(w|l)}{p(w)}$$\n",
    "\n",
    "$l$ - коллекция документов, соответствующая метке класса $L$  \n",
    "$w$ - слово из словаря  \n",
    "$p(w, l) = DocCount(w, l) / Size(l)$ - вероятность встретить слово w в документе класса L  \n",
    "$p(w) = \\sum_l DocCount(w, l) / \\sum_l Size(l)$ - маржинальная вероятность употребления слова w  \n",
    "$p(l) = Size(l) / \\sum_m Size(m)$ - маржинальная вероятность встретить документ класса L\n",
    "\n",
    "- Взаимная информация минимальна, когда события w и l несовместны (если происходит одно, второе не может произойти).\n",
    "- Взаимную информацию можно применить для оценки совместной встречаемости слов pmi(w_1, w_2)\n",
    "- Взаимная информация максимальна, когда события w и l всегда появляются одновременно.\n",
    "\n",
    "Все три варианта формулы эквивалентны. \n",
    "\n",
    "Т.о. у нас есть два события. Первое — \"L\": \"мы наблюдаем документ из класса L\". Второе событие — \"W\": \"мы видим в документе слово W\". Все вероятности вычисляются по классическому определению вероятности, то есть как отношение количества положительных исходов к общему числу исходов. \n",
    "\n",
    "**Взаимная информация (PMI)** — это тоже способ взвешивания и отбора категориальных признаков.[5] В первую очередь, он подходит для задач классификации. В задачах регрессии его тоже можно применять — например, дискретизировав целевое распределение, но это уже сложнее. Он требует наличия двух событий, что усложняет его применение в задачах обучения без учителя, хотя он используется для получения плотных векторных представлений слов.\n",
    "\n",
    "[1] Точечная взаимная информация https://en.wikipedia.org/wiki/Pointwise_mutual_information  \n",
    "[2] Взаимная информация - мера связанности двух случайных величин - мат.ожидание PMI https://en.wikipedia.org/wiki/Mutual_information  \n",
    "[3] Применение PMI для представления смыслов слов (про это будут ещё лекции 3.2 и 3.3) Levy, Omer, and Yoav Goldberg. \"Neural word embedding as implicit matrix factorization.\" Advances in neural information processing systems. 2014. https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf  \n",
    "[4] Маргинальное (частное, маржинальное) распределение вероятностей https://en.wikipedia.org/wiki/Marginal_distribution  \n",
    "[5] Ещё несколько способов взвешивания и отбора признаков https://scikit-learn.org/stable/modules/feature_selection.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Создаём нейросеть для работы с текстом](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дано:**\n",
    "\n",
    "- коллекция документов по тематике - положительные примеры (класс 1)\n",
    "- фоновая коллекция документов - отрицательные примеры (класс 0)\n",
    "- длина документов - от нескольких предложений (для твиторов используются другие подходы)\n",
    "\n",
    "**Задача:**\n",
    "\n",
    "Построить бинарный классификатор\n",
    "$$Classifier: \\mathbb R^d \\longrightarrow \\{0, 1\\}$$\n",
    "\n",
    "$d$ - размер словаря\n",
    "\n",
    "**Решение:**\n",
    "\n",
    "Представление документов - **мешок слов**  \n",
    "Решающее правило - **логистическая регрессия**\n",
    "\n",
    "**Модель:**\n",
    "\n",
    "$$\\hat y(x) = \\sigma (w^Tx+b)$$\n",
    "\n",
    "$x \\in \\mathbb R^d$ - вектор признаков, если надо, дополненный единицей для смещения   \n",
    "$\\hat y \\in \\mathbb R, \\hat y \\in [0,1]$ - вероятность выпадения 1 для сл.вел. $y \\in \\{0,1\\}$  \n",
    "$w \\in \\mathbb R^d$ - вектор весов  \n",
    "$\\sigma(x) = \\frac {1}{1+e^{-x}}$ - логистическая функция (сигмоида)\n",
    "\n",
    "Это можно рассматривать как искусственный нейрон с $d$ входами и 1 выходом.\n",
    "\n",
    "**Алгорим (процесс обучения):**\n",
    "\n",
    "Функция потерь - бинарная кросс-энтропия\n",
    "\n",
    "$$BCE(\\hat y, y) = -y \\log \\hat y - (1-y) \\log(1-\\hat y) \\longrightarrow \\min$$\n",
    "\n",
    "- первое слагаемое - штраф за ложноположительный отве, второе - за ложноотрицательный\n",
    "\n",
    "Алгоритм настройки (поиск весов модели) - градиентный спуск."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дано:**\n",
    "\n",
    "- несколько коллекций документов по разным тематикми\n",
    "- длина документов - от нескольких предложений (для богомерзких твиторов используются другие подходы)\n",
    "\n",
    "**Задача:**\n",
    "\n",
    "Построить n-арный классификатор\n",
    "$$Classifier: \\mathbb R^d \\longrightarrow \\{1, ..., c\\}$$\n",
    "\n",
    "$d$ - размер словаря\n",
    "\n",
    "**Решение:**\n",
    "\n",
    "Представление документов - **мешок слов**  \n",
    "Решающее правило - **логистическая регрессия**\n",
    "\n",
    "**Модель:**\n",
    "\n",
    "$$\\hat y(x) = \\text{softmax} (W \\cdot x)$$\n",
    "\n",
    "$x \\in \\mathbb R^d$ - вектор признаков, если надо, дополненный единицей для смещения   \n",
    "$\\hat y \\in \\mathbb R^с, \\sum_{i=1}^c \\hat y_i = 1$ - на выходе вектор - распределение вероятностей отнесения к классу $i$     \n",
    "$W \\in \\mathbb R^{c \\times d}$ - матрица весов (количество классов Х количество признаков)   \n",
    "$\\text {softmax}(x) = \\left \\{ \\frac {e^{x_j}}{\\sum_{j=1}^c e^{x_j}} \\right \\}_i$ - функция активации softmax: отображает вещественный вектор в такой вектор положительных чисел, что сумма координат всегда равна 1.\n",
    "\n",
    "Это можно рассматривать как искусственный нейрон с $d$ входами и $c$ выходами.\n",
    "\n",
    "**Алгорим (процесс обучения):**\n",
    "\n",
    "Функция потерь - кросс-энтропия\n",
    "\n",
    "$$CE(\\hat y, y) = - \\sum_{i=1}^c y_i \\log \\hat y_i \\longrightarrow \\min$$\n",
    "\n",
    "- т.к. классы взаимоисключающие, то достаточно штрафовать за ложноотрицательные предсказания\n",
    "\n",
    "Алгоритм настройки (поиск весов модели) - градиентный спуск."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы поговорили про логистическую регрессию (или однослойную нейронную сеть), которая, по сути, является линейной регрессией с функцией активации в виде сигмоиды или софтмакса, а также мы рассмотрели варианты для двух и нескольких классов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8085580320712731 4.615220521841592\n",
      "4.615220521841592 1.3862943611198906\n",
      "1.3862943611198906 4.615220521841592\n",
      "0.8085580320712731 0.8615658321849085\n",
      "4.615220521841592 0.8085580320712731\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "def TBCE(y_pred, y_true=(1, 1)):\n",
    "    r = 0\n",
    "    for i in range(len(y_true)):\n",
    "        r += BCE(y_pred[i], y_true[i])\n",
    "    return r\n",
    "\n",
    "def BCE(y_hat, y):\n",
    "    result = -y * log(y_hat) - (1 - y) * log(1-y_hat)\n",
    "    return result\n",
    "\n",
    "print(TBCE([0.99, 0.45]), TBCE([0.99, 0.01]))\n",
    "print(TBCE([0.99, 0.01]), TBCE([0.5,0.5]))\n",
    "print(TBCE([0.5,0.5]), TBCE([0.99, 0.01]))\n",
    "print(TBCE([0.99,0.45]), TBCE([0.65,0.65]) )\n",
    "print(TBCE([0.99, 0.01]), TBCE([0.99, 0.45]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите в общем виде производную функции потерь $\\frac{\\partial BCE(\\hat{y}, y)}{\\partial w}$ и $\\frac{\\partial BCE(\\hat{y}, y)}{\\partial b}$ и запишите в ответ её формулу.\n",
    "\n",
    "Ну там еще с регуляризацией производные, вот это все.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x*(-y*exp(b + w*x) - y + exp(b + w*x))/(exp(b + w*x) + 1)\n",
      "(-y*exp(b + w*x) - y + exp(b + w*x))/(exp(b + w*x) + 1)\n",
      "(2*c*w*exp(b + w*x) + 2*c*w - x*y*exp(b + w*x) - x*y + x*exp(b + w*x))/(exp(b + w*x) + 1)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\operatorname{Set}\\left(\\frac{x \\left(- y e^{b + w x} - y + e^{b + w x}\\right)}{e^{b + w x} + 1}, \\frac{- y e^{b + w x} - y + e^{b + w x}}{e^{b + w x} + 1}, c \\left(b^{2} + w^{2}\\right) - y \\log{\\left(\\frac{1}{e^{- b - w x} + 1} \\right)} - \\left(1 - y\\right) \\log{\\left(1 - \\frac{1}{e^{- b - w x} + 1} \\right)}\\right)$"
      ],
      "text/plain": [
       "Set(x*(-y*exp(b + w*x) - y + exp(b + w*x))/(exp(b + w*x) + 1), (-y*exp(b + w*x) - y + exp(b + w*x))/(exp(b + w*x) + 1), c*(b**2 + w**2) - y*log(1/(exp(-b - w*x) + 1)) - (1 - y)*log(1 - 1/(exp(-b - w*x) + 1)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import * \n",
    "\n",
    "\n",
    "hy, y, x, w, b, c, t = symbols(\"hy y x w b c t\")\n",
    "hy = 1 / (1 + exp(- w*x - b))\n",
    "BCE = -y*log(hy) - (1 - y) * log(1 - hy)\n",
    "BCE_w = diff(BCE, w).simplify() \n",
    "print(BCE_w)\n",
    "\n",
    "BCE_b = diff(BCE, b).simplify() \n",
    "print(BCE_b)\n",
    "\n",
    "loss_reg = BCE + c * (w**2 + b**2)\n",
    "loss_w = diff(loss_reg, w).simplify() \n",
    "print(loss_w)\n",
    "\n",
    "# diff_str = str(BCE_w)\n",
    "# expr = parsing.sympy_parser.parse_expr(diff_str)\n",
    "# sample_value = expr.evalf(subs=dict(x=0.5, y=1, w=4, b=1))\n",
    "# print(sample_value)\n",
    "\n",
    "Set(BCE_w, BCE_b, loss_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию для вычисления точечной взаимной информации двух случайных событий.\n",
    "\n",
    "$$pmi(a, b) = \\log \\frac {p(a, b)} {p(a) p(b)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693147\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array([int(s.strip()) for s in s.strip().split(' ')])\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def calculate_pmi(a, b):\n",
    "    pa = np.mean(a)\n",
    "    pb = np.mean(b)\n",
    "    pab = np.mean(a & b)\n",
    "\n",
    "    return np.log(pab/pa/pb)\n",
    "\n",
    "a = parse_array('1 0 0 1 1 0')\n",
    "b = parse_array('1 0 0 0 1 0')\n",
    "pmi_value = calculate_pmi(a, b)\n",
    "\n",
    "print('{:.6f}'.format(pmi_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы предлагаем вам собрать регулярное выражение из \"деталей\" так, чтобы оно выделяло в отдельные токены знаки препинания, числа и слова.\n",
    "\n",
    "А именно:\n",
    "\n",
    "    Числа с плавающей точкой вида 123.23 выделяются в один токен. Десятичным разделителем может быть точка или запятая.\n",
    "    Число может быть отрицательным: иметь знак -123.4−123.4\n",
    "    Целой части числа может вовсе не быть: последовательности  -0.15−0.15  и  -.15−.15   означают одно и то же число.\n",
    "    При этом числа с нулевой дробной частью не допускаются:  строка \"12345.12345.\" будет разделена на два токена \"12345\" и \".\"\n",
    "    Идущие подряд знаки препинания выделяются каждый в отдельный токен.\n",
    "    Наконец множество букв в словах ограничивается только кириллическим алфавитом (33 буквы, включая букву ё).\n",
    "    Обратите внимание, что в результате токенизации не должно получаться пустых токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "контактный телефон : 123123 .\n",
      "что - нибудь надо придумать .\n",
      "значение числа е = 2.7182 .\n",
      "демон 123 , как тебя зовут в реале ?\n",
      "-1 -.15 = -1.15\n",
      "- 1 - .15 = -1.15\n",
      "какого ; % : ? * тут происходит ?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "lines = [\"Контактный телефон: 123123.\", \n",
    "         \"Что-нибудь надо придумать.\", \n",
    "         \"Значение числа Е=2.7182.\", \n",
    "         \"Демон123, как тебя зовут в реале?\", \n",
    "         \"-1-.15=-1.15\", \n",
    "         \"- 1 - .15 = -1.15\", \n",
    "         \"Какого ;%:?* тут происходит?\"]\n",
    "\n",
    "# модифицируйте это регулярное выражение\n",
    "TOKENIZE_RE = re.compile(r'[а-яё]+|-?\\d*[.,]?\\d+|\\S', re.I)\n",
    "\n",
    "\n",
    "def tokenize(txt):\n",
    "    return TOKENIZE_RE.findall(txt)\n",
    "\n",
    "for line in lines:\n",
    "    print(' '.join(tokenize(line.strip().lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дана следующая коллекция текстов. Постройте словарь (отображение из строкового представления токенов в их номера) и вектор весов (DF). Вывести в порядке возрастания частоты встречаемости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "наказывать не обязательно казнить освободить нельзя помиловать\n",
      "0.25 0.25 0.25 0.5 0.5 0.75 0.75\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Казнить нельзя, помиловать. Нельзя наказывать.',\n",
    "    'Казнить, нельзя помиловать. Нельзя освободить.',\n",
    "    'Нельзя не помиловать.',\n",
    "    'Обязательно освободить.']\n",
    "\n",
    "# Получение DF\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(smooth_idf=False, use_idf=True)\n",
    "vectorizer.fit_transform(corpus)\n",
    "\n",
    "# из IDF  в DF\n",
    "word_doc_freq = 1 / np.exp(vectorizer.idf_ - 1) ## ВОТ ОСОБЕННОСТЬ ПЕРЕВОДА ИЗ IDF В DF\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "vocabulary = vocabulary[np.argsort(word_doc_freq)]\n",
    "word_doc_freq = word_doc_freq[np.argsort(word_doc_freq)]\n",
    "\n",
    "print(*vocabulary)\n",
    "print(*np.round(word_doc_freq, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте матрицу признаков для текстов с предыдущего шага с использованием словаря и вектора весов, полученного на предыдущем шаге. Используйте взвешивание $lTFIDF = \\ln(TF + 1) \\cdot IDF$.\n",
    "\n",
    "Значения признаков следует отмасштабировать так, чтобы для каждого признака его среднее значение по выборке равнялось 0, а среднеквадратичное отклонение 1: $x^{scaled}_{i} = \\frac{x_{i} - E(x)} {\\sigma(x)}$.\n",
    "\n",
    "В результате масштабирования для каждого столбца матрицы признаков среднее должно равняться 0, а среднеквадратичное отклонение 1.\n",
    "\n",
    "При расчёте среднеквадратического отклонения необходимо использовать скорректированную оценку $\\sigma=\\sqrt{\\frac{\\sum_{i-1}^n(x_i - E(x))^2}{n - 1}}$. Чтобы получить такую оценку с помощью numpy, необходимо передать параметр ddof=1: \n",
    "\n",
    "    feature_matrix = np.zeros((num_docs, num_feats))\n",
    "    feats_std = feature_matrix.std(0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 -0.5 -0.5 0.87 -0.76 0.6 0.16\n",
      "-0.5 -0.5 -0.5 0.87 0.18 0.6 0.16\n",
      "-0.5 1.5 -0.5 -0.87 -0.76 0.29 1.04\n",
      "-0.5 -0.5 1.5 -0.87 1.34 -1.48 -1.36\n"
     ]
    }
   ],
   "source": [
    "IDF = np.log(1 / word_doc_freq) + 1     # перевели обратно в IDF (специфичность слов в корпусе)\n",
    "\n",
    "def tokenize(doc):                      # простой токенизатор\n",
    "    return re.findall(r'[\\w\\d]+', doc.lower())\n",
    "\n",
    "def count(word, doc):                   # простой счетчик\n",
    "    tokens = tokenize(doc)\n",
    "    doc_len = len(tokens)\n",
    "    return tokens.count(word) / doc_len\n",
    "\n",
    "vcount = np.vectorize(count)\n",
    "TF = vcount(word=vocabulary, \n",
    "            doc=np.array(corpus).reshape(-1, 1))   # корпус развернут в столбец\n",
    "\n",
    "lTFIDF = np.log(TF + 1) * IDF           # сглаженный как просили TFIDF                         \n",
    "\n",
    "res = (lTFIDF - lTFIDF.mean(axis = 0)) / lTFIDF.std(0, ddof=1)      # центрированный масштабированный TFIDF\n",
    "\n",
    "[print(*row) for row in np.round(res, 2)]\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Семинары в директории `./stepic-dl-nlp`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53d00ba0b92f737b23b3e678e3a3ceb3fe4e948ad1ab95d9c6fdcbb4b4ec65f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
