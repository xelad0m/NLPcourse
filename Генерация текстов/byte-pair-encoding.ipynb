{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-pair encoding\n",
    "\n",
    "Че получается: \n",
    "- Есть две крайности кодирование текста как последовательности символов и как последовательности токенов. Оба хуже.\n",
    "- Можно брать просто n-граммы и подавать их как символы (долго), или например усреднять их вектора (fastText - (для неизвестных слов вектор строится как наиболее близкий по символьным эмбеддингам)) и но это слишком абстрактный подход, для генерации текста не очень подходит\n",
    "- Промежуточный вариант - учет n-грамм в составе последовательностей по частоте (**byte pair encoding**) \n",
    "  - \"aabc aabaa\" -> \"Xbc XbX -> \"Yc YX\"\n",
    "\n",
    "Это позволяет сильно сократить размер словаря и, при этом, не сильно увеличить длину входных цепочек токенов, в отличие от символьных языковых моделей.\n",
    "\n",
    "Нужно отметить, что пары имеет смысл выделять только в рамках токена, поэтому в пределе алгоритм просто выходит на кодирование отдельных токенов целиком.\n",
    "- получается, что это промежуточный вариант, начинается с символьного кодирования и итеративно доходит до токенов\n",
    "- поэтому надо где-то остановиться, \n",
    "- то есть глубина кодирования это еще один гиперпараметр модели, который надо подбирать\n",
    "  - некоторый процент от разности количества слов (максимум объема кодирующего словаря) и количества символов (минимум). Можно добавить, что это максимум и минимум, если различных слов в корпусе больше, чем различных символов, остальное можно считать вырожденными случаями в рамках таких задач, иначе зачем было вообще алфавит придумывать, если не так, то могут быть ньюансы (интересно, как этот момент с иероглифами пересекается).\n",
    "- данный гиперпараметр идентичен гиперпараметру - размер словаря модели\n",
    "- добавлены токены для начала слова и конца слова `<w>` и `</w>`, начала и конца абзаца `<n>` и `</n>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 38)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "text1 = \"\"\"Че получается: \n",
    "- Есть две крайности кодирование текста как последовательности символов и как последовательности токенов. Оба хуже.\n",
    "- Можно брать просто n-граммы и подавать их как символы (долго), или например усреднять их вектора (fastText - (для неизвестных слов вектор строится как наиболее близкий по символьным эмбеддингам)) и но это слишком абстрактный подход, для генерации текста не очень подходит\n",
    "- Промежуточный вариант - учет n-грамм в составе последовательностей по частоте (**byte pair encoding**) \n",
    "  - \"aabc aabaa\" -> \"Xbc XbX -> \"Yc YX\"\n",
    "\n",
    "Это позволяет сильно сократить размер словаря и, при этом, не сильно увеличить длину входных цепочек токенов, в отличие от символьных языковых моделей.\n",
    "\n",
    "Нужно отметить, что пары имеет смысл выделять только в рамках токена, поэтому в пределе алгоритм просто выходит на кодирование отдельных токенов целиком.\n",
    "- получается, что это промежуточный вариант, начинается с символьного кодирования и итеративно доходит до токенов\n",
    "- поэтому надо где-то остановиться, \n",
    "- то есть глубина кодирования это еще один гиперпараметр модели, который надо подбирать\"\"\"\n",
    "\n",
    "text = \"There is an 80% chance of rainfall today. We are pretty sure it is going to rain.\"\n",
    "\n",
    "# get the word frequency and add the end of word (</w>) token at the end of each word\n",
    "def make_word_dict(text):\n",
    "    \"\"\"возвращает частоты токентов в тексте\"\"\"\n",
    "    words = [\"<w>\" + ' '.join(token) + \"</w>\" for token in text.strip().split(\" \")]\n",
    "    return Counter(words)\n",
    "\n",
    "def get_pairs(word_freq_dict):\n",
    "    \"\"\"возвращает биграммы в токенах и их частоты\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in word_freq_dict.items():\n",
    "        chars = word.split()\n",
    "        for i in range(len(chars)-1):\n",
    "            pairs[chars[i], chars[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_byte_pairs(best_pair, word_freq_dict):\n",
    "    \"\"\"возвращает копию словаря с объединенными в подстроку биграммами\"\"\"\n",
    "    merged_dict = {}\n",
    "    bigram = re.escape(' '.join(best_pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') # все такие биграммы, обрамленные пробелами\n",
    "    for word in word_freq_dict:\n",
    "        w_out = p.sub(''.join(best_pair), word)     # убрать из биграммы пробел (объединение)\n",
    "        merged_dict[w_out] = word_freq_dict[word]\n",
    "    return merged_dict\n",
    "\n",
    "def get_subword_tokens(word_freq_dict):\n",
    "    \"\"\"возвращает словарь частот подстрок\"\"\"\n",
    "    char_freq_dict = defaultdict(int)\n",
    "    for word, freq in word_freq_dict.items():\n",
    "        chars = word.split()\n",
    "        for char in chars:\n",
    "            char_freq_dict[char] += freq\n",
    "    return char_freq_dict\n",
    "\n",
    "def train(word_freq_dict, rate=0.5):\n",
    "    \"\"\"\n",
    "    Возвращает список субтокенов заданной длины:\n",
    "        rate - целевой размер кодирующего словаря как доля от исходного количества токенов\n",
    "        rate = 1 (кодирование токенов целиком), rate = 0 (символьное кодирование)\n",
    "    \"\"\"\n",
    "    init_len = len(word_freq_dict)\n",
    "    temp_word_freq_dict = word_freq_dict.copy()\n",
    "    subword_tokens = get_subword_tokens(temp_word_freq_dict)\n",
    "\n",
    "    while len(subword_tokens) / init_len <= rate:\n",
    "        pairs = get_pairs(temp_word_freq_dict)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        temp_word_freq_dict = merge_byte_pairs(best_pair, temp_word_freq_dict)\n",
    "        subword_tokens = get_subword_tokens(temp_word_freq_dict)\n",
    "    \n",
    "    return subword_tokens\n",
    "\n",
    "word_freq_dict = make_word_dict(text)\n",
    "new_freq_dict1 = train(word_freq_dict, rate=1)\n",
    "new_freq_dict0 = train(word_freq_dict, rate=0)\n",
    "len(new_freq_dict0), len(new_freq_dict1)\n",
    "# new_freq_dict0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__sow', 'vi', 'z', 'zi', 'ni', '__eow', '__sow', ':', '__eow', 'he', 'didn', \"'\", 't', 'fall', '__sow', '?', '__eow', '__sow', 'in', 'co', 'n', 'ce', 'iv', 'ab', 'le', '__eow', '__sow', '!', '__eow']\n",
      "[24, 108, 82, 83, 71, 25, 24, 154, 25, 14, 10, 11, 12, 13, 24, 85, 25, 24, 140, 59, 39, 157, 87, 165, 114, 25, 24, 148, 25]\n",
      "vizzini : he didn ' t fall ? inconceivable !\n"
     ]
    }
   ],
   "source": [
    "from bpe import Encoder     # trash\n",
    "\n",
    "# Generated with http://pythonpsum.com\n",
    "test_corpus = '''\n",
    "    Object\\nraspberrypi functools dict kwargs. Gevent raspberrypi functools. Dunder raspberrypi decorator dict didn't lambda zip import pyramid, she lambda iterate?\n",
    "    Kwargs raspberrypi diversity unit object gevent. Import fall integration decorator unit django yield functools twisted. Dunder integration decorator he she future. Python raspberrypi community pypy. Kwargs integration beautiful test reduce gil python closure. Gevent he integration generator fall test kwargs raise didn't visor he itertools...\n",
    "    Reduce integration coroutine bdfl he python. Cython didn't integration while beautiful list python didn't nit!\n",
    "    Object fall diversity 2to3 dunder script. Python fall for: integration exception dict kwargs dunder pycon. Import raspberrypi beautiful test import six web. Future integration mercurial self script web. Return raspberrypi community test she stable.\n",
    "    Django raspberrypi mercurial unit import yield raspberrypi visual rocksdahouse. Dunder raspberrypi mercurial list reduce class test scipy helmet zip?\n",
    "'''\n",
    "\n",
    "encoder = Encoder(200, pct_bpe=0.88)  # params chosen for demonstration purposes\n",
    "encoder.fit(test_corpus.split('\\n'))\n",
    "\n",
    "example = \"Vizzini: He didn't fall? INCONCEIVABLE!\"\n",
    "print(encoder.tokenize(example))\n",
    "# ['__sow', 'vi', 'z', 'zi', 'ni', '__eow', '__sow', ':', '__eow', 'he', 'didn', \"'\", 't', 'fall', '__sow', '?', '__eow', '__sow', 'in', 'co', 'n', 'ce', 'iv', 'ab', 'le', '__eow', '__sow', '!', '__eow']\n",
    "print(next(encoder.transform([example])))\n",
    "# [24, 108, 82, 83, 71, 25, 24, 154, 25, 14, 10, 11, 12, 13, 24, 85, 25, 24, 140, 59, 39, 157, 87, 165, 114, 25, 24, 148, 25]\n",
    "print(next(encoder.inverse_transform(encoder.transform([example]))))\n",
    "# vizzini : he didn ' t fall ? inconceivable !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "44 44 43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{',': 4,\n",
       " 'f': 24,\n",
       " 'b': 20,\n",
       " 'g': 25,\n",
       " ' ': 1,\n",
       " 'R': 18,\n",
       " 'e': 23,\n",
       " '?': 9,\n",
       " 'i': 27,\n",
       " 'v': 39,\n",
       " 'I': 14,\n",
       " 'p': 34,\n",
       " 'a': 19,\n",
       " '2': 6,\n",
       " '\\n': 0,\n",
       " 'l': 30,\n",
       " 'h': 26,\n",
       " \"'\": 3,\n",
       " 's': 36,\n",
       " 'm': 31,\n",
       " '.': 5,\n",
       " ':': 8,\n",
       " 'F': 12,\n",
       " 'k': 29,\n",
       " '!': 2,\n",
       " 'c': 21,\n",
       " 'd': 22,\n",
       " 'w': 40,\n",
       " 'P': 17,\n",
       " 'C': 10,\n",
       " 'z': 43,\n",
       " 'y': 42,\n",
       " 'G': 13,\n",
       " 'u': 38,\n",
       " 'r': 35,\n",
       " 'j': 28,\n",
       " 'D': 11,\n",
       " 'K': 15,\n",
       " 't': 37,\n",
       " '3': 7,\n",
       " 'o': 33,\n",
       " 'O': 16,\n",
       " 'n': 32,\n",
       " 'x': 41}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tokenizers import CharBPETokenizer\n",
    "from tokenizers.pre_tokenizers import Metaspace, Whitespace, Sequence\n",
    "\n",
    "from tokenizers import Tokenizer, normalizers, decoders, models\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizers = Sequence([Metaspace(\"\\n\"), Whitespace()])\n",
    "\n",
    "tokenizer.train_from_iterator(test_corpus)\n",
    "print(tokenizer.get_vocab_size(), len(tokenizer.get_vocab()), max(tokenizer.get_vocab().values()))  # OK\n",
    "\n",
    "coded = tokenizer.encode(\"unit object\\ngevent\")\n",
    "coded.tokens, coded.ids, tokenizer.decode(coded.ids)\n",
    "tokenizer.get_vocab()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75291dc0307ea48294888123147845d2e15abd18d38848ca6ac05a6fe8c88425"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
