{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Содержание**<a id='toc0_'></a>    \n",
    "- [Моделирование языка](#toc1_)    \n",
    "- [Подходы к моделированию языка](#toc2_)    \n",
    "      - [Авторегрессионные языковые модели](#toc2_1_1_1_)    \n",
    "      - [Модели со свободным порядком факторизации](#toc2_1_1_2_)    \n",
    "  - [Классическая N-граммная модель](#toc2_2_)    \n",
    "  - [Модели SkipGram и CBOW](#toc2_3_)    \n",
    "  - [Авторегрессионные модели](#toc2_4_)    \n",
    "  - [Двунаправленные языковые модели. ELMo](#toc2_5_)    \n",
    "  - [BERT](#toc2_6_)    \n",
    "  - [Модель XLNet](#toc2_7_)    \n",
    "  - [Актуальные архитектурные решения в языковых моделях](#toc2_8_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Моделирование языка](#toc0_)\n",
    "\n",
    "\\*) наверно все-таки языковая модель?\n",
    "\n",
    "На **вход** поступает токенизированный текст. \n",
    "\n",
    "$$[t_1, t_2, ..., t_l] \\sim P(t_1, t_2, ..., t_l)$$\n",
    "\n",
    "Считаем входящий текст реализацией случайной величины (семпл). \n",
    "\n",
    "**Выход**: оценка плотности вероятности $Q(t_1, t_2, ..., t_l) = ?$\n",
    "\n",
    "\n",
    "Мы строим модель, которая должна уметь, для заданного фрагмента текста, оценивать вероятность встретить такой фрагмент в настоящем тексте (генеральной совокупности), насколько данный фрагмент похож на настоящий текст, или — насколько он правдоподобен. \n",
    "\n",
    "В статистике это - задача оценки плотности распределения.\n",
    "\n",
    "В контексте машинного обучения это является одной из возможных постановок задач обучения без учителя. \n",
    "\n",
    "Если у нас есть вычислимая оценка плотности распределения, то мы можем сэмплировать (делать выборки) из этого распределения — то есть, в нашем случае, мы можем генерировать тексты с помощью языковых моделей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Зачем нам это всё надо?**\n",
    "\n",
    "- Исправление ошибок (proofreading) [1]: \n",
    "  - оцениваем правдоподобие набираемого текста с помощью, находим слова, более всего снижающие правдоподобие текста, и подсвечиваем их как возможное место, где есть ошибка. \n",
    "- Предложение вариантов при генерации текстов:\n",
    "  - распознавание речи (let it be / let eat bee)\n",
    "  - машинный перевод\n",
    "- Использование неразмеченных данных для обогащения моделей машинного обучения\n",
    "  - **перенос знаний**, перенос навыков (transfere learning): в обучении с учителем размеченные данные обычно сравнительно небольшие. Модель, обученная без учителя, будет отражать гораздо большее число харакетристик данных, чем только одни метки классов\n",
    "  - предобучение (pretraining), дообучение (fine-tuning), извлечение признаков (feature extration): \n",
    "    - Мы можем обучить большую модель на большом корпусе — предпочтительно, без учителя (если можем, конечно). \n",
    "    - Потом — использовать небольшой корпус, предназначенный для одной конкретной задачи, чтобы \"донастроить\" полученную большую модель именно под эту задачу. \n",
    "\n",
    "**Языковые модели** — это основа переноса навыков в обработке текстов. Это как нейросети, обученные на ImageNet, если проводить аналогию с областью компьютерного зрения.\n",
    "\n",
    "[1] https://habr.com/ru/post/108831/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Подходы к моделированию языка](#toc0_)\n",
    "\n",
    "Даны тексты $[t_1, t_2, ..., t_l] \\sim P(t_1, t_2, ..., t_l)$, $P$ - неизвестно (некоторое многомерное распределение)\n",
    "\n",
    "Правило цепочки (chain rule):\n",
    "\n",
    "$$P(a, b, c) = P(a|b, c) P(b|c) P(c) = P(b|a, c) P(c|a) P(a)$$\n",
    "\n",
    "- совместное распределение нескольких случайных величин можно представить в виде произведения условных распределений **в различном порядке**.\n",
    "\n",
    "Порядок, в котором записывается условные распределение, называется порядком факторизации (англоязычный термин factorization order). В зависимости от того, как мы раскручиваем эту цепочку, мы можем получить разные постановки задачи моделирования языка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_1_'></a>[Авторегрессионные языковые модели](#toc0_)\n",
    "\n",
    "Классическая и одна из самых популярных постановок (**жесткий порядок факторизации**), когда мы предсказываем слово за словом. \n",
    "\n",
    "$$P(t_1, t_2, ..., t_l) = P(t_l|t_1,t_2,, ..., t_{l-1})P(t_{l-1}|t_1,, ..., t_{l-2}))...P(t_1)\n",
    "\\\\P(t_1, t_2, ..., t_l) = \\prod_{i=1}^l P(t_i|t_{1:i-1})$$\n",
    "\n",
    "- Вероятность следующего слова при наблюдении какого-то количества предыдущих слов. \n",
    "- К этой группе относятся популярные модели — такие, как **ELMo, GPT, OpenAI Transformer[1], XLNet**\n",
    "\n",
    "#### <a id='toc2_1_1_2_'></a>[Модели со свободным порядком факторизации](#toc0_)\n",
    "\n",
    "Другая постановка подразумевает отсутствие жёстко заданного порядка факторизации. \n",
    "\n",
    "Такие модели предсказывают слова в середине фрагмента текста по остальным словам. \n",
    "\n",
    "К таким моделям относиться **word2vec, CBOW, BERT, XLNet**.\n",
    "\n",
    "[1] Попробуйте пообщаться с трансформером: https://talktotransformer.com/ (лучше, на английском)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Классическая N-граммная модель](#toc0_)\n",
    "\n",
    "Простейшая авторегрессионная модель (AR-модель).\n",
    "\n",
    "$$P(t_1, t_2, ..., t_l) = \\prod_{i=1}^l P(t_i|t_{1:i-1})$$\n",
    "\n",
    "Длина истории ограничена $n$: $P(t_i|t_{1:i-1}) = P(t_i|t_{i-n:i-1})$\n",
    "- не учитываем вероятности слов далее $n$\n",
    "\n",
    "Это ограничение называется **свойство Марковости n-го порядка**:\n",
    "- $n=1$ это простейшая марковская цепь\n",
    "\n",
    "Все вероятности хранятся в модели в явном виде (с помошью хэш-таблицы или префиксного дерева)\n",
    "\n",
    "Обучение - это подсчет частот: $P(t_i|t_{i-n:i-1}) = \\frac{P(t_{i-n:i})} {P(t_{i-n:i-1})}$\n",
    "\n",
    "Преимущества:\n",
    "- простота реализации \n",
    "- скорость\n",
    "\n",
    "Недостатки:\n",
    "- при увеличении N, количество N-грамм соответствующей длины растёт в геометрической прогрессии (на практике N не превышает 3)\n",
    "- полученные представления сложно закодировать в признаки для дальнейшей работы\n",
    "- сильное обобщение, сжатие представления\n",
    "- нет работы с неизвестными словами (нет вектора - нет расстояния)\n",
    "  - частично решается переходом с токенов на символы, но тут в полный рост встает проблема большого N для N-грамм\n",
    "- такие модели ничего не знают про синтаксический анализ, про сложную многосвязную структуру текста. \n",
    "  - всё, что они видят — это только сколько-то слов слева от текущего. Другими словами они не особо-то выразительны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Модели SkipGram и CBOW](#toc0_)\n",
    "\n",
    "**SkipGram**: предсказание соседних слов в некотором окне при условии наблюдения центрального слова\n",
    "\n",
    "$$P(t_{k+m}|t_{k}), m = \\{-d, -d+1, -d+2, ..., d\\}$$\n",
    "\n",
    "**Continuous Bag-of-words (CBoW)**: предсказывает центральное слово по остальным словам, в рамках окна\n",
    "\n",
    "$$P(t_k|t_{k-d},..,t_{k-1},t_{k+1},..,t_{k+d} )$$\n",
    "\n",
    "- Принципиальное отличие обоих моделей от классической N-граммной модели - векторное представление в виде 2 векторов (центральный \"w\" и контекстный \"d\")\n",
    "\n",
    "- Вероятности считаются через $softmax$ от скалярного произведения векторов\n",
    "  - softmax: отображает вещественный вектор в такой вектор положительных чисел, что сумма координат всегда равна 1\n",
    "  - neg.sampling: тоже самое, но по небольшой подвыборке словаря\n",
    "  - есть еще иерархический софтмакс[1]\n",
    "\n",
    "$$P(t_j|t_i) = \\frac {e^{w_j \\cdot d_i}}{\\sum_{j=1}^|V| e^{w_j \\cdot d_i}} = \\text{softmax} \\approx \\text{neg.sampling} = \\frac {e^{w_j \\cdot d_i^+}}{\\sum_{j=1}^M e^{w_j \\cdot d_i^-}}, M \\ll |V|$$\n",
    "\n",
    "- Настраиваются модели word2vec (а это модели данного класса) с помощью **градиентного спуска** и в качестве целевой функции выступает **кросс-энтропия**.\n",
    "\n",
    "- По сути, модель языка решает задачу **классификации**, в которой количество классов равно размеру словаря.\n",
    "\n",
    "- можно оценить правдоподобие текста:\n",
    "$$P(t_1, t_2, ..., t_l) \\approx \\prod_{i=1}^l \\prod_{j=1, j\\neq i}^l P(t_j|t_i)$$\n",
    "\n",
    "**Преимущества:**\n",
    "- простота\n",
    "- сжатое представление, вектора удобны для использования в других алгоритмах в рамках конвееров\n",
    "- арифметика смыслов\n",
    "- работа сбольшими словарями (порядка $10^6$)\n",
    "\n",
    "**Недостатки:**\n",
    "- нет работы с неизвестными словами\n",
    "  - но зато **FastText** умеет\n",
    "- невозможность учета сложного контекста\n",
    "  - у нас только 1 вектор для слова, он характеризует только 1 смысл\n",
    "\n",
    "[1] Hierarchical softmax and negative sampling: short notes worth telling  \n",
    "[2] https://dumps.wikimedia.org/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[Авторегрессионные модели](#toc0_)\n",
    "\n",
    "Вот тут (с учетом недостатков word2vec по работе с контекстом) на сцену выходят **авторегрессионные языковые модели на основе нейросетей**. \n",
    "\n",
    "Модель:\n",
    "\n",
    "- каждый токен по-отдельности преобразуется в вектор, получается embedding токена без учёта контекста (правило цепочки)\n",
    "\n",
    "$$P(t_1, t_2, ..., t_l) = \\prod_{i=1}^l P(t_i|t_{1:i-1})$$\n",
    "\n",
    "- несколько слоев рекуррентных (RNN), свёрточных (CNN) или архитектур с вниманием (Self-attention/Transformer decoder) - происходит учёт контекста, \n",
    "  \n",
    "- на последнем слое - предсказываются вероятности слов. \n",
    "  \n",
    "Чаще всего в последнее время используется честный софтмакс, а не его аппроксимация, так как использование приближений несколько ухудшает результаты. \n",
    "\n",
    "Поэтому важно не раздувать словарь слишком сильно — на практике, многие большие языковые модели работают со словарями размера не больше 50 000 уникальных токенов. \n",
    "\n",
    "Очевидно, что при этом мы отбрасываем очень большое количество слов. \n",
    "\n",
    "Как с этим работать:\n",
    "- символьные N-граммы\n",
    "- FastText эмбеддинги (для неизвестных слов вектор строится как наиболее близкий по символьным эмбеддингам)\n",
    "- промежуточный вариант (**byte pair encoding**) \n",
    "\n",
    "**byte pair encoding**\n",
    "\n",
    "Простой алгоритм кодирования, который позволяет сжать длину последовательностей за счёт расширения словаря (то есть можно выбирать разные соотношения длины последовательностей и размера словаря).\n",
    "\n",
    "Этот алгоритм работает в два этапа:\n",
    "\n",
    "- обучение - построение правил замены символов\n",
    "  - самая часто встречающаяся биграмма заменяется на новый символ, который раньше не использовался\n",
    "  - находится новая биграмма и всё повторяется\n",
    "- кодирование - применение построенных правил замены, чтобы сократить длину текста\n",
    "\n",
    "Это позволяет сильно сократить размер словаря и, при этом, не сильно увеличить длину входных цепочек токенов, в отличие от символьных языковых моделей (\"aabc aabaa\" -> \"Xbc XbX -> \"Yc YX\")\n",
    "\n",
    "**Преимущества:**\n",
    "- сжатое представление, обобщение. В построенных так моделях\n",
    "  - первые слои выделяют признаки (морфология, части речи и т.п.)\n",
    "  - средние слои отвечают за согласование слов (синтаксис)\n",
    "  - последние слои передают смысл (семантические роли, смыслы)\n",
    "  - То есть всё, практически, как в работе с изображениями — на каждом новом уровне мы абстрагируемся всё больше и больше, выделяем всё более крупные паттерны\n",
    "- Учет сложного контекста\n",
    "- Хорошая работа с неизвестными словами (если byte pair encoding)\n",
    "\n",
    "**Проблемы:**\n",
    "- вычислительная сложность (хорошая модель до сходимости требует несолько дней вычислений на десятках GPU)\n",
    "- математические прикольчики (RNN дают затухание/вызрыв градиента)\n",
    "- длина контекста слева ограничена (особенно если в основе CNN)\n",
    "\n",
    "Если строить авторегрессионную языковую модель с помощью **свёрточных нейросетей**, сложно учитывать широкий контекст, так как ширину рецептивного поля свёрточных нейросетей можно увеличивать только вместе с количеством параметров (добавляя слои).\n",
    "\n",
    "Если строить авторегрессионную языковую модель с помощью **рекуррентных нейросетей**, то с ростом длины последовательности острее проявляется проблема затухания градиента и забывания со временем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_5_'></a>[Двунаправленные языковые модели. ELMo](#toc0_)\n",
    "\n",
    "В русском языке свободный порядок слов. Авторегрессионная модель учитывает контекст только слева. А что, если сделать две авторегрессионных модели, которые будут получить один и тот же текст в противоположном порядке.\n",
    "\n",
    "$$P_{left}(t_1, t_2, ..., t_l) = \\prod_{j=1}^l P(t_j|t_{1:i-1}) \\\\\n",
    "P_{right}(t_1, t_2, ..., t_l) = \\prod_{j=1}^l P(t_j|t_{i+1:l})$$\n",
    "\n",
    "- При этом, признаки от обеих моделей можно использовать для лучшего решения прикладных задач. Скрытые состояния со всех шагов и со всех слоев используются как признаки для решения прикладной задачи\n",
    "\n",
    "Такой подход лежит в основе модели **ELMo**, которая была предложена в 2017-2018 годах.[1] \n",
    "\n",
    "[1] Peters M. E. et al. Deep contextualized word representations //arXiv preprint arXiv:1802.05365. – 2018. (https://arxiv.org/abs/1802.05365)  \n",
    "\n",
    "\n",
    "\n",
    "## <a id='toc2_6_'></a>[BERT](#toc0_)\n",
    "\n",
    "<img src=\"./img/bert.png\" width=\"700\">\n",
    "\n",
    "Однако можно пойти ещё дальше и учитывать не только контекст строго слева или строго справа, а вообще весь доступный текст, кроме некоторых слов. \n",
    "\n",
    "- Так как модель основана на **трансформере** (далее подробнее), у неё нет особых проблем с моделированием далёких зависимостей — это позволяет моделировать не только одно предложение, а сразу большой непрерывный фрагмент текста. \n",
    "  - 12 или 14 слоев\n",
    "- При обучении модели BERT, берётся фрагмент текста, в нём выбирается сколько-то слов, и они заменяются на фиктивный токен ([MASK]). \n",
    "- Модель должна предсказать эти слова, используя все остальные входные данные. Тут учитывается вообще весь текст. \n",
    "$$P(t_1^{mask}, t_2^{mask}, ..., t_m^{mask}|t_1^{input}, t_2^{input}, ..., t_n^{input})$$\n",
    "  \n",
    "**Преимущества:**\n",
    "- предсказывается еще и следующее предложение\n",
    "- учитывается контекст справа\n",
    "- SOTA, гибкость\n",
    "  \n",
    "Данная модель позволяет достичь лучших на настоящее время результатов на ряде задач, при этом не обязательно строить какие-то специальные архитектуры для каждой задачи — можно, не меняя общей архитектуры BERT, несколько до-обучить его на маленьком датасете, и задача уже будет решаться отлично. \n",
    "  \n",
    "**Проблемы:**\n",
    "- пропущенные слова [MASK] считаются условно-независимыми при наблюдении остальных слов. \n",
    "  $$P(t_1^{mask}, t_2^{mask}, ..., t_m^{mask}|t_{1:n}^{input}) = \\prod_{i=1}^m P(t_i^{mask}|t_{1:n}^{input})$$\n",
    "  - Это означает, что их можно предсказывать независимо друг от друга — это не вполне корректно, так как от выбора одного пропущенного слова может зависеть выбор второго. \n",
    "- т.к. процесс обучения BERT опирается на внесение шума во входные данные (добавляется фиктивный токен [MASK]) фактическое распределение входных данных не вполне соответствует настоящему. \n",
    "  - Это может препятствовать переносу на реальные данные. \n",
    "\n",
    "Ну, как бы то ни было, в целом эта модель работает отлично.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_7_'></a>[Модель XLNet](#toc0_)\n",
    "\n",
    "Позже была предложена модель XLNet[1], показывающая ещё лучшее качество, чем BERT: объединение авторегрессионных моделей и моделей, основанных на восстановлении зашумлённых данных — таких как BERT. \n",
    "\n",
    "Правило цепочки (chain rule) может быть представлено в виде различных последовательностей:\n",
    "\n",
    "$$P(a, b, c) = P(a|b, c) P(b|c) P(c) = P(b|a, c) P(c|a) P(a)$$\n",
    "\n",
    "- первый вариант соответствует цепочке C-B-A, \n",
    "- второй вариант соответствует цепочке A-C-B. \n",
    "- авторегрессионные модели (ELMp, OpenAI GPT) всегда слева-направо $P(t_1, t_2, ..., t_l) = \\prod_{i=1}^l P(t_i|t_{1:i-1})$\n",
    "- BERT использует сразу все входные данные — и слева, и справа: $P(t_1^{mask}, t_2^{mask}, ..., t_m^{mask}|t_{1:n}^{input}) = \\prod_{i=1}^m P(t_i^{mask}|t_{1:n}^{input})$ (но в итоге работает не совсем с тем текстом, который есть, а с зашумленным вариантом). \n",
    " \n",
    "XLNet: используется обычная авторегрессионная модель для предсказания слова за словом, но для каждого следующего обучающего примера использовать новый порядок факторизации. \n",
    "$$P(t_1, t_2, ..., t_l) = \\prod_{i=1}^l P(t_{order(i)}|t_{order(1:i-1)}), order \\in AllPermutations(1, .., l) $$\n",
    "\n",
    "XLNet основан на трансформере. Это позволяет очень удобно всё сделать за счёт передачи масок в механизм внимания. Это, на сегодняшний день, самая мощная языковая модель[2].\n",
    "\n",
    "[1] [Yang Z. et al. Xlnet: Generalized autoregressive pretraining for language understanding //Advances in neural information processing systems. – 2019. – С. 5754-5764.](https://arxiv.org/abs/1906.08237)   \n",
    "[2] Прогресс можно отслеживать на страницах [GLUE Benchmark](https://gluebenchmark.com/leaderboard) и [SuperGLUE Benchmark](https://super.gluebenchmark.com/leaderboard)\n",
    "\n",
    "    - уже не в топе\n",
    "    - в топе некая ансамблевая модель с трансформерами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример**\n",
    "\n",
    "Допустим, мы выбрали порядок факторизации 3 -> 1 -> 2 для предложения \"мама мыла раму\". Это означает, что будут формироваться следующие обучающие примеры (в формате \"Вход -> Эталонный выход\", в скобках после слова стоит его исходная позиция в тексте):\n",
    "\n",
    "    <start> -> раму(3)\n",
    "    <start> раму(3) -> мама(1)\n",
    "    <start> раму(3) мама(1) -> мыла(2)\n",
    "\n",
    "Так как порядок слов в исходном предложении очень важен для понимания его смысла, необходимо добавлять к эмбеддингу слов эмбеддинг его исходной позиции в тексте. Это называется **позиционным кодированием**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_8_'></a>[Актуальные архитектурные решения в языковых моделях](#toc0_)\n",
    "\n",
    "- модели, использующие сжатие текста и словаря через \"byte pair encoding\" и, при этом, использующие честный софтмакс (а не его аппроксимацию)\n",
    "\n",
    "- модель трансформер (заняла доминирующее положение), так как она легче обучается и способен моделировать далёкие зависимости\n",
    "  - в механизм внимания можно подавать маски и, тем самым, управлять зависимостями между словами\n",
    "  - лучше обучать модель на длинных входных последовательностях (вносит в модель информацию о связи предложений, о дискурсе)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Семинар в директории `./stepik-dl-nlp`"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75291dc0307ea48294888123147845d2e15abd18d38848ca6ac05a6fe8c88425"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
