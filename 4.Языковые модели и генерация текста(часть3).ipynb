{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Содержание**<a id='toc0_'></a>    \n",
    "- [Агрегация, механизм внимания. Трансформер и self-attention](#toc1_)    \n",
    "  - [Агрегация (pooling)](#toc1_1_)    \n",
    "    - [Пример агрегации (пуллинга)](#toc1_1_1_)    \n",
    "      - [Проблемы пуллинга:](#toc1_1_1_1_)    \n",
    "  - [Идея умного пуллинга - механизм внимания](#toc1_2_)    \n",
    "  - [Механизм внимания как набор компонентов](#toc1_3_)    \n",
    "  - [Популярные архитектуры механизма внимания](#toc1_4_)    \n",
    "  - [Сравнение текстов с механизмом внимания](#toc1_5_)    \n",
    "- [Трансформер и self-attention](#toc2_)    \n",
    "  - [Механизм внутреннего внимания (self-attention, intra attention)](#toc2_1_)    \n",
    "    - [Использование раздельных проекций для запросов, ключей и значений](#toc2_1_1_)    \n",
    "    - [Недостатки механизма внимания](#toc2_1_2_)    \n",
    "    - [Повышение качества механизма внутреннего внимания. Multihead self-attention](#toc2_1_3_)    \n",
    "  - [Управление зависимостями](#toc2_2_)    \n",
    "- [Трансформер](#toc3_)    \n",
    "  - [Позиционное кодирование](#toc3_1_)    \n",
    "- [Теоретические вопросы: Модель языка и трансформеры](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Агрегация, механизм внимания. Трансформер и self-attention](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "История о том, как получить один вектор, представляющий целое предложение или даже текст. То есть — как **агрегировать глобальный контекст**.\n",
    "\n",
    "## <a id='toc1_1_'></a>[Агрегация (pooling)](#toc0_)\n",
    "Вход:\n",
    "\n",
    "$$Length \\times EmbeddingSize$$\n",
    "\n",
    "Выход:\n",
    "\n",
    "$$NewLength \\times EmbeddingSize \\text{ или } EmbeddingSize$$\n",
    "\n",
    "Цель:\n",
    "\n",
    "- расширение контекста с потерей пространственной информации\n",
    "- учет глобального контекста\n",
    "  \n",
    "Агрегация (пуллинг) работают подобно свертке\n",
    "- проход по входу окном и выбор:\n",
    "  - среднего\n",
    "  - максимума\n",
    "- в отличие от свертки, применяется к каждому каналу (столбцы) независимо\n",
    "- никаких параметров кроме самого вида преобразования (среднее или максимум нету)\n",
    "- повторяется до тех пор пока не останется 1 вектор \n",
    "- в итоге учитывается весь контекст документа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_1_'></a>[Пример агрегации (пуллинга)](#toc0_)\n",
    "Операция агрегации (pooling) принимает на вход матрицу $Input \\in \\mathbb{R} ^ {InLen \\times EmbSize}$, где $InLen$ - количество строк, соответствующее длине входной последовательности, а $EmbSize$ - количество столбцов, соответствующее количеству признаков для каждого элемента.\n",
    "\n",
    "У агрегации два основных гиперпараметра:\n",
    "\n",
    "$k$ - размер окна (как для свёртки)\n",
    "$f(x): \\mathbb{R}^k \\rightarrow \\mathbb{R}$ - функция агрегации, как правило это нахождение среднего значения `avg(x)` или нахождение максимального значения `max(x)`\n",
    "\n",
    "Результат агрегации - новая матрица $Output \\in \\mathbb{R} ^ {OutLen \\times EmbSize}$, где $OutLen = InLen - k + 1$ - длина выходной последовательности.\n",
    "\n",
    "Тогда операцию агрегации можно записать следующей формулой: \n",
    "$$Output[OutPos, Ch] = f(Input[OutPos:OutPos + k, Ch])$$\n",
    "\n",
    "В этой формуле:\n",
    "\n",
    "$OutPos$ - номер строки в результирующей матрице (то есть порядковый номер элемента в выходной последовательности), \n",
    "$Ch$ - номер столбца (то есть номер признака), \n",
    "$Input[OutPos:OutPos + k, Ch] \\in \\mathbb{R}^k$ - вектор из $k$ значений признака $Ch$, соответствующих входным элементам последовательности начиная с позиции $OutPos$ до позиции $OutPos + k - 1$ включительно (то есть как `slice` в `NumPy`)\n",
    "\n",
    "Примените операцию max-пулинга с ядром $k=2$ к матрице $(InLen=3, EmbSize=3)$\n",
    "\n",
    "$$Input = \\left( \\begin{matrix} 1 & 0 & 2 \\\\ 0 & 1 & 3 \\\\ 1 & 3 & 0 \\end{matrix} \\right) \\in \\mathbb{R} ^ {InLen \\times EmbSize}$$\n",
    "\n",
    "Шаг скользящего окна (`stride` в `PyTorch`) считаем равным 1.\n",
    "\n",
    "**A:**\n",
    "\n",
    "$$Input = \\left( \\begin{matrix} 1 & 0 & 2 \\\\ 0 & 1 & 3 \\\\ 1 & 3 & 0 \\end{matrix} \\right) \\rightarrow \n",
    "\\left( \\begin{matrix} 1 & 1 & 3 \\\\ 1 & 3 & 3 \\end{matrix} \\right) \\rightarrow \n",
    "\\left( \\begin{matrix} 1 & 3 & 3 \\end{matrix} \\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 3.],\n",
       "         [1., 3., 3.]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "data = [[[1.0,0.0,2.0],\n",
    "         [0.0,1.0,3.0],\n",
    "         [1.0,3.0,0.0]]]            \n",
    "\n",
    "x_data = torch.tensor(data)                 # InLen X EmbSize\n",
    "m = nn.MaxPool1d(kernel_size=2, stride=1)\n",
    "\n",
    "m(x_data.transpose(1, 2)).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Такой пуллинг является \"**мягким**\" в смысле постепенных, но можно применять и **глобальный пуллинг** (ширина окна == размеру входа) - за 1 шаг получаем сразу вектор, т.е. простое среднее/максимум по всем входам\n",
    "- как правило глобальный пуллинг используют ближе к выходным слоям нейросети\n",
    "- после него обычно идет полносвязный слой, т.е. (мульти)линейный классификатор (например, для задачи классификации)\n",
    "\n",
    "#### <a id='toc1_1_1_1_'></a>[Проблемы пуллинга:](#toc0_)\n",
    "- каждый канал агрегируется независимо\n",
    "- амплитуда агрегированного значения отвечает и за значимость и за полезную информацию одновременно\n",
    "- это слишком слабое преобразование для многих задач (особенно глобальный вариант)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примените операцию глобального avg-пулинга к матрице.**\n",
    " \n",
    "$$avg(Input) = \\frac{1}{k} \\sum_{i=0}^{k-1} Input[i]$$\n",
    "\n",
    "$$Input = \\left( \\begin{matrix} 1 & 0 & 2 \\\\ 0 & 1 & 3 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & 0\\end{matrix} \\right) \\in \\mathbb{R} ^ {InLen \\times EmbSize} \\\\\n",
    "Pooled = \\rightarrow \\left( \\begin{matrix} 0.5 & 1 & 1.25 \\end{matrix} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.5000, 1.0000, 1.2500]]]), torch.Size([1, 4, 3]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[[1.0,0.0,2.0],\n",
    "         [0.0,1.0,3.0],\n",
    "         [1.0,3.0,0.0],\n",
    "         [0.0,0.0,0.0]]]            \n",
    "\n",
    "x_data = torch.tensor(data)                 # 1 x InLen X EmbSize\n",
    "m = nn.AvgPool1d(kernel_size=4, stride=1)\n",
    "\n",
    "m(x_data.transpose(1, 2)).transpose(1, 2), x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Идея умного пуллинга - механизм внимания](#toc0_)\n",
    "\n",
    "Аналогия:\n",
    "- входные векторы — это люди, которые что-то обсуждают вместе. \n",
    "- вы — один из участников такого обсуждения и вы хотите высказаться. \n",
    "- вы хотите чтобы вас услышали, но все вокруг бурно дискутируют, поэтому бессмысленно что-то пытаться говорить\n",
    "- сначала нужно привлечь к себе внимание, и убедиться, что вас слушают, \n",
    "- после этого можно говорить то что хотите сказать. \n",
    "\n",
    "Таким образом, гораздо более эффективно разделить операции **оценки значимости информации** (то есть, привлечение внимания), и **использования этой информации** (то есть, передачи дальше по нейросети)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более формально:\n",
    "\n",
    "1. На вход подаются вектора слов/токенов текста - матрица: \n",
    "$$ Words \\in \\R^{L \\times S}$$\n",
    "2. Для каждого вектора независимо расчитывается оценка значимости - **релевантность\\***, которая ничем не ограничена  (т.е. применяется какбэ \"нейросеть\" с 1 выходом, у которой все веса постоянны, например однослойная свертка Conv1d, но ничего не мешает применять более сложные преобразования). Считаем, что полученные оценки это Logit (логарифм вероятности/шансов, тут у меня путаница):\n",
    "$$UnnormScores = Net(Words) \\in \\R^L$$\n",
    "3. Полученные оценки нормируются софтмаксом к [0, 1] превращаясь в \"вероятности\", ну или веса:\n",
    "$$AttScores = SoftMax(UnnormScores) \\in \\R^L \\\\\n",
    "Softmax(x) = \\left\\{ \\frac{e^{x_i}}{\\sum_j e^{x_j}}\\right\\}_i$$\n",
    "4. Полученные вектор оценок/весов/вероятностей длинной по количеству слов в матрице характеризует важность каждого слова/токене в исходном тексте. Но его размерность не позволяет сравнивать тексты между собой, т.к. они разной длины, поэтому его нужно привести к единой размерности, которой в анализе текстов явялется размерность **эмбеддинга**. Для это этот вектор оценок слов умножается на матрицу слов:\n",
    "$$Result = AttScores_{1 \\times L} \\cdot Words_{L \\times S} \\in \\R^S$$\n",
    "\n",
    "\\*) Релеваантность (от англ. relevant — существенный, уместный) в информационной науке и информационном поиске означает степень соответствия найденного документа или набора документов информационным нуждам пользователя (Релевантность, Relevance).\n",
    "\n",
    "Другими словами, мы просто взяли взвешенную сумму эмбеддингов слов данного текста, а в качестве весов взяты нормированные к единице свертки этих эмбеддингов. \n",
    "\n",
    "Как здесь обеспечивается привлечение внимания к конкретному слову? Наверно настройкой свертки, т.к. параметрами ее ядра, которые и определяют важность тех или иных компонентов эмбеддинга. А т.к. на выходе нам нужно ровно одно число, то окно свертки (для случая одномерной свертки) это длина эмбеддинга и все определяется ядром, также размера эмбеддинга. А эначит что это ядро - это просто тот образец эмбеддинга, по близости к которому и оценивается важность векторов слов для текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примените механизм внимания к входной матрице:**\n",
    "\n",
    "$$Input = \\left( \\begin{matrix} 1 & 0 & 2 \\\\ 0 & 1 & 3 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & 0\\end{matrix} \\right) \\in \\mathbb{R} ^ {InLen \\times EmbSize}$$ \n",
    "\n",
    "с учётом оценок значимости $AttScores = \\left( \\begin{matrix} 0.1 & 0.5 & 0.3 & 0.1 \\end{matrix} \\right)$\n",
    "\n",
    "Технически, основное отличие механизма внимания от avg-пулинга - наличие весов при слагаемых: \n",
    "$$Attention[Ch] = \\sum_{i=0}^{InLen-1} AttScores[i] \\cdot Input[i,Ch]$$\n",
    "\n",
    "где $Input \\in \\mathbb{R} ^ {InLen \\times EmbSize}$ - матрица признаков входной последовательности, $Ch$ - номер столбца (номер признака), $AttScores[i]$ - оценка значимости для i-го элемента входной последовательности ($AttScores \\in \\mathbb{R}^{InLen}, 0 \\le AttScores[i] \\le 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4000, 1.4000, 1.7000]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ker = torch.tensor([0.1, 0.5, 0.3, 0.1])\n",
    "ker @ x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Механизм внимания как набор компонентов](#toc0_)\n",
    "\n",
    "Механизм включает следующие компоненты:\n",
    "\n",
    "1. Входные данные (Inputs)\n",
    "- матрица слов\n",
    "- вектор запрос (опционально должен уметь считать)\n",
    "2. Механизм расчета релевантности (UnnormScores)\n",
    "- как правило это простое скалярное произведение (с ядром/образцом) или нейросеть\n",
    "3. Механизм вычисления значений (AttScores)\n",
    "- обычно это софтмакс (нормирование к 1)\n",
    "4. Механизм получения единого вектора (агрегации)\n",
    "- обычно это взвешенная сумма, т.е. тоже скалярное произведение, то с AttScores\n",
    "\n",
    "Учитывая такую структурированность, придумана **масса вариантов** данного механизма в целом\n",
    "\n",
    "**Преимущества механизма внимания:**\n",
    "- большая гибкость по сравнению с avg/max агрегацией\n",
    "- значения результирующего вектора не являются независимыми, одни компоненты входов работают на привлечение внимания, а другие на передачу информации (видимо, имеется в виду про длину (внимание) и направление (информация) вектора))\n",
    "- лучше работают на длинных последовательностях (по сравнению с пуллингом и RNN)\n",
    "- представляет собой универсальную операцию, которая может заменить и RNN и CNN, можно всю нейросеть построить на механизме внимания\n",
    "- возможность интерпретации решений, принятых нейросетью, если добавить к ней этот механизм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Популярные архитектуры механизма внимания](#toc0_)\n",
    "\n",
    "1. Пропущенные через софтмакс оценки релевантности (вектор внимания) на последнем этапе умножается не на входы (исходные данные), а на входы, обработанные например сверточным слоем, выделяющим более явно информацию из исходных данных.\n",
    "   - т.е. входы пропускаются через 2 различные сверточные сети, одна дает оценку релевантности (внимание), которые называют \"**ключи**\", вторая выделяет признаки (информация), которые называют \"**значения**\"\n",
    "2. Часто нужно оценивать релевантность слов тексте не в рамках некоторой задачи классификации, а относительно некоторого запроса извне (еще некоторого слова или набора слов, но тогда его нужно как то, агрегацией или механизмом внимания, самого привести к форме одного вектора, чтоб работало скалярное умножение)\n",
    "   - в таких случаях релевантность оценивается простым скалярным произведением вектора запроса и векторов из входной матрицы. Это можно также записать как матричное произведение вектора на всю матрицу\n",
    "   - иначе говоря, чем ближе векторы входных слов к вектору запроса в смысле некоторой метрики (например, косинусной), тем более они значимы\n",
    "3. Можно объеденить оба этих варианта\n",
    "   - это даст больше гибкости\n",
    "4. И наконец, вектор запроса можно получать не извне, а из самого текста (например, глобальным пулингом)\n",
    "   - оценка значимости каждого слова будет обусловлена на весь текст (мы потеряем часть пространственной информации, но увеличим часть смысловой информации)\n",
    "   - технически, если это макс-пуллинг, то это получается чето типа усилителя - векторы слов увеличиваются пропорционально максимальным своим компонентам, если усреднение - то ембеддинги увеличиваются пропорционально близости к среднему\n",
    "\n",
    "Механизмы внимания, использующие в качестве **ключей** и **значений** элементы одного и того же текста ещё называются механизмами \"самовнимания\" (или \"self-attention\"). Такое внимание уже, в принципе, может заменить рекуррентную нейросеть в ряде задач.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример механима внимания**\n",
    "\n",
    "Рассмотрим механизм внимания, в котором релевантности элементов входной последовательности расчитываются с помощью скалярного произведения с фиксированным вектором-запросом.\n",
    "\n",
    "Как и раньше, $Input \\in \\mathbb{R} ^ {InLen \\times EmbSize}$ - входная матрица.\n",
    "\n",
    "Релевантности элементов входной последовательности можно посчитать с помощью матричного произведения $UnnormScores = Input \\cdot Query$, где $Query \\in \\mathbb{R}^{EmbSize}$ - вектор-запрос, характеризующий значимость признаков. Тогда $UnnormScores \\in \\mathbb{R}^{InLen}$ - вектор релевантностей элементов входной последовательности (чем больше $UnnormScores[i]$, тем значимее i-ый элемент).\n",
    "\n",
    "Затем релевантности нормируют: $AttScores = Softmax(UnnormScores)$.\n",
    "\n",
    "Ну и наконец, результат операции внимания считается по формуле из предыдущей задачи $Attention[Ch] = \\sum_{i=0}^{InLen-1} AttScores[i] \\cdot Input[i,Ch]$\n",
    "\n",
    "Примените механизм внимания к входной матрице\n",
    "\n",
    "$$Input = \\left( \\begin{matrix} 1 & 0 & 2 \\\\ 0 & 1 & 3 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & 0\\end{matrix} \\right)$$\n",
    "\n",
    "с учётом вектора-запроса $Query = \\left( \\begin{matrix} 0 & 0 & 1 \\end{matrix} \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28461991, 0.78323514, 2.54574246])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax = lambda x: np.exp(x) / np.exp(x).sum()\n",
    "\n",
    "x_data = np.array([[1,0,2],[0,1,3],[1,3,0],[0,0,0]])\n",
    "query = np.array([0,0,1])\n",
    "\n",
    "unnorm_scores  = x_data @ query.T\n",
    "att_scores = softmax(unnorm_scores)\n",
    "attention = att_scores @ x_data\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Сравнение текстов с механизмом внимания](#toc0_)\n",
    "\n",
    "Даны 2 текста длины $L_1, L_2$. \n",
    "\n",
    "Тексты построены с одинм эмбеддингом, поэтому можно их матрицы перемножить. Элемент этой матрицы $L_1 \\times L_2$ будет нести смысл:\n",
    "- $(i, j)$ элемент показывает, насколько i-ое это слово из первого текста похоже на j-ое слово из второго текста\n",
    "\n",
    "Есть 2 варианта — два измерения, по которым можно нормализовать (то есть, к которым можно применить софтмакс и получить $L_1 \\times L_2$ \"вероятностей\")  \n",
    "- если нормализуем по столбцам ($L_2$), т.е. умножим \"вероятности\" софтмакса на $L_1$, то, в результате, получим матрицу, физический смысл которой:\n",
    "  - размер будет соответствовать размеру второго текста, \n",
    "  - i-ый столбец ($EmbSize \\times 1$) будет хранить представление **всего** первого текста относительно i-го слова второго текста\n",
    "\n",
    "Такие представления могут быть полезны, например, в **вопросно-ответных системах**, когда первое предложение обозначает вопрос, а второе — текст, в котором мы ищем ответ. Тогда, после такой агрегации по каждому вектору, в полученной матрице мы можем, например, предсказать, содержится ли в районе этого слова ответ на вопрос, или нет. \n",
    "\n",
    "Или же мы можем нормализовать матрицу релевантности по строкам — тогда получим матрицу, соответствующую размеру первого предложения, и физический смысл отобразится зеркально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Трансформер и self-attention](#toc0_)\n",
    "\n",
    "<img src=\"./img/transformer.png\" width=\"500\">\n",
    "\n",
    "Трансформер и механизм внутреннего внимания (или self-attention) - модель, которая очень хорошо встряхнула всю область обработки текстов. \n",
    "\n",
    "Вспомним основные архитертуры НС для текста:\n",
    "\n",
    "- RNN\n",
    "  - последовательные вычисления (LSTM, GRU)\n",
    "  - улучшенный параллелизм (SimleRU)\n",
    "  - стоимость учета зависимости/закономерностей длины $n$ требуется $O(n)$ операций\n",
    "    - при этом важные закономерности в естественном языке носят нелокальный характер\n",
    "- CNN\n",
    "  - параллельные вычисления ОК\n",
    "  - длина учитываемых последовательностей (рецептивное поле) зависит от глубины сети и $k$\n",
    "    - либо увеличивать количество слоев\n",
    "    - либо увеличивать размер ядра свертки\n",
    "  - **непрерывные свертки** - стоимость учета зависимости/закономерностей длины $n$ требуется $O(n/k)$ операций\n",
    "  - **прореженные свертки** (dilated) - стоимость $O(\\log_k(n))$ операций\n",
    "\n",
    "Связанные с этим проблемы:\n",
    "- информация о далеких зависимостях теряется\n",
    "  - есть гипотеза о том, что  чем больше нам нужно сделать шагов, чтобы учесть какую-то зависимость, чтобы перенести информацию из одной точки в другую, тем выше вероятность того, что мы потеряем много информации на этом пути, и, следовательно, будем хуже учитывать далёкие зависимости\n",
    "- поэтому такие нейросети либо\n",
    "  -  медленно учатся, но компактнее (RNN)\n",
    "  -  лучше распараллеливаются, но требуют больше вычислений (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Механизм внутреннего внимания (self-attention, intra attention)](#toc0_)\n",
    "\n",
    "<img src=\"./img/self-att.png\" width=\"700\">\n",
    "\n",
    "Разработан давно, а раскручен примерно с 2017 года гуглом. \n",
    "\n",
    "В целом он похож на свертки, но учитывает зависимости любой длины и вычислительно намного дешевле.\n",
    "\n",
    "1. **Принимает на вход** матрицу токенов/признаков (ембеддинги), пусть вектора токенов - это столбцы:\n",
    "\n",
    "$$W_{EmbSize \\times Len}$$\n",
    "\n",
    "2. **Происходит учет контекста**\n",
    "\n",
    "$$W \\rightarrow Logits_{Len \\times Len} \\rightarrow Scores_{Len \\times Len}$$\n",
    "\n",
    "А именно:\n",
    "- в качестве векторов запроса используются все векторы  $W$, т.е. оценивается сходство токенов - каждый с каждым:\n",
    "$$Logits_{Len \\times Len} = W^T \\cdot W$$\n",
    "- с помощью софтмакса, нормируется по строкам или по столбцам (она сирамно симметичная) так, чтобы сумма весов по строке или столбцу равнялась единице (допустим по столбцам):\n",
    "$$AttScores = Softmax(Logits, col)$$\n",
    "- затем исходные векторы выступают в роли значений. Они взвешиваются с помощью полученных весов и складываются:\n",
    "$$Result = W \\cdot AttScores$$\n",
    "3. **Возвращает** результирующию матрицу, где каждый токен представлен вектором признаков в контексте всех остальных токенов:\n",
    "$$Result_{Len \\times EmbSize} = SelfAttention = W \\cdot Softmax(W^T \\cdot W, col)$$\n",
    "\n",
    "Это самый простой вариант внутреннего внимания, он демонстрирует основной принцип работы\n",
    "- и нет ли тут путаницы с размерностями (W надо в конец формулы?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[Использование раздельных проекций для запросов, ключей и значений](#toc0_)\n",
    "\n",
    "На практике обычно добавляется один шаг - из матрицы токенов строиться три различные проекции для пролучения векторов запросов, ключей (оценок релевантности токенов) и значений (оценок информационных признаков токенов):\n",
    "\n",
    "$$Q = Proj_Q \\cdot W \\\\ K = Proj_K \\cdot W \\\\ V = Proj_V \\cdot W $$\n",
    "\n",
    "Часто используется простое линейное преобразование — домножение на матрицу. Здесь матрицы проекции — квадратные, и длина их стороны равняется размеру эмбеддинга токена.\n",
    "\n",
    "Это даёт гораздо больше гибкости и при расчёте релевантности токен будет иметь разные признаки, в зависимости от того, выступает он в качестве запроса или в качестве ключа. \n",
    "\n",
    "Такое преобразование можно понимать как применение одномерной свёртки с ядром размера 1 (`Conv1d(kernel=1)`). \n",
    "\n",
    "Аналогично может преобразовываться и результат:\n",
    "\n",
    "$$Output = Proj_O \\cdot W $$\n",
    "Итого:\n",
    "$$SelfAttention = V \\cdot Softmax(K^T \\cdot Q, col) =\\\\ \n",
    "= Proj_V \\cdot W \\cdot Softmax(W^T \\cdot Proj_K^T \\cdot Proj_Q \\cdot W, col)$$\n",
    "\n",
    "**Это и есть механизм внутреннего внимания - основной строительный блок трансформера** (и многих других современных архитектур для обработки текста)\n",
    "\n",
    "В ориг. статье это выглядит как: $A(Q,K,V) = Softmax(\\frac{Q \\cdot K^T}{\\sqrt {d_k}})\\cdot V$, где $d_k$ - размерность вектора ключей (равна $d_q$ если че). Такое нормирование, чтобы при больших размерностях в $Q \\cdot K^T$ не возникало слишком больших значений, которые на софтмаксе забьют остальные - высокая контрастность.\n",
    "\n",
    "Для одного запроса: $A(q,K,V) = \\sum_i \\frac{e^{q \\cdot k_i}}{\\sum_j e^{q \\cdot k_j}})\\cdot v_i$\n",
    "\n",
    "\n",
    "### <a id='toc2_1_2_'></a>[Недостатки механизма внимания](#toc0_)\n",
    "- **теряет информацию**: операция усреднения, пусть даже и с взвешиванием (чем является матричное произведение) - это достаточно грубая операция, много информации теряется\n",
    "- **усложняет сходимость**: может концентрировать внимание только на одном аспекте сравнения:\n",
    "  - т.е. внимание получит только один токен в результате единственного сравнения всех со всем единственным способом, хотя похожесть токенов может проявляться по разному\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример**\n",
    "\n",
    "Механизм self-attention принимает на вход матрицу $Input \\in \\mathbb{R} ^ {InLen \\times EmbSize}$, где $InLen$ - количество строк, соответствующее длине входной последовательности, а $EmbSize$ - количество столбцов, соответствующее количеству признаков для каждого элемента.\n",
    "\n",
    "Найдите матрицу попарного сходства элементов $Logits=Input \\cdot Input^T$.\n",
    "\n",
    "Входная матрица имеет следующий вид\n",
    "\n",
    "$$Input = \\left( \\begin{matrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\\\ 0 & 0\\end{matrix} \\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 1 0\n",
      "0 1 1 0\n",
      "1 1 2 0\n",
      "0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 0],[0, 1],[1, 1],[0, 0],]).T  # шоб как в формуле\n",
    "\n",
    "logits = x.T @ x\n",
    "for s in logits.tolist():\n",
    "    print(*s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример**\n",
    "\n",
    "Используя матрицу $Logits$, полученную в результате решения предыдущей задачи, найдите выходную матрицу $Result \\in \\mathbb{R}^{InLen \\times EmbSize}$ для механизма **self-attention**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7310585786300049 0.5\n",
      "0.5 0.7310585786300049\n",
      "0.7310585786300049 0.7310585786300049\n",
      "0.5 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_scores = np.vectorize(softmax, signature='(d)->(d)')(logits)\n",
    "result = att_scores @ x.T\n",
    "\n",
    "for s in result.tolist():\n",
    "    print(*s)\n",
    "    \n",
    "result.shape == x.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример**\n",
    "\n",
    "Посмотрим, как работает более общий вариант self-attention - когда в качестве ключей, запросов и значений используются разные матрицы. Вход тот же, что и выше. Алгоритм еще раз:\n",
    "\n",
    "1. Найти значения ключей, запросов и значений, используя линейное преобразование\n",
    "$$Keys = Input \\cdot Proj_K + Bias_K \\\\ \n",
    "Queries = Input \\cdot Proj_Q + Bias_Q \\\\\n",
    "Values = Input \\cdot Proj_V + Bias_V$$\n",
    " \n",
    "2. Найти матрицу попарного сходства, используя полученные матричное произведение запросов и ключей $Logits=Queries \\cdot Keys^T$\n",
    "3. Найти коэффициенты усреднения, нормировав матрицу попарного сходства с помощью softmax по строкам \n",
    "$AttScores = softmax(Logits, rows)$\n",
    "4. Найти результат с помощью матричного произведения матриц значений и коэффициентов $Result=AttScores \\cdot Values$\n",
    "\n",
    "вания:\n",
    "\n",
    "Пусть:\n",
    "\n",
    "$$Proj_K = \\left(\\begin{matrix}1 & 0 \\\\ 0 & 0\\end{matrix} \\right), Proj_Q = \\left(\\begin{matrix}0 & 0 \\\\ 1 & 0\\end{matrix} \\right), Proj_V = \\left(\\begin{matrix}1 & 0 \\\\ 0 & 1\\end{matrix} \\right) \\\\\n",
    "\n",
    "Bias_K = Bias_Q = Bias_V = \\left(\\begin{matrix}0 & 0\\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n",
      "0.7310585786300049 0.5\n",
      "0.7310585786300049 0.5\n",
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "proj_k = np.array([[1, 0], [0, 0]])\n",
    "proj_q = np.array([[0, 0], [1, 0]])\n",
    "proj_v = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "bias_k = bias_q = bias_v = np.array([[0,0]])\n",
    "\n",
    "keys = x.T @ proj_k + bias_k\n",
    "queries = x.T @ proj_q + bias_q\n",
    "values = x.T @ proj_v + bias_v\n",
    "\n",
    "logits = queries @ keys.T\n",
    "\n",
    "att_scores = np.vectorize(softmax, signature=\"(d)->(d)\")(logits)    # сумма по строкам == 1\n",
    "result = att_scores @ values\n",
    "\n",
    "for s in result.tolist():\n",
    "    print(*s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[Повышение качества механизма внутреннего внимания. Multihead self-attention](#toc0_)\n",
    "\n",
    "Вместо одного механизма внимания предлагается запускать несколько (Heads, \"головы\"), с различными весами (матрицами проекций), причем **меньшей размерности**, чем эмбеддинг исходного текста.\n",
    "\n",
    "Таким образом Каждая \"голова\" работает с пространством признаков меньшего размера. Потом, на выходе, они конкатенируются — так, что размер выходной матрицы остаётся прежним — таким же, каким и был на входе. Именно такой вариант внутреннего внимания и используется в трансформере (и в большинстве современных архитектур).\n",
    "\n",
    "Эффективный размер векторов внутри каждой \"головы\" - $EmbSize // HeadsN$. В целом о количестве \"голов\" надо думать в духе \"в каком количестве разных аспектов я хочу сравнивать слова друг с другом?\". Даже в больших моделях \"голов\" не очень много, 6-12-16.\n",
    "\n",
    "- получается эмбеддинг токена анализируется по-частям, кусками его упорядоченных координат. Это дает то, что взвешивание релевантности токенов нормируется не по всем координатам эмбеддинга, а по частям и одни части не забивают другие (усиливаются слабые части)\n",
    "\n",
    "В ориг.статье: \n",
    "$$head_i= Attention(QW_i^Q, KW_i^K, VW_i^V) \\\\\n",
    "MultiHead(Q,K,V)=Concat(head_1, ..., head_h)\\cdot W^O$$\n",
    "\n",
    "<img src=\"./img/heads.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример multihead self-attention**\n",
    "\n",
    "Общий алгоритм - точно такой же, как и в предыдущей задаче. Отличие в том, что нам нужно несколько раз применить механизм внимания с разными параметрами преобразований $Result^i = SelfAttention(Input, Proj^i_K, Proj^i_Q, Proj^i_V)$, $Result^i \\in \\mathbb{R}^{InLen \\times \\frac{EmbSize} {HeadsN}}$.\n",
    "\n",
    "Результат $MHResult \\in \\mathbb{R} ^ {InLen \\times EmbSize}$ получается конкатенацией $Result^i$ по столбцам: $MHResult = \\left[ Result^1, Result^2, ..., Result^{HeadsN} \\right]$.\n",
    "\n",
    "Вам требуется найти $MHResult$ для входных данных \n",
    "\n",
    "$Input = \\left( \\begin{matrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\\\ 0 & 0\\end{matrix} \\right)$​\n",
    " \n",
    "с учётом количества \"голов\" $HeadsN = 2$ и параметров преобразований\n",
    "\n",
    "$$Proj^1_K = \\left(\\begin{matrix}1 & 0 \\\\ 0 & 0\\end{matrix} \\right), Proj^2_K = \\left(\\begin{matrix}0 & 0 \\\\ 1 & 0\\end{matrix} \\right)$$\n",
    "\n",
    "$$Proj^1_Q = \\left(\\begin{matrix}0 & 1 \\\\ 1 & 0\\end{matrix} \\right), Proj^2_Q = \\left(\\begin{matrix}1 & 1 \\\\ 1 & 1\\end{matrix} \\right)$$\n",
    "\n",
    "$$Proj^1_V = \\left(\\begin{matrix}1 \\\\ 0\\end{matrix} \\right), Proj^2_V = \\left(\\begin{matrix}0 \\\\ 1\\end{matrix} \\right)$$\n",
    "\n",
    "$$Bias^i_K = Bias^i_Q = \\left(\\begin{matrix}0 & 0\\end{matrix}\\right), \\quad Bias^i_V = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.7310585786300049\n",
      "0.7310585786300049 0.7310585786300049\n",
      "0.7310585786300049 0.8807970779778824\n",
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "proj_k_1 = np.array([[1, 0], [0, 0]])\n",
    "proj_q_1 = np.array([[0, 1], [1, 0]])\n",
    "proj_v_1 = np.array([[1, 0]]).T\n",
    "\n",
    "proj_k_2 = np.array([[0, 0], [1, 0]])\n",
    "proj_q_2 = np.array([[1, 1], [1, 1]])\n",
    "proj_v_2 = np.array([[0, 1]]).T\n",
    "\n",
    "bias_k_1 = bias_k_2 = bias_q_1 = bias_q_2 = np.array([[0,0]])\n",
    "bias_v_1 = bias_v_2 = np.array([[0]])\n",
    "\n",
    "# head1\n",
    "keys_1 = x.T @ proj_k_1 + bias_k_1      # 4 x 2\n",
    "queries_1 = x.T @ proj_q_1 + bias_q_1   # 4 x 2\n",
    "values_1 = x.T @ proj_v_1 + bias_v_1    # 4 x 1\n",
    "logits_1 = queries_1 @ keys_1.T         # 4 x 4\n",
    "att_scores_1 = np.vectorize(softmax, signature=\"(d)->(d)\")(logits_1)    # 4 x 4\n",
    "result_1 = att_scores_1 @ values_1      # 4 x 1\n",
    "\n",
    "# head2\n",
    "keys_2 = x.T @ proj_k_2 + bias_k_2      # 4 x 2\n",
    "queries_2 = x.T @ proj_q_2 + bias_q_2   # 4 x 2\n",
    "values_2 = x.T @ proj_v_2 + bias_v_2    # 4 x 1\n",
    "logits_2 = queries_2 @ keys_2.T         # 4 x 4\n",
    "att_scores_2 = np.vectorize(softmax, signature=\"(d)->(d)\")(logits_2)    # 4 x 4\n",
    "result_2 = att_scores_2 @ values_2      # 4 x 1\n",
    "\n",
    "result = np.hstack((result_1, result_2))\n",
    "for s in result.tolist():\n",
    "    print(*s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Управление зависимостями](#toc0_)\n",
    "\n",
    "Допустим, что мы делаем модель для генерации текста и хотим предсказывать следующее слово на основе предыдущих и чтобы остальные слова, которые есть на входе сети, вообще никак не учитывались. Если мы генерируем третье слово, то мы хотим чтобы учитывались только первое и второе.\n",
    "\n",
    "В механизме внимания для работы с подобными ситуациями используется **маски**.\n",
    "\n",
    "**Маска** — это матрица такой же размерности, что и маска \"сходство токенов\" ($InLen \\times InLen$), элементы которой на $i$-ой/$j$-ой позиции, если для $j$-го токена надо учитывать $i$-ый токен, равные 1, если не надо учитывать - равны 0.\n",
    "\n",
    "**Маска применяется к $Logits$, т.е. перед нормализацией матрицы сходства, т.е. перед софтмаксом.**\n",
    "\n",
    "Тогда \n",
    "\n",
    "$$MaskedLogits_{ij} = \\left\\{ \\begin{array}{ll}\n",
    "      Logits_{ij}, & Masks_{ij}=1 \\\\\n",
    "      -\\inf, & Masks_{ij}=0 \\\\\n",
    "\\end{array} \\right. $$\n",
    "\n",
    "Применение софтмакса к матрице с \"минус бесконечностями\" приводит к тому, что на их месте появляются нули.\n",
    "\n",
    "$$Softmax([-\\inf, 1, 5]) = [0, 0.3, 0.7]$$\n",
    "\n",
    "Механизм маскирования делает архитектуру с внутренним вниманием очень гибкой и практически универсальной. Маски можно менять динамически, для каждого слова можно формировать маску независимо от других, и так далее. Этим свойством трансформера пользуются авторы некоторых крутых архитектур, которые сейчас показывают самое лучшее качество решения многих задач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Трансформер](#toc0_)\n",
    "\n",
    "Включает несколько слоев:\n",
    "\n",
    "1. Вход\n",
    "2. Механизм внимания для учёта глобального контекста\n",
    "3. Сверточные слои:\n",
    "   - признаки каждого токена независимо преобразовываются с помощью двухслойной нейросети\n",
    "   - ко всем токенам сеть применяется с одними и теми же параметрами\n",
    "   - по смыслу это похоже на одномерные свёртки с размером ядра \"1\".\n",
    "4. Ещё тут есть связи в обход нелинейностей, как в ResNet для классификации изображений — это ускоряет процесс обучения за счёт лучшего протекания градиентов (к выходу механиза внимания, а также к выходу сверток добавляется их выход)\n",
    "5. Слои 2-4 повторяются несколько раз\n",
    "6. Выход\n",
    "\n",
    "Но, в таком виде механизм внимания ничего не знает о порядке слов!\n",
    "   - RNN обрабатывают текст последовательно, такой проблемы нет\n",
    "   - CNN проходят по тексту окном, и знают соседние слова в окне, такой проблемы почти нет\n",
    "   - в трансформере информация о порядке слов погружается непосредственно в эмбеддинг\n",
    "\n",
    "## <a id='toc3_1_'></a>[Позиционное кодирование](#toc0_)\n",
    "\n",
    "1. **Кодирование позиции** (Обучаемые коды ?). Как вариант - просто складывать:\n",
    "$$W(t_i) = Emb(t_i) + PosCode(t_i), \\\\\n",
    "PosCode(t_i) \\in \\R^{EmbSize}$$\n",
    "\n",
    "- часто это не вариант, т.к. кодируется только фиксированное количество позиций (например, сеть обучалась на примерах длины не более 10, она не поймет никаких других позиций), \n",
    "\n",
    "2. **Периодический (синусоидный) сигнал**\n",
    "- подмешивать (прибавлять) его к эмбеддингу, так сеть будет узнавать/запоминать положение данного токена в тексте в рамках некоторого цикла. Какая-то такая штука (вектор гармоник какой-то):\n",
    "\n",
    "$$PosCode(t_i) = \\left[\\begin{matrix} f(t_i)^{(1)} \\\\ f(t_i)^{(1)}  \\\\ ... \\\\ f(t_i)^{(k)} \\\\ f(t_i)^{(k)} \\\\... \\\\ f(t_i)^{(d/2)} \\\\ f(t_i)^{(d/2)} \\end{matrix} \\right]_{d \\times 1} = \\left[\\begin{matrix} sin(\\omega_1 \\cdot i) \\\\ cos(\\omega_1 \\cdot i)  \\\\ ... \\\\ sin(\\omega_k \\cdot i) \\\\ cos(\\omega_k \\cdot i) \\\\... \\\\ sin(\\omega_{d/2} \\cdot i) \\\\ cos(\\omega_{d/2} \\cdot i) \\end{matrix} \\right]_{d \\times 1} $$\n",
    "\n",
    "где\n",
    "$$f(t_i)^{(k)} = \\left\\{ \\begin{array}{ll}\n",
    "      sin(\\omega_k \\cdot i), & i = 2k \\\\\n",
    "      cos(\\omega_k \\cdot i), & i = 2k+1\\\\\n",
    "\\end{array} \\right. \\\\\n",
    "\\omega_k = \\frac{1}{10000^{2k/d}}$$\n",
    "\n",
    "- $d$ - размерность эмбеддинга, \n",
    "- через $k$ мы определяем значение частоты и начальной фазы (т.е. синус или косинус) для $i$-того элемента в векторе $PosCode(t_i)$, \n",
    "- $i$ - это номер токена.\n",
    "\n",
    "В семинаре - пример, на примере - все просто.\n",
    "\n",
    "На практике эти два подхода — обучаемые коды и периодически сигнал — работают примерно одинаково, но сигнал гораздо удобнее использовать. Вот так выглядит общая архитектура трансформера, как её нарисовали авторы. Они изначально решали задачу машинного перевода, то есть генерации предложения по предложению на другом языке. \n",
    "\n",
    "Поэтому их архитектура состоит из энкодера (мы его уже рассматривали) и декодера. Декодер отличается тем, что в нём используются маски. А ещё, в него подаются выходы энкодера. Но, в остальном, это практически одна и та же нейросеть (но параметры, конечно же, у них разные). И в энкодере, и в декодере, используется много слоёв вот такого типа — в оригинальной статье по 6, а потом стали делать ещё больше — по 12, по 24... \n",
    "\n",
    "Такая архитектура позволяет решать задачи, связанные с анализом последовательностей (не только с текстами), часто — лучше, чем это получается делать с помощью рекурренток, и, при этом, можно тратить меньше времени на обучение.\n",
    "\n",
    "[1] [Attention and its Different Forms](https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc)  \n",
    "[2] [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Семинар в директории `./stepik-dl-nlp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Теоретические вопросы: Модель языка и трансформеры](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию, реализующую max-pooling с заданной шириной окна. В качестве пособия можно использовать другую задачу.\n",
    "\n",
    "Функция должна возвращать два тензора одинаковой размерности $OutLen \\times EmbSize$:\n",
    "\n",
    "- первый тензор - основной результат - результат применения max-пулинга к каждому столбцу для каждой позиции скользящего окна\n",
    "- второй тензор - информация, нужная для расчёта градиента, - относительные индексы максимальных элементов для каждого столбца для каждой позиции скользящего окна\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8106187774962709, 0.4395507898340978, 0.8290096270213004, 0.20134327995534496], [0.938964520620817, 0.4395507898340978, 0.8290096270213004, 0.07860176987690048], [0.938964520620817, 0.6411287103553837, 0.509253427569025, 0.7094441369109541], [0.691552879511939, 0.6411287103553837, 0.509253427569025, 0.7631262538014161]]\n",
      "[[1, 1, 1, 0], [1, 0, 0, 1], [0, 1, 1, 1], [1, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "SAMPLES = \"\"\"[[0.6046018907385543, 0.0812964077275945, 0.6366439552273822, 0.20134327995534496], [0.8106187774962709, 0.4395507898340978, 0.8290096270213004, 0.05773841312522798], [0.938964520620817, 0.3860407857274528, 0.21318174478828456, 0.07860176987690048], [0.04840110723428537, 0.6411287103553837, 0.509253427569025, 0.7094441369109541], [0.691552879511939, 0.4979735285634219, 0.07060470682483455, 0.7631262538014161]]\\n2\"\"\", \n",
    "READER = (x for x in SAMPLES[0].split('\\n')); \n",
    "sys.stdin.readline = lambda: next(READER)       # тупо шоп не переписывать\n",
    "\n",
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_array(s):\n",
    "    return np.array(ast.literal_eval(s))\n",
    "\n",
    "def read_array():\n",
    "    return parse_array(sys.stdin.readline())\n",
    "\n",
    "def write_array(arr):\n",
    "    print(repr(arr.tolist()))\n",
    "\n",
    "\n",
    "def max_pooling(features, kernel_size):\n",
    "    \"\"\"\n",
    "    features - InLen x EmbSize - features of elements of input sequence\n",
    "    kernel_size - positive integer - size of sliding window\n",
    "\n",
    "    returns tuple of two matrices of shape OutLen x EmbSize:\n",
    "         - output features (main result)\n",
    "         - relative indices of maximum elements for each position of sliding window\n",
    "    \"\"\"\n",
    "    InLen = features.shape[0]\n",
    "    KernelSize = kernel_size\n",
    "    OutLen = InLen - KernelSize + 1\n",
    "\n",
    "    sliding_indexer = np.expand_dims(np.arange(KernelSize), 0) + \\\n",
    "                      np.expand_dims(np.arange(OutLen), 0).T   # KernelSize x OutLen\n",
    "\n",
    "    features_slide = features[sliding_indexer]\n",
    "    max_pool = np.max(features_slide, axis=1)\n",
    "    indicies = np.argmax(features_slide, axis=1)\n",
    "    \n",
    "    # обратная операция - если в argmax не доступен keepdims (numpy<1.22.0)\n",
    "    # np.take_along_axis(features_slide, np.expand_dims(indicies, axis=1), axis=1)  \n",
    "\n",
    "    return max_pool, indicies\n",
    "\n",
    "\n",
    "features = read_array()\n",
    "kernel_size = int(sys.stdin.readline())\n",
    "\n",
    "result, indices = max_pooling(features, kernel_size)\n",
    "\n",
    "write_array(result)\n",
    "write_array(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прямому проходу по модулю max-пулинга была посвящена предыдущая задача.\n",
    "\n",
    "Теперь займёмся обратным проходом: напишите функцию, вычисляющую производную функции потерь по входам модуля max-пулинга $\\frac{\\partial Loss}{\\partial features}$\n",
    "\n",
    "Функция принимает следующие аргументы:\n",
    "\n",
    "- $features \\in \\mathbb{R}^{InLen \\times EmbSize}$ - признаки, которые были переданы на вход модулю при прямом проходе\n",
    "- $2 \\leq kernel\\_size \\leq InLen$ - размер скользящего окна\n",
    "- $indices \\in \\mathbb{N}^{OutLen \\times EmbSize}, \\quad 0 \\leq indices < kernel\\_size$ - относительная позиция максимального элемента внутри скользящего окна для каждого элемента выходного тензора (смещение относительно номера строки)\n",
    "- $dldout = \\frac{\\partial Loss} {\\partial out} \\in \\mathbb{R}^{OutLen \\times EmbSize}$ - значение производной функции потерь по выходам слоя max-пулинга (то есть производная по входам следующего слоя)\n",
    "\n",
    "Вам может быть полезна формула производной кусочно-линейной функции \n",
    "\n",
    "$$ReLU(x) = \\begin{cases} 0 & if \\quad x \\leq 0 \\\\ x & otherwise \\end{cases} \\\\\n",
    " \\frac{\\partial ReLU(x)} {\\partial x} = \\begin{cases} 0 & if \\quad x \\leq 0 \\\\ 1 & otherwise \\end{cases}$$ \n",
    "\n",
    "А также помните про правило цепочки: $\\frac{\\partial Loss} {\\partial features} = \\frac{\\partial Loss} {\\partial out} \\frac{\\partial out} {\\partial features}$.\n",
    "\n",
    "**Получилось не очень...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.7454337173675266, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, -2.1273406024899657, 0.0, 0.0], [-0.0763791951131031, 0.0, 0.0, 0.0], [0.0, 0.0, -1.0015622079642417, -2.508470377801969], [0.0, 0.0, 0.0, 0.0], [-0.9080189656976042, 0.0, 0.0, 2.2803247286304087], [-0.7771529142648868, 0.0, 0.0, 0.0], [0.0, -2.371055190136431, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "SAMPLES = \"\"\"[[-1.2420542766989977, -0.045100789663994285, 1.858151857421511, 0.10732741246325356], [-1.480497780371414, -0.12486054931133332, -0.18422425981847368, -1.4228130362490647], [-0.8417536968892625, 0.9802583655274091, -0.18413492661665792, -1.5582607186399924], [1.325799250424393, 0.08149768959330334, -1.454876921308986, 0.1408031456023352], [0.1637602967235608, -0.21250114632967532, 0.8362859721448469, 0.717774697701287], [-0.7641399532198978, -2.112568530488304, 0.20121440705964902, 0.015624280892385661], [1.3862200103422582, 0.6508694196448389, -1.162417318743681, 1.5202488401790915], [1.3947418297193952, -1.013483406336198, -2.0608332074129545, -1.733019236247151], [1.0932612618870112, 0.8071262618398916, 0.15924519176972282, -0.6885825807454318]]\\n6\\n[[3.0, 2.0, 0.0, 4.0], [5.0, 1.0, 3.0, 5.0], [5.0, 0.0, 2.0, 4.0], [4.0, 5.0, 1.0, 3.0]]\\n[[-0.0763791951131031, -0.8729161683329371, 0.7454337173675266, -2.508470377801969], [-0.9080189656976042, 0.6952579391985969, 0.2829942797947518, 0.35396918585149195], [0.339009358836277, -1.9496823733556254, -0.11017174942549533, 1.4591363247582954], [-1.1161622731011638, -2.371055190136431, -1.174384738333498, 0.4672192180206214]]\"\"\", \\\n",
    "          \"\"\"[[-4.15901891e-02, -1.56756601e+00, -1.23642116e+00],[-1.50168154e-02,  2.62078972e-02, -9.12503559e-01],  [-8.97504443e-01,  4.80590968e-01, -1.53892657e+00],  [-7.95696114e-01,  8.81377550e-04, -7.66638264e-02],  [-1.38615994e+00, -1.58366520e+00, -5.78961762e-01],  [ 4.34240133e-01,  9.26438834e-01,  7.38135813e-01]]\\n5\\n[[1, 2, 3],[4, 4, 4]]\\n[[-1.13774517, -1.3217654 , -0.45930326],  [ 0.05638115,  0.77569495, -0.02360533]]\"\"\", \n",
    "READER = (x for x in SAMPLES[0].split('\\n')); \n",
    "sys.stdin.readline = lambda: next(READER)  # тупо шоп не переписывать\n",
    "\n",
    "\n",
    "def max_pooling_dldfeatures_fast(features, kernel_size, indices, dldout):\n",
    "    \"\"\"Хорошее решение\"\"\"\n",
    "    doutdfeatures = np.zeros(features.shape)\n",
    "    out_len = indices.shape[0] \n",
    "    for i in range(kernel_size):\n",
    "        doutdfeatures[i:i+out_len] += (indices == i) * dldout\n",
    "    return doutdfeatures\n",
    "\n",
    "def max_pooling_dldfeatures_fasta(features, kernel_size, indices, dldout):\n",
    "    result = np.zeros(features.shape)\n",
    "    for i in range(features.shape[0] - kernel_size + 1):\n",
    "        result[indices[i, :]+i, range(features.shape[1])] += dldout[i,:]\n",
    "    return result\n",
    "\n",
    "def max_pooling_dldfeatures(features, kernel_size, indices, dldout):\n",
    "    \"\"\"\n",
    "    features - InLen x EmbSize - features of elements of input sequence\n",
    "    kernel_size - positive integer - size of sliding window\n",
    "    indices - OutLen x EmbSize - relative indices of maximum elements for each window position\n",
    "    dldout - OutLen x EmbSize - partial derivative of loss function with respect to outputs of max_pooling layer\n",
    "\n",
    "    returns InLen x EmbSize\n",
    "    \"\"\"\n",
    "    in_len = features.shape[0]\n",
    "    out_channels = indices.shape[0]\n",
    "    \n",
    "    # None в индексах добавляет пустую размерность, нужно для броадкаста \n",
    "    abs_indices = indices + np.arange(out_channels)[None,:].T\n",
    "    loc_indices = np.zeros_like(features) + np.arange(in_len)[None,:].T\n",
    "    mask = loc_indices == abs_indices[:, None] \n",
    "    dldf_by_out_channel = mask * dldout[:, None]\n",
    "\n",
    "    return np.sum(dldf_by_out_channel, axis=0)\n",
    "\n",
    "\n",
    "features = read_array()\n",
    "kernel_size = int(sys.stdin.readline())\n",
    "indices = read_array().astype('uint32')\n",
    "dldout = read_array()\n",
    "\n",
    "dldfeatures = max_pooling_dldfeatures(features, kernel_size, indices, dldout)\n",
    "\n",
    "write_array(dldfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, [(900, 400), (601, 400), (601, 400)])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale = 100\n",
    "\n",
    "# avoid caching\n",
    "x = np.random.random_sample([scale*s for s in features.shape])\n",
    "ker_size = kernel_size * scale // 2\n",
    "dldo, ind = max_pooling(x, ker_size)\n",
    "\n",
    "ker_size, [i.shape for i in (x, ind, dldo)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 418 ms, sys: 5.61 ms, total: 423 ms\n",
      "Wall time: 440 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tmp0 = max_pooling_dldfeatures_fast(x, ker_size, ind, dldo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.8 ms, sys: 2.41 ms, total: 52.2 ms\n",
      "Wall time: 68.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tmp1 = max_pooling_dldfeatures_fasta(x, ker_size, ind, dldo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.16 s, sys: 297 ms, total: 1.46 s\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tmp2 = max_pooling_dldfeatures(x, ker_size, ind, dldo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(tmp1, tmp0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектор-функция $softmax(x) = \\left( \\begin{matrix} \\frac{e^{x_1}}{\\sum_j e^{x_j}} & ... & \\frac{e^{x_n}}{\\sum_j e^{x_j}} \\end{matrix} \\right)$ - популярный способ нормировать вектор чисел $x \\in \\mathbb{R}^n$ так, чтобы $0 \\leq softmax_i(x) \\leq 1$ и $\\sum_i softmax_i(x) = 1$.\n",
    "\n",
    "Напишите функцию, вычисляющую softmax для заданного вектора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06860598204389033, 0.03703718180652343, 0.06169051603553571, 0.006999663809155501, 0.02530593948353462, 0.02723806143745914, 0.040806327204274295, 0.019262891730251305, 0.18134764032840614, 0.01644130749673833, 0.007109074723334883, 0.03555704949423753, 0.033931576448048006, 0.0267173505099923, 0.032210162471156406, 0.25362223807297396, 0.028250079060620836, 0.09786695784386741]\n"
     ]
    }
   ],
   "source": [
    "SAMPLES = \"\"\"[0.7903253367110061, 0.1738679257213426, 0.6840758121402977, -1.4921922753864911, -0.20701526564877176, -0.13343908330179777, 0.27078275189785883, -0.47987385916752834, 1.762361457920409, -0.6382574781276095, -1.476682298043406, 0.13308403857533435, 0.08629164346752129, -0.15274120983311792, 0.03422761142701722, 2.0977915122558075, -0.09695813983037735, 1.145554587286743]\"\"\",\n",
    "READER = (x for x in SAMPLES[0].split('\\n')); \n",
    "sys.stdin.readline = lambda: next(READER)  # тупо шоп не переписывать\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x - vector of n elements - input\n",
    "\n",
    "    returns vector of n elements - softmax output\n",
    "    \"\"\"\n",
    "    return np.exp(x) / np.exp(x).sum()\n",
    "\n",
    "\n",
    "x = read_array()\n",
    "\n",
    "result = softmax(x)\n",
    "\n",
    "write_array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию, реализующую производную выхода softmax по входам $\\frac{\\partial softmax(x)} {\\partial x}$\n",
    "\n",
    "Помните, что и $softmax$ и $x$ - вектора из $n$ элементов, поэтому производная - это матрица, в $ij$-ячейке которой стоит производная $i$-го элемента $softmax_i(x)$ по $j$-му входу $x_j$ (i соответствует номеру строки, j - номер столбца). Такая матрица ещё называется матрицей Якоби.\n",
    "\n",
    "Подсказка: возможно, будет проще решать эту задачу, рассматривая два случая, когда $i=j$ и когда $i \\neq j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04520990642005896, -0.01496136384662353, -0.0005831584750616378, -0.0013113823146393676, -0.001369840806658998, -0.0038138833133273915, -0.004065315035397694, -0.005168124298133091, -0.001985359996840069, -0.0037154023993240712, -0.0011048889071803723, -0.0010044813810742047, -0.006126705645798534], [-0.01496136384662353, 0.21585791972231008, -0.0038730635991859903, -0.008709582942971316, -0.009097836680483412, -0.025330014505627975, -0.026999905439354373, -0.03432424452555927, -0.0131858248895869, -0.024675950714133716, -0.007338151103938566, -0.006671291663363668, -0.04069068981148137], [-0.0005831584750616378, -0.0038730635991859903, 0.012135730472354802, -0.0003394788843793973, -0.0003546120941472777, -0.000987303883778268, -0.0010523922714690004, -0.0013378776360475468, -0.0005139521780146152, -0.0009618100285956132, -0.0002860237242683219, -0.000260031125035206, -0.0015860265723719278], [-0.0013113823146393676, -0.008709582942971316, -0.0003394788843793973, 0.02686639459090744, -0.0007974368009876185, -0.0022202075554586314, -0.0023665755911748154, -0.003008563102643473, -0.0011557540971131601, -0.0021628780434165113, -0.0006431981521542989, -0.0005847470854143117, -0.0035665900205545404], [-0.001369840806658998, -0.009097836680483412, -0.0003546120941472777, -0.0007974368009876185, 0.028028490452586954, -0.0023191794450546825, -0.002472072240600495, -0.0031426781201811345, -0.001207275031099009, -0.0022592943115246175, -0.000671870487921686, -0.0006108138032925341, -0.0037255806306354905], [-0.0038138833133273915, -0.025330014505627975, -0.000987303883778268, -0.0022202075554586314, -0.0023191794450546825, 0.07389852776941823, -0.006882693975777504, -0.00874978142237634, -0.003361270939895075, -0.006290281931106345, -0.0018706083437909525, -0.0017006155464219198, -0.010372686906803137], [-0.004065315035397694, -0.026999905439354373, -0.0010523922714690004, -0.0023665755911748154, -0.002472072240600495, -0.006882693975777504, 0.07831657234824194, -0.009326614122810346, -0.0035828640174308843, -0.006704971183060922, -0.0019939289172219475, -0.0018127292794043132, -0.011056510274539641], [-0.005168124298133091, -0.03432424452555927, -0.0013378776360475468, -0.003008563102643473, -0.0031426781201811345, -0.00874978142237634, -0.009326614122810346, 0.09703166928519828, -0.004554797457063466, -0.008523847275730117, -0.002534827533934738, -0.002304473368792744, -0.014055840421926011], [-0.001985359996840069, -0.0131858248895869, -0.0005139521780146152, -0.0011557540971131601, -0.001207275031099009, -0.003361270939895075, -0.0035828640174308843, -0.004554797457063466, 0.04008023596611945, -0.0032744772424536947, -0.0009737662823977987, -0.0008852746134293082, -0.005399619220795472], [-0.0037154023993240712, -0.024675950714133716, -0.0009618100285956132, -0.0021628780434165113, -0.0022592943115246175, -0.006290281931106345, -0.006704971183060922, -0.008523847275730117, -0.0032744772424536947, 0.07215276857928996, -0.001822306074344211, -0.001656702778353048, -0.010104846597247091], [-0.0011048889071803723, -0.007338151103938566, -0.0002860237242683219, -0.0006431981521542989, -0.000671870487921686, -0.0018706083437909525, -0.0019939289172219475, -0.002534827533934738, -0.0009737662823977987, -0.001822306074344211, 0.02273722712642639, -0.0004926714055603228, -0.003004986193713176], [-0.0010044813810742047, -0.006671291663363668, -0.000260031125035206, -0.0005847470854143117, -0.0006108138032925341, -0.0017006155464219198, -0.0018127292794043132, -0.002304473368792744, -0.0008852746134293082, -0.001656702778353048, -0.0004926714055603228, 0.020715738092779736, -0.002731906042638157], [-0.006126705645798534, -0.04069068981148137, -0.0015860265723719278, -0.0035665900205545404, -0.0037255806306354905, -0.010372686906803137, -0.011056510274539641, -0.014055840421926011, -0.005399619220795472, -0.010104846597247091, -0.003004986193713176, -0.002731906042638157, 0.11242198833850454]]\n"
     ]
    }
   ],
   "source": [
    "SAMPLES = \"\"\"[-0.36170314084137395, 1.531638983431016, -1.7131284538840788, -0.9027503682845508, -0.8591376176087115, 0.16481576122888014, 0.2286590883015934, 0.4686776665093307, -0.4880318948728026, 0.13865483857501165, -1.0740873577508447, -1.1693607815929845, 0.6388250392697977]\"\"\",\n",
    "READER = (x for x in SAMPLES[0].split('\\n')); \n",
    "sys.stdin.readline = lambda: next(READER)  # тупо шоп не переписывать\n",
    "\n",
    "def dsoftmax_dx(x):\n",
    "    \"\"\"\n",
    "    x - vector of n elements - input\n",
    "\n",
    "    returns matrix n x n\n",
    "    \"\"\"\n",
    "    s = softmax(x)\n",
    "    jacobian_m = np.diag(s)\n",
    "\n",
    "    for i in range(len(jacobian_m)):\n",
    "        for j in range(len(jacobian_m)):\n",
    "            if i == j:\n",
    "                jacobian_m[i][j] = s[i] * (1 - s[i])\n",
    "            else: \n",
    "                jacobian_m[i][j] = -s[i] * s[j]\n",
    "    return jacobian_m\n",
    "\n",
    "def dsoftmax_dx_vect(x):\n",
    "    s = softmax(x).reshape(-1,1)\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "\n",
    "x = read_array()\n",
    "\n",
    "result = dsoftmax_dx(x)\n",
    "\n",
    "write_array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию, реализующую простой механизм внимания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4740581778159266, 0.34432834551260527, 0.4665167530554428, 0.6711507992179184, 0.3014932578908269, 0.5216013512810533, 0.5739206841874825, 0.5011664230502015, 0.6659426824711602, 0.5870403517141657, 0.4845604544566492, 0.5519092842133653, 0.6631513043916593, 0.6203158151313123, 0.679961421985706]\n"
     ]
    }
   ],
   "source": [
    "SAMPLES = \"\"\"[[0.3504305689198156, 0.871844425624726, 0.29345316540775357, 0.49159320438393916, 0.16391992930609034, 0.24589641847050037, 0.34921020303336925, 0.09968814035867879, 0.8652385667745919, 0.00906484385602968, 0.6134586521086117, 0.08104312584086149, 0.643129733435556, 0.6610968673257929, 0.6825169003800382], [0.005641686042561211, 0.3397733866278605, 0.4408793722092307, 0.6618752692611525, 0.4192615374283991, 0.6718589897811911, 0.23503584107912667, 0.9972834040264165, 0.6907780153811639, 0.5160598448726361, 0.4200243418824855, 0.7745997472321381, 0.9124177261957108, 0.627661131744206, 0.7792319239076758], [0.44947044223343224, 0.39851993627332394, 0.27645205987950927, 0.3360502940952873, 0.20207394761469466, 0.27730469648938627, 0.9647449489128369, 0.38480306917172535, 0.7014748335636187, 0.5616919724157547, 0.3082991954077743, 0.43320540280287834, 0.7682716834674514, 0.04669826413239708, 0.7975639937877288], [0.3529089999231677, 0.5085801437940869, 0.6686697864089949, 0.9579051761714787, 0.14893344048972235, 0.6504801381978066, 0.6554087909852483, 0.2182290972559655, 0.8874805349214916, 0.8549111586624765, 0.2256959432511686, 0.7090739886051667, 0.9215898392742404, 0.8361038069049278, 0.9807575571901945], [0.93096165894251, 0.21204415175966806, 0.005393301311816812, 0.6868163541395496, 0.17651743121260177, 0.4276211882347165, 0.7172630747046246, 0.6222321154458413, 0.782866044726867, 0.9401403417712956, 0.6009251310321828, 0.7689712777389628, 0.011137370287858661, 0.6750220130270511, 0.3656918897133191], [0.344423832576648, 0.5200078573131781, 0.08090528060543856, 0.6187002344784092, 0.24428489996011238, 0.18400539459399, 0.40308101020726217, 0.19255989698913867, 0.8010944590934469, 0.20324438899818598, 0.4927144298170735, 0.03783988662477278, 0.7705093963091103, 0.2520865403496373, 0.40266725180440743], [0.6681310493060861, 0.2801674372250399, 0.6224648405839522, 0.6287746784150413, 0.864080498689899, 0.23833127705610258, 0.20311743810136906, 0.6646132937899738, 0.23575457417289924, 0.1869625695994146, 0.7712148738157554, 0.15237041670323637, 0.2763150373902683, 0.46500408886101585, 0.991468614310106], [0.47955269815875845, 0.18371674117676162, 0.4749895427072034, 0.5127159626377625, 0.14327300286458633, 0.5921086963579639, 0.21467664382766927, 0.08984875049424312, 0.5619088573772313, 0.6324525220346037, 0.65145500723789, 0.5118736033583858, 0.3791794826772541, 0.7062193547285907, 0.12888775429739185], [0.8936198110390413, 0.1499351596848777, 0.23230300209801535, 0.6275970485906217, 0.14179412142521963, 0.44590423506527455, 0.6398118989481705, 0.44025473834142315, 0.9690917160909921, 0.7329911430579539, 0.4723409208689966, 0.30051845327308735, 0.6517065287372249, 0.11682964074366375, 0.3356912564688531], [0.5819483213886298, 0.12715730125007862, 0.6624081011547434, 0.27210122174739293, 0.3321414469174103, 0.6738692045564213, 0.49979407643990537, 0.923095453187033, 0.8688108133354637, 0.29672803800781, 0.8422355709858387, 0.22967466587429908, 0.48606633908371566, 0.3302629931498261, 0.9271715208940098], [0.814229644181095, 0.6269508015754929, 0.19067116118180638, 0.6597416333912787, 0.3042396495694798, 0.5349586078017191, 0.9889297007928726, 0.5059647631701082, 0.6586214714303461, 0.19972197385065704, 0.730120041302739, 0.9254129585548022, 0.7774768791337286, 0.5880525770183761, 0.4404909426586451], [0.8393608070151912, 0.551470751477307, 0.3776646281929925, 0.7403545806778788, 0.01464073752506101, 0.49682079457661743, 0.12829037985166736, 0.8323709714882789, 0.4861583628986299, 0.10966510942571872, 0.36384711262095637, 0.008343156485128067, 0.05481969871494197, 0.11036480291979456, 0.3495717657917], [0.575668909069271, 0.1948209406820347, 0.5066632418120769, 0.5610866065811511, 0.7503051258152065, 0.20250301475454058, 0.9387177222181186, 0.4214964558859865, 0.2441688535705906, 0.2852282954051667, 0.7185375048873539, 0.09961745251862686, 0.507873295740294, 0.9713796833363287, 0.8218946227484244], [0.14519733396011691, 0.07264015089790021, 0.7254237309701331, 0.7437297525624584, 0.3465971472185204, 0.6489212261982703, 0.2152569561085349, 0.6476151760429897, 0.2045187871395916, 0.9599712380254137, 0.28554199184758966, 0.7701922251424572, 0.7095119328780166, 0.7579558453415812, 0.4251898428876446]]\\n[0.30760147020407946, 0.1528992448227442, 0.9387231083505163, 0.12201982125460176, 0.3159744925438269, 0.555332538642272, 0.8654043562058316, 0.5523485724329922, 0.6405492495162189, 0.8421217300945876, 0.03415012932624606, 0.0914780538557024, 0.745151636966557, 0.9885343010237021, 0.02289480154711454]\"\"\",\n",
    "READER = (x for x in SAMPLES[0].split('\\n')); \n",
    "sys.stdin.readline = lambda: next(READER)  # тупо шоп не переписывать\n",
    "\n",
    "def attention(features, query):\n",
    "    \"\"\"\n",
    "    features - InLen x EmbSize - features of elements of input sequence\n",
    "    query - EmbSize - features of query object\n",
    "\n",
    "    returns vector of size EmbSize - features, aggregated according to the query\n",
    "    \"\"\"\n",
    "    unnorm_scores  = features @ query.T\n",
    "    att_scores = softmax(unnorm_scores)\n",
    "    attention = att_scores @ features\n",
    "    attention\n",
    "\n",
    "    return attention\n",
    "\n",
    "\n",
    "features = read_array()\n",
    "query = read_array()\n",
    "\n",
    "result = attention(features, query)\n",
    "\n",
    "write_array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию, реализующую механизм self-attention внимания с линейными преобразованиями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3828516277455327, -0.7052491362196253], [1.2990231347325172, 0.07313412990950194], [1.7291639000678214, 0.057097555962845575], [4.9977493686508465, 0.9611194260544595], [4.920776273177979, 0.9417305419570732], [5.027240226701748, 0.9675417617162319], [5.0084058482460545, 0.9647146788718596], [4.6177066707952, 0.8915957294670734], [5.01858654091172, 0.966041694502124], [1.412170986514584, -1.0082854213057408], [0.8082085153678324, -0.028634744390539153], [4.261630070405553, 0.7878600291134994], [1.1488305812076605, -1.188280258100271], [1.1929080669359462, -1.220148314614849], [3.906830387768465, 0.7744398548226678]]\n"
     ]
    }
   ],
   "source": [
    "SAMPLES = \"\"\"[[0.5175129778200084, 0.13021330700949507], [-0.3578609445921744, -0.07768163060380659], [-0.046577477754636734, -0.12288550821838619], [0.4424092505449793, -1.431399548551344], [0.753992222548331, -1.1210257338970167], [1.6736061037504428, -1.9789731491226337], [-1.4985152255486565, -1.6614802556117283], [-0.610065708073959, -0.8475335063027695], [-0.1657640783522184, -1.7079776825852762], [0.7857341373616981, 0.2956255012408635], [-0.49243028413686984, 0.01065675311085114], [0.20598401523943788, -0.6339670563637549], [-0.15698934123474126, 0.9516567843056503], [-0.08965595798444444, 0.9923765422389716], [-0.9649404480809814, -0.6203623955866846]]\\n[[-0.4777373377067222, 1.384780896738418], [1.5537173245233542, -1.6151073640132454]]\\n[0.34068677065672126, -1.7225350645946451]\\n[[1.667192089239799, 0.9072106203091014], [1.0017863742909645, -0.8578876449703756]]\\n[-0.3811843050970959, -0.15922065479353645]\\n[[2.396375885002737, 0.23995979796695774], [0.21631803999882093, -0.6475173781192963]]\\n[1.448781271879823, -0.7144164516316529]\"\"\",\n",
    "READER = (x for x in SAMPLES[0].split('\\n')); \n",
    "sys.stdin.readline = lambda: next(READER)  # тупо шоп не переписывать\n",
    "\n",
    "\n",
    "def self_attention(features, proj_k, bias_k, proj_q, bias_q, proj_v, bias_v):\n",
    "    \"\"\"\n",
    "    features - InLen x EmbSize - features of elements of input sequence\n",
    "    proj_k - EmbSize x EmbSize - projection matrix to make keys from features\n",
    "    bias_k - EmbSize - bias vector to make keys from features\n",
    "    proj_q - EmbSize x EmbSize - projection matrix to make queries from features\n",
    "    bias_q - EmbSize - bias vector to make queries from features\n",
    "    proj_v - EmbSize x EmbSize - projection matrix to make values from features\n",
    "    bias_v - EmbSize - bias vector to make values from features\n",
    "\n",
    "    returns InLen x EmbSize\n",
    "    \"\"\"\n",
    "    # head1\n",
    "    keys = features @ proj_k + bias_k     \n",
    "    queries = features @ proj_q + bias_q  \n",
    "    values = features @ proj_v + bias_v   \n",
    "    logits = queries @ keys.T        \n",
    "    att_scores = np.vectorize(softmax, signature=\"(d)->(d)\")(logits)   \n",
    "    result = att_scores @ values     \n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "features = read_array()\n",
    "proj_k = read_array()\n",
    "bias_k = read_array()\n",
    "proj_q = read_array()\n",
    "bias_q = read_array()\n",
    "proj_v = read_array()\n",
    "bias_v = read_array()\n",
    "\n",
    "result = self_attention(features, proj_k, bias_k, proj_q, bias_q, proj_v, bias_v)\n",
    "\n",
    "write_array(result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75291dc0307ea48294888123147845d2e15abd18d38848ca6ac05a6fe8c88425"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
