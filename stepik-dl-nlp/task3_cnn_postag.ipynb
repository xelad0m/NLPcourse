{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Свёрточные нейросети и POS-теггинг\n",
    "\n",
    "Попробуем на практике разобраться, как применять свёрточные нейросети к задачам обработки текста. Разбираться мы будем на примере задачи определения частей речи слов, по-английски она называется \"POS-tagging\" (part of speech tagging). Зачем эта задача нужна и в чём сложности — мы увидим в процессе\n",
    "\n",
    "POS-теггинг - определение частей речи (снятие частеречной неоднозначности)\n",
    "\n",
    "Эта задача относится к области лингвистического анализа, то есть анализа структуры текстов. Основная сложность в этой задаче, обычно, заключается в том, чтобы правильно определять часть речи для омонимов, то есть для слов, которые пишутся одинаково, но, на самом деле, имеют разные части речи. Другая сложность заключается в том, чтобы правильно определять части речи для неизвестных слов, то есть каких-нибудь неологизмов или специальной редкой лексики (научной, например). \n",
    "\n",
    "**Модели, которые получились ниже — уже достаточно неплохие, хотя промышленные POS тэггеры работают с гораздо более высоким качеством (на уровне 0.97 f-мер), у нас где-то 0.9.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- здесь 4 модели:\n",
    "  - время обучения одной модели на полном наборе данных:\n",
    "    - до 0,89: **30-40 минут**\n",
    "    - до сходимости `5e-3`: **1.5 часа**\n",
    "    - gpu: GF760gtx 2Gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle,\n",
    "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
    "\n",
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "# import sys; sys.path.append('./stepik-dl-nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:42:57.976431Z",
     "start_time": "2019-10-29T19:42:57.959538Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pyconll\n",
    "# !pip install spacy_udpipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делает load_ext autoreload \n",
    "\n",
    "По умолчанию, если модуль уже импортировался и встречается его повторный импорт, то модуль ищется в `sys.modules`, и повторный импорт ничего не дает. На такой случай по-классике есть `importlib.reload`. Расширение `autoreload` интерактивного питона просто снимает необходимость следить за изменениями импортированных модулей, само посмотрит время изменения, если надо перезагрузит. Великая вещь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:34.549739Z",
     "start_time": "2019-10-29T19:49:32.179692Z"
    }
   },
   "outputs": [],
   "source": [
    "# расширение IPython для переимпорта\n",
    "# зачем? потому что есть самописная dlnlputils, если ее править на ходу, то автоимпорт полезен\n",
    "%load_ext autoreload    \n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc  # будем удалять большие промежуточные объекты, а то память как не в себя кушает (привет языковой сервер микрософт)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyconll      # для загрузки размеченного корпуса\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import dlnlputils\n",
    "from dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n",
    "    character_tokenize, pos_corpus_to_tensor, POSTagger\n",
    "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "\n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем пару ускоряшек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверка работоспособности, модели не сохраняются\n",
    "QUICK_RUN = True     \n",
    "# не обучать, а взять последнюю обученную модель\n",
    "LOAD_LAST_MODEL = True     \n",
    "# доля исходных датасетов для обучения/валидации\n",
    "SAMPLE_SIZE = 1             \n",
    "# число эпох обучения НС\n",
    "NUM_TRAIN_EPOCHS = 10       \n",
    "\n",
    "if QUICK_RUN:\n",
    "    LOAD_LAST_MODEL = False\n",
    "    SAMPLE_SIZE = 0.05\n",
    "    NUM_TRAIN_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка текстов и разбиение на обучающую и тестовую подвыборки\n",
    "\n",
    "Используем размеченный корпус, который называется \"SynTag Rus\":\n",
    "- размечен лингвистами\n",
    "- содержит разметку:\n",
    "  - по частям речи\n",
    "  - по нормальным формам слов\n",
    "  - синтаксическую разметку \n",
    "- предназначен для того, чтобы настраивать и проверять методы лингвистического анализа текстов:\n",
    "  -  морфологического разбора\n",
    "  -  синтаксического разбора\n",
    "  \n",
    "Просто скачиваем с гитхаба\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:08.433599Z",
     "start_time": "2019-10-29T19:46:05.110693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "# !wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
    "# !wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разметка в этом корпусе представлена в [формате CoNLL](https://gitlab.com/mwetoolkit/mwetoolkit2-legacy/-/blob/master/test/filetype-samples/corpus.conll) — это достаточно распространённый формат для того, чтобы хранить аннотированные деревья и разную лингвистическую разметку. Он используется не только в этом корпусе — он достаточно популярный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.525561Z",
     "start_time": "2019-10-29T19:49:37.315213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "full_train = pyconll.load_from_file('./datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('./datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:\t1226\n",
      "Test size:\t445\n"
     ]
    }
   ],
   "source": [
    "# pyconll это пакет про язык, масочную идексацию не понимает\n",
    "full_train = full_train[ : round(len(full_train) * SAMPLE_SIZE)]\n",
    "full_test = full_test[ : round(len(full_test) * SAMPLE_SIZE)]\n",
    "print(f\"Train size:\\t{len(full_train)}\\nTest size:\\t{len(full_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные состоят из предложений, предложения из токенов. Что такое токен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('form', 'приемной'),\n",
       " ('id', '2'),\n",
       " ('_form', 'приемной'),\n",
       " ('lemma', 'приемная'),\n",
       " ('upos', 'NOUN'),\n",
       " ('xpos', None),\n",
       " ('feats',\n",
       "  {'Animacy': {'Inan'},\n",
       "   'Case': {'Loc'},\n",
       "   'Gender': {'Fem'},\n",
       "   'Number': {'Sing'}}),\n",
       " ('head', '6'),\n",
       " ('deprel', 'obl'),\n",
       " ('deps', {'6': ('obl', 'в', 'loc', None)}),\n",
       " ('misc', {})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = full_train[2][1]\n",
    "[(attr, t.__getattribute__(attr)) for attr in ['form'] + t.__slots__]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое предложение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.548127Z",
     "start_time": "2019-10-29T19:49:56.527559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В\tADP\n",
      "приемной\tNOUN\n",
      "его\tPRON\n",
      "с\tADP\n",
      "утра\tNOUN\n",
      "ожидали\tVERB\n",
      "посетители\tNOUN\n",
      ",\tPUNCT\n",
      "-\tPUNCT\n",
      "кое-кто\tPRON\n",
      "с\tADP\n",
      "важными\tADJ\n",
      "делами\tNOUN\n",
      ",\tPUNCT\n",
      "а\tCCONJ\n",
      "кое-кто\tPRON\n",
      "и\tPART\n",
      "с\tADP\n",
      "такими\tDET\n",
      ",\tPUNCT\n",
      "которые\tPRON\n",
      "легко\tADV\n",
      "можно\tADV\n",
      "было\tAUX\n",
      "решить\tVERB\n",
      "в\tADP\n",
      "нижестоящих\tADJ\n",
      "инстанциях\tNOUN\n",
      ",\tPUNCT\n",
      "не\tPART\n",
      "затрудняя\tVERB\n",
      "Семена\tPROPN\n",
      "Еремеевича\tPROPN\n",
      ".\tPUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in full_train[2]:\n",
    "    print(f\"{token.form}\\t{token.upos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статистика предложений и  токенов (используется для задания размеров тензоров)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.916262Z",
     "start_time": "2019-10-29T19:49:56.549806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 70\n",
      "Наибольшая длина токена 22\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = max(max(len(sent) for sent in full_train), max(len(sent) for sent in full_test)) \n",
    "MAX_ORIG_TOKEN_LEN = max(max(len(token.form) for sent in full_train for token in sent), \n",
    "                         max(len(token.form or [0]) for sent in full_test for token in sent))\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:57.251433Z",
     "start_time": "2019-10-29T19:49:56.919818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета .\n",
      "Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n",
      "В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n",
      "Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n",
      "Приемная была обставлена просто , но по-деловому .\n",
      "У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n",
      "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n",
      "Кабинет отличался скромностью , присущей Семену Еремеевичу .\n",
      "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n",
      "Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n"
     ]
    }
   ],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решать задачу определения части речи мы будем с помощью **свёрточных нейросетей.**  \n",
    "Будем использовать нейросети, которые принимают на вход номера **отдельных символов**\n",
    "\n",
    "Это вполне оправдано, потому что часть речи во многом определяется именно структурой слова, наличием суффиксов, окончаний определённого вида... \n",
    "\n",
    "Если бы мы работали на уровне отдельных токенов, то мы бы просто не могли анализировать структуру слов, нам бы приходилось просто запоминать, что такое-то слово — это существительное, другое слово это просто глагол... такого типа задача совсем из другой плоскости. Поэтому следующий этап обработки корпуса — это **построение словаря символов**:\n",
    "- словарь - просто `dict`\n",
    "- добавлен фиктивный символ:\n",
    "  - нужно для выравнивания векторных представлений\n",
    "    - нужно для удобной заливки на GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.124148Z",
     "start_time": "2019-10-29T19:49:57.254191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных символов 79\n",
      "[('<PAD>', 0), (' ', 1), ('о', 2), ('а', 3), ('е', 4), ('и', 5), ('.', 6), ('л', 7), ('н', 8), ('т', 9)]\n"
     ]
    }
   ],
   "source": [
    "train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n",
    "char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\n",
    "print(\"Количество уникальных символов\", len(char_vocab))\n",
    "print(list(char_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_char_tokenized    # надо чистить от ненужного\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- аналогично кодируем части речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.524125Z",
     "start_time": "2019-10-29T19:49:58.125577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<NOTAG>': 0,\n",
       " 'ADJ': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'AUX': 4,\n",
       " 'CCONJ': 5,\n",
       " 'DET': 6,\n",
       " 'INTJ': 7,\n",
       " 'NOUN': 8,\n",
       " 'NUM': 9,\n",
       " 'PART': 10,\n",
       " 'PRON': 11,\n",
       " 'PROPN': 12,\n",
       " 'PUNCT': 13,\n",
       " 'SCONJ': 14,\n",
       " 'SYM': 15,\n",
       " 'VERB': 16,\n",
       " 'X': 17}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n",
    "label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переводим все в тензоры, чтобы скормить торчу.\n",
    "\n",
    "- `TensorDataset` торча - принимает на вход списки тензоров и их меток: тензоры идентификаторов символов и идентификаторы меток частей речи.\n",
    "\n",
    "`pos_corpus_to_tensor` (пришлось прикостылить немного, часть тестового набора не читается):\n",
    "\n",
    "- создает эти тензоры,\n",
    "- входной тензор трехмерный (предложения, токены, символы)\n",
    "- в символьных тензорах добавлено две колонки с 0:\n",
    "  - далее в них будет указываться положение N-граммы в токене (начало, середина, конец), чтобы НС могла учитывать, что\n",
    "    - Одна и та же последовательность символов может быть как частью суффикса, так и частью приставки, при этом неся разную функцию\n",
    "    - Для определения части речи в русском языке конец слова (окончания и суффиксы) важнее, чем середина и начало слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.752672Z",
     "start_time": "2019-10-29T19:49:58.526431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train read errors:\t0\n",
      "Test read errors:\t0\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_labels, err1 = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "\n",
    "test_inputs, test_labels, err2 = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "\n",
    "print(f\"Train read errors:\\t{err1}\\nTest read errors:\\t{err2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del full_train, full_test   # больше не нужны\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входной тензор:\n",
    "\n",
    "- прямоугольный тензор, \n",
    "- каждая строчка этого тензора представляет один токен и содержит номера символов, которые в этом токене используются, в том порядке, в котором они встречались в самом токене\n",
    "- стартовый ноль в токене нужен для того, чтобы указать нейросети, что это — начало токена. Всё, что после последнего символа — заполняется нулями\n",
    "- количество значащих элементов в каждой такой строчке у нас отличается, есть короткие и длинные токены. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.754883Z",
     "start_time": "2019-10-29T19:49:40.582Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 39,  3, 25,  3,  7, 19,  8,  5, 13,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  2, 24,  7,  3, 11,  9,  8,  2, 22,  2,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 16, 17, 10,  3, 12,  7,  4,  8,  5, 18,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 11, 12, 18, 21,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 37,  4, 15,  4,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[1][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выходной (целевой) тензор:\n",
    "- одномерный тензор, просто список чисел\n",
    "- каждое число представляет номер тэга соответствующего токена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.756496Z",
     "start_time": "2019-10-29T19:49:40.711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  1,  8,  8, 12, 12,  4,  8,  1, 13, 16,  2,  8,  3,  3, 13, 16,  2,\n",
       "         8,  2,  8,  5,  3, 10, 16,  2,  8,  8,  2,  8, 13,  8, 13, 13,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_inputs, test_inputs       # уже погрузили в тензоры, больше не нужны, а лейблы нужны будут на валидации\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens:\t17263\n",
      "Test tokens:\t8557\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train tokens:\\t{train_labels.count_nonzero()}\\nTest tokens:\\t{test_labels.count_nonzero()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательная свёрточная архитектура\n",
    "\n",
    "Вспомогательный нейросетевой модуль, включает:\n",
    "- свертки\n",
    "- функцию активации\n",
    "- дропауты (методы регуляризации нейронной сети для предотвращения переобучения)\n",
    "\n",
    "`forward`:\n",
    "- модуль (`ModuleList`) состоит из **слоев**\n",
    "- для ускорения сходимости к выходу каждого слоя прибавляется вход\n",
    "  - такая штука называется **skip connection**\n",
    "  - без skip connection мы можем делать нейросеть глубины 5-9 \n",
    "    - иначе сеть не будет сходиться, т.к. большая размерность и небольшое изменение градиента уводит очень далеко в пространстве\n",
    "  - со skip connection сеть будет (должна) сходиться с произвольной глубиной, градиент двинется не так далеко от входа\n",
    "  - такая штука похожа на простой вариант [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network) (Residual neural network)\n",
    "\n",
    "**Слои**:\n",
    "- реализуются объектом торча `Sequential`:\n",
    "  - сам по себе является конвеером\n",
    "  - берет кортеж слоев и последовательно по ним передает выходы на входы\n",
    "-  **Первый слой**:\n",
    "   -  одномерная свертка (основной вариант для текста), число каналов на входе равно числу каналов на выходе\n",
    "   -  размер ядра по умолчанию равен 3\n",
    "   -  добавлен паддинг (фиктивные элементы по краям), чтобы тензор вообще не менялся в размере\n",
    "- **Второй слой**:\n",
    "  - дропаут (dropout): нужен, чтобы НС меньше переобучалась, для этого в начале обучения зануляет случайные ячейки тензора, а когда НС уже обучена - ничего не делает\n",
    "  - параметр - это вероятность зануления (в торче по умолчанию 0.5), у нас по-умолчаю 0.0 (**отключено**)\n",
    "- **Третий слой**:\n",
    "  - функция активации, тут это `LeakyReLU` (часто это — неплохой выбор для текста)\n",
    "\n",
    "ReLU:\n",
    "$$f(x) =\n",
    "  \\begin{cases}\n",
    "    x, & x > 0\\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{cases}$$\n",
    "    \n",
    "LeakyReLU:\n",
    "$$f(x) =\n",
    "  \\begin{cases}\n",
    "    x, & x > 0\\\\\n",
    "    \\alpha x, & \\text{otherwise}\n",
    "  \\end{cases}$$\n",
    "\n",
    "В LeakyReLU по умолчанию $\\alpha = 0.01$. Дает маленький ненулевой градиент в том случае, когда нейрон достиг насыщения и неактивен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.316516Z",
     "start_time": "2019-10-29T19:46:17.539Z"
    }
   },
   "outputs": [],
   "source": [
    "class StackedConv1d(nn.Module):\n",
    "    def __init__(self, features_num, layers_n=1, kernel_size=3, dropout=0.0, dilation=1, conv_layer=nn.Conv1d):\n",
    "        super().__init__()\n",
    "        pd = int((dilation * (kernel_size - 1)) / 2)    # паддинг должен учитывать разреженность окна\n",
    "        layers = []\n",
    "        for _ in range(layers_n):\n",
    "            layers.append(nn.Sequential(\n",
    "                conv_layer(features_num, features_num, kernel_size, padding=pd, dilation=dilation),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.LeakyReLU()))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Про дропаут\n",
    "\n",
    "Стоит отметить момент про коррекцию весов у обученной с dropout'ом сети. В train-time нейроны (их выходы) отключаются с вероятностью $p$, в test-time они уже обучены присутствуют все (генерируют выходы), следовательно на следующие нейроны приходит сигнал более высокий, чем был на этапе обучения, поэтому веса связей необходимо скорректировать.\n",
    "\n",
    "**Для этого, исходящие веса нейронов, которые обучались в режиме dropout умножаются на $(1-p)$.** Ну или бывает реализация, когда меняются не веса, а добавляется коррекция непосредственно на значения входов следующего слоя нейронов, результат тот же.\n",
    "\n",
    "В pytorch перед обучением модели ее необходимо перевести в режим обучения (критически важно для слоев `DropOut` и `BatchNorm`): \n",
    "\n",
    "    model.train()\n",
    "\n",
    "Перед оценкой качества/применением модели необходимо вызвать\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "Тут мы это делаем ниже внутри `train_eval_loop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание частей речи на уровне отдельных токенов\n",
    "\n",
    "Будет предсказывать метки частей речи только по векторам токенов, не используя контекст их употребления в предложениях. Случаи \"Сорок сорок сидело на дереве\" или \"Три да три будет шесть или три да три будет дырка?\" не осилит.\n",
    "\n",
    "`forward`\n",
    "\n",
    "1. Делаем **эбмеддинги**:\n",
    "- получим переменные, представляющие форму исходного тензора\n",
    "- схлопываем первое и второе измерения, чтобы получить двухмерный тензор\n",
    "  - так мы забываем о том, что токены у нас были как-то объединены в предложения\n",
    "- `nn.Embedding` - слой, который для каждого символа выдает вектора ембеддингов\n",
    "- \"транспонируем\" (permute) получившийся 3д-тензор, чтобы можно было его передать на свертку в соответствии с **конвенцией торча о порядке размерностей**:\n",
    "  - размер батча\n",
    "  - количество признаков у каждого элемента (тут это размер эмбеддинга)\n",
    "  - остальные размерности элементов\n",
    "    - тексты одномерные, поэтому тут у нас просто длина токена (в картинках будет 2д, ну и т.д.)\n",
    "\n",
    "Теперь char_embeddings содержит вектора для отдельных символов (без информации о контексте этих символов, просто факт, что они есть). В начале обучения они инициализируются случайным шумом.\n",
    "\n",
    "2. Передаем эти вектора в **backbone** сетку (в роли которого простенький ResNet, который описан выше)\n",
    "   - она (StackedConv1d) пройдет по случайным эмбеддингам символов сверткой с ядром размера 3 и вот мы уже получили признаки с учетом контекста символов в токене (`features`)\n",
    "   - **НО** тэги нам нужно предсказывать не для каждого символа, а для каждого токена\n",
    "   - поэтому мы передаем эти векторы признаков в агрегатор (pooling)\n",
    "  \n",
    "3. Используем **pooling** — в данном случае это max pooling:\n",
    "   - допустим у нас есть матрица (строки - символы в токене, столбцы - признаки), max pooling выдаст вектор по длине признаков, где каждое значение это максимум по столбцу (берется признак самого значимого символа в токене для каждого символа)\n",
    "   - пройдя по тензорам всех токенов получим уже двумерный тензор (`global_features`), каждая строчка этого тензора представляет отдельный токен\n",
    "\n",
    "4. Передаем в выходной модуль **out** (в роли которого полносвязный слой `nn.Linear`, что по сути есть линейный (мульти)классификатор, только тензор вместо матрицы $y=xA^T+b$ ). Он должен навесить метки на вектора признаков:\n",
    "   - он выдает также 2д тензор, но его размерность не длина эмбеддинга, а \"количество меток частей речи\" (по количеству токенов в тензоре)\n",
    "   - меняем форму этого тензора, преобразуем его в трёхмерный, просто возвращая разбивку на предложения (max_sent_len)\n",
    "   - транспонируем для того, чтобы порядок измерений соответствовал порядку измерений в исходном тензоре \"tokens\"\n",
    "\n",
    "Физический смысл того, что мы сейчас описали, заключается в том, чтобы рассмотреть все возможные N-граммы символов, которые встречаются в каждом токене, и по ним попробовать определить часть речи. \n",
    "\n",
    "Благодаря тому, что основная наша нейросеть (backbone) содержит \"skip connections\", N-граммы, которые учитываются этой нейросетью, по сути, имеют различную длину. Например, если мы используем размер ядра свёртки, равный 3, то первый блок учитывает трёхграммы, второй блок уже учитывает пятиграммы, а третий — семиграммы, соответственно. При этом, благодаря тому, что есть \"skip connection\", информация о трёхграммах не теряется, она пробрасывать до самого конца. **Здорово, правда?!**\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.317452Z",
     "start_time": "2019-10-29T19:46:23.135Z"
    }
   },
   "outputs": [],
   "source": [
    "class SingleTokenPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.backbone = StackedConv1d(embedding_size, **kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Linear(embedding_size, labels_num)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "        \n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        \n",
    "        features = self.backbone(char_embeddings)\n",
    "        \n",
    "        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "        \n",
    "        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n",
    "        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n",
    "        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пояснения к алгоритму, почему тензор батча трехмерный\n",
    "\n",
    "В начале мы фиксируем максимальную длину предложения $SentenceSize$:\n",
    "\n",
    "$$Batch = \\left( \\begin{matrix} w_{1,1} & w_{1,2} & ... & w_{1,S} \\\\ w_{2,1} & w_{2,2} & ... & w_{2,S} \\\\w_{3,1} & w_{3,2} & ... & w_{3,S} \\end{matrix} \\right) \\in \\mathbb{R} ^ {Batch \\times SentenceSize}$$\n",
    "\n",
    "Каждое слово состоит из букв + тэги начала и конца слова, все они имеют свой embedding vector:\n",
    "\n",
    "$$w_{1,1} = \\left( \\begin{matrix} c_{1,1} & c_{1,2} & ... & c_{1,k} \\end{matrix} \\right) = \\left( \\begin{matrix} \\left[ \\begin{matrix} e_{1,1} \\\\ e_{2,1} \\\\ ... \\\\ e_{E,1} \\end{matrix} \\right] & \\left[ \\begin{matrix} e_{1,2} \\\\ e_{2,2} \\\\ ... \\\\ e_{E,2} \\end{matrix} \\right] & ... & \\left[ \\begin{matrix} e_{1,k} \\\\ e_{2,k} \\\\ ... \\\\ e_{E,k} \\end{matrix} \\right] \\end{matrix} \\right) \\in \\mathbb{R} ^ {EmbedSize \\times WordSize}$$ \n",
    "\n",
    "Таким образом $Batch$ представляет собой тензор размерности 4:\n",
    "\n",
    "$$\\newline Batch = \\left( \\begin{matrix} \\left( \\begin{matrix} \\left[ \\begin{matrix} e_{1,1} \\\\ e_{2,1} \\\\ ... \\\\ e_{E,1} \\end{matrix} \\right] & \\left[ \\begin{matrix} e_{1,2} \\\\ e_{2,2} \\\\ ... \\\\ e_{E,2} \\end{matrix} \\right] & ... & \\left[ \\begin{matrix} e_{1,k} \\\\ e_{2,k} \\\\ ... \\\\ e_{E,k} \\end{matrix} \\right] \\end{matrix} \\right) & ... & ... \\\\ ... & ... & ... \\\\... & ... & ... \\end{matrix} \\right) \\in \\mathbb{R} ^ {Batch \\times SentenceSize \\times EmbedSize \\times WordSize}$$\n",
    "\n",
    "Первая нейросеть принимает на вход одно слово и не учитывает его контекст (соседние слова), поэтому для нее мы переделываем батч:\n",
    "\n",
    "1. Транспонируем  embedding vectors для каждого слова\n",
    "\n",
    "2. Один элемент батча - одно слово\n",
    "\n",
    "$$\\newline Batch' = \\left( \\begin{matrix} \\left( \\begin{matrix} \\left[ \\begin{matrix} e_{1,1} & e_{2,1} & ... & e_{E,1} \\end{matrix} \\right] \\\\ \\left[ \\begin{matrix} e_{1,2} & e_{2,2} & ... & e_{E,2} \\end{matrix} \\right] \\\\ ... \\\\ \\left[ \\begin{matrix} e_{1,k} e_{2,k} & ... & e_{E,k} \\end{matrix} \\right] \\end{matrix} \\right) \\\\ ... \\end{matrix} \\right) \\in \\mathbb{R} ^ {Batch * SentenceSize \\times WordSize \\times EmbedSize}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем экземпляр созданного нейросетевого модуля с параметрами, в т.ч.:\n",
    "- эмбеддинг 64\n",
    "- три сверточный слоя\n",
    "- размер ядра 3\n",
    "- дропаут 0.3\n",
    "\n",
    "Три слоя с ядром 3 с учетом skip_connection охватят N-граммы размера 3, 5 и 7.\n",
    "\n",
    "В нейросети получилось 47 тыс. параметров, это микросетка по современным меркам.\n",
    "\n",
    "*На 760gtx 10 эпох считались более получаса, но что характерно, у авторов курса на эпоху уходит 120 сек., а старичок 760gtx выдает 180-190 сек. что наверно очень даже не плохо, ведь врядли авторы на таком старье считали*.\n",
    "\n",
    "Более того, в комментариях пишут, что в гуглоколабе на карточке Tesla P100-PCIE-16GB время обучения одной эпохи примерно в два раза выше авторсого, т.е. около 300 сек. (мож он там шарится на несколько клиентов?). **А то выходит, 760gtx до сих пор актуальна**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.318497Z",
     "start_time": "2019-10-29T19:46:23.764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 43282\n"
     ]
    }
   ],
   "source": [
    "single_token_model = SingleTokenPOSTagger(len(char_vocab), len(label2id), \n",
    "                                          embedding_size=64, layers_n=3, kernel_size=3, \n",
    "                                          dropout=0.3)\n",
    "                                          \n",
    "print('Количество параметров', sum(np.product(t.shape) for t in single_token_model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь т.к. это многомерная классификация - кроссэнтропия:\n",
    "\n",
    "- кроссэнтропия батча токенов - это вектор кроссэнетропий размера батча, а `F.cross_entropy` возвращает среднее по этому вектору"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.319470Z",
     "start_time": "2019-10-29T19:46:25.552Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0\n",
      "Эпоха: 20 итераций, 3.06 сек\n",
      "Среднее значение функции потерь на обучении 1.0005273409187794\n",
      "Среднее значение функции потерь на валидации 0.5056548970086234\n",
      "Новая лучшая модель!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if LOAD_LAST_MODEL:\n",
    "    # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "    try:\n",
    "        msg = single_token_model.load_state_dict(torch.load('./models/single_token_pos.pth'))\n",
    "        print(\"Loading pretrained: \", msg)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Конфигурация сети поменялась, сохраненная модель не подходит\")\n",
    "        raise e\n",
    "else:\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    (val_loss,\n",
    "    single_token_model) = train_eval_loop(single_token_model,\n",
    "                                          train_dataset,\n",
    "                                          test_dataset,\n",
    "                                          F.cross_entropy,\n",
    "                                          lr=5e-3,\n",
    "                                          epoch_n=NUM_TRAIN_EPOCHS,\n",
    "                                          batch_size=64,\n",
    "                                          device='cuda',\n",
    "                                          early_stopping_patience=5,\n",
    "                                          max_batches_per_epoch_train=500,\n",
    "                                          max_batches_per_epoch_val=100,\n",
    "                                          lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                  factor=0.5,\n",
    "                                                                                                                  verbose=True))\n",
    "    \n",
    "    if not QUICK_RUN:\n",
    "        torch.save(single_token_model.state_dict(), './models/single_token_pos.pth')                                                                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим что получилось\n",
    "\n",
    "- распределение классов сильно неравномерное, поэтому accuracy нас совсем не интересует\n",
    "- т.к. классов много, то смотрим интегральные показатели `macro avg` - тут все прилично\n",
    "  - на обучающей и валидационной выборках порядка **0.87-0.89**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.324276Z",
     "start_time": "2019-10-29T19:46:48.445Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [00:01, 37.97it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.2912595868110657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00     68557\n",
      "         ADJ       0.37      0.47      0.41      1349\n",
      "         ADP       0.70      0.80      0.75      1638\n",
      "         ADV       0.27      0.09      0.14      1021\n",
      "         AUX       0.00      0.00      0.00       125\n",
      "       CCONJ       0.87      0.60      0.71       641\n",
      "         DET       0.00      0.00      0.00       404\n",
      "        INTJ       0.00      0.00      0.00         7\n",
      "        NOUN       0.57      0.62      0.59      3925\n",
      "         NUM       0.00      0.00      0.00       174\n",
      "        PART       0.93      0.45      0.60       451\n",
      "        PRON       0.48      0.53      0.51       912\n",
      "       PROPN       0.08      0.00      0.00       414\n",
      "       PUNCT       0.96      0.98      0.97      3538\n",
      "       SCONJ       0.39      0.52      0.45       257\n",
      "         SYM       0.00      0.00      0.00         2\n",
      "        VERB       0.50      0.72      0.59      2403\n",
      "           X       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.93     85820\n",
      "   macro avg       0.40      0.38      0.37     85820\n",
      "weighted avg       0.92      0.93      0.92     85820\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101%|██████████| 14/13.90625 [00:00<00:00, 37.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.5066335201263428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       0.99      1.00      1.00     22593\n",
      "         ADJ       0.49      0.50      0.50       895\n",
      "         ADP       0.61      0.79      0.69       875\n",
      "         ADV       0.13      0.09      0.11       275\n",
      "         AUX       0.00      0.00      0.00        58\n",
      "       CCONJ       0.82      0.69      0.75       224\n",
      "         DET       0.00      0.00      0.00       145\n",
      "        NOUN       0.66      0.64      0.65      2513\n",
      "         NUM       0.00      0.00      0.00       299\n",
      "        PART       0.91      0.29      0.44       142\n",
      "        PRON       0.31      0.55      0.39       170\n",
      "       PROPN       1.00      0.00      0.01       590\n",
      "       PUNCT       0.84      0.90      0.87      1574\n",
      "       SCONJ       0.23      0.25      0.24        85\n",
      "        VERB       0.30      0.69      0.42       652\n",
      "           X       0.00      0.00      0.00        60\n",
      "\n",
      "    accuracy                           0.88     31150\n",
      "   macro avg       0.46      0.40      0.38     31150\n",
      "weighted avg       0.89      0.88      0.87     31150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(single_token_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(single_token_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "TEST_UNIQUE_TAGS = np.array(UNIQUE_TAGS)[np.unique(test_labels)].tolist()   # классов в тесте может быть меньше\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=TEST_UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_pred, train_loss, test_pred, test_loss    # больше не нужны\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание частей речи на уровне предложений (с учётом контекста)\n",
    "\n",
    "В `forward` тоже самое, кроме:\n",
    "\n",
    "1. эмбеддинги такие же (тоже схлопываем измерение предложений!)\n",
    "2. backbone нейромодуль такой же (просто перименован в single_token_backbone)\n",
    "3. пулинг такой же\n",
    "\n",
    "Дальше отличия - проделываем примерно тоже самое что и с символами в токене, но уже на уровне токенов в предложении. \n",
    "\n",
    "Для этого:\n",
    "- добавлен еще один сверточный модуль `context_backbone` (архитектура идентичная, со \"skip connections\")\n",
    "- выходной модуль `out` это не линейный классификатор (логит), а еще одна свертка `nn.Conv1d`\n",
    "  \n",
    "4. Сверточный модуль: \n",
    "    - возвращаем измерение предложений, меняем порядок измерений под требования торча\n",
    "    - передаем на свертку\n",
    "    - получаем тоже 3д тензор, той же размерности, но теперь тензоры токенов учитывают положение токена в предложении\n",
    "5. Принятие решения о классе:\n",
    "   - это **одномерна свертка с ядром размера 1** (окно ядра это один токен)\n",
    "   - она проецирует признаки (вектор/тензор) каждого токена на пространство классов\n",
    "\n",
    "Физический смысл того, что мы описали сейчас, заключается в том, чтобы сначала проанализировать структуру каждого слова, найти там какие-то суффиксы и окончания (пп. 1-3) а затем — смешать информацию о структуре каждого слова с контекстом, в котором это слово употребляется (п. 4). \n",
    "\n",
    "\\*) Почему до этого **был полносвязный слой (линейный мультиклассификатор), а теперь одномерная свертка с ядром 1**? \n",
    "\n",
    "Разницы нет, применить к матрице $A$ размера $C_{in} \\times L_{in}$ (в нашем случае число каналов — это размерность вектора эмбеддинга, а длина последовательности — это максимальная длина предложения) одномерную свертку $K$ c числом выходных каналов $C_{out}$ — это то же, что представить $K$ как матрицу размера $C_{in} \\times C_{out}$ и умножить $A^T$ на $K$ (в pytorch в `Conv1d` размерности расставлены в порядке, обратном тому, который мы брали ранее)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.325744Z",
     "start_time": "2019-10-29T19:46:50.139Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentenceLevelPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n",
    "        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "        \n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        \n",
    "        char_features = self.single_token_backbone(char_embeddings)\n",
    "        \n",
    "        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "\n",
    "        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n",
    "        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "\n",
    "        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество параметров почти удвоилось (84 тыс.), т.к.:\n",
    "- добавился набор параметров для анализа контекста токена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.326925Z",
     "start_time": "2019-10-29T19:46:50.310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 80338\n"
     ]
    }
   ],
   "source": [
    "sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n",
    "                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На времени обучения это не сказывается, т.к. количество параметров съедает память, а не время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.327888Z",
     "start_time": "2019-10-29T19:46:50.737Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0\n",
      "Эпоха: 20 итераций, 2.56 сек\n",
      "Среднее значение функции потерь на обучении 0.8832546755671501\n",
      "Среднее значение функции потерь на валидации 0.4321576399462564\n",
      "Новая лучшая модель!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if LOAD_LAST_MODEL:\n",
    "    try:\n",
    "        msg = sentence_level_model.load_state_dict(torch.load('./models/sentence_level_pos.pth'))\n",
    "        print(\"Loading pretrained: \", msg)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Конфигурация сети поменялась, сохраненная модель не подходит\")\n",
    "        raise e\n",
    "else:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    (val_loss,\n",
    "    sentence_level_model) = train_eval_loop(sentence_level_model,\n",
    "                                            train_dataset,\n",
    "                                            test_dataset,\n",
    "                                            F.cross_entropy,\n",
    "                                            lr=5e-3,\n",
    "                                            epoch_n=NUM_TRAIN_EPOCHS,\n",
    "                                            batch_size=64,\n",
    "                                            device='cuda',\n",
    "                                            early_stopping_patience=5,\n",
    "                                            max_batches_per_epoch_train=500,\n",
    "                                            max_batches_per_epoch_val=100,\n",
    "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                        factor=0.5,\n",
    "                                                                                                                        verbose=True))\n",
    "    if not QUICK_RUN:\n",
    "        torch.save(sentence_level_model.state_dict(), './models/sentence_level_pos.pth')                                                                                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного улучшились метрики, но не кардинально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.092139Z",
     "start_time": "2019-08-29T13:56:16.567242Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [00:01, 36.61it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.2532041668891907\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00     68557\n",
      "         ADJ       0.33      0.44      0.38      1349\n",
      "         ADP       0.91      0.72      0.81      1638\n",
      "         ADV       0.30      0.09      0.14      1021\n",
      "         AUX       0.00      0.00      0.00       125\n",
      "       CCONJ       0.81      0.62      0.70       641\n",
      "         DET       0.10      0.00      0.00       404\n",
      "        INTJ       0.00      0.00      0.00         7\n",
      "        NOUN       0.47      0.84      0.60      3925\n",
      "         NUM       0.00      0.00      0.00       174\n",
      "        PART       0.94      0.45      0.61       451\n",
      "        PRON       0.84      0.27      0.41       912\n",
      "       PROPN       0.00      0.00      0.00       414\n",
      "       PUNCT       0.95      1.00      0.97      3538\n",
      "       SCONJ       0.60      0.41      0.49       257\n",
      "         SYM       0.00      0.00      0.00         2\n",
      "        VERB       0.63      0.50      0.56      2403\n",
      "           X       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.93     85820\n",
      "   macro avg       0.44      0.35      0.37     85820\n",
      "weighted avg       0.93      0.93      0.92     85820\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101%|██████████| 14/13.90625 [00:00<00:00, 36.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.433011919260025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00     22593\n",
      "         ADJ       0.41      0.44      0.43       895\n",
      "         ADP       0.76      0.72      0.74       875\n",
      "         ADV       0.20      0.08      0.11       275\n",
      "         AUX       0.00      0.00      0.00        58\n",
      "       CCONJ       0.77      0.69      0.73       224\n",
      "         DET       0.50      0.01      0.01       145\n",
      "        NOUN       0.55      0.77      0.64      2513\n",
      "         NUM       0.00      0.00      0.00       299\n",
      "        PART       0.95      0.29      0.44       142\n",
      "        PRON       0.59      0.19      0.29       170\n",
      "       PROPN       0.00      0.00      0.00       590\n",
      "       PUNCT       0.83      0.93      0.88      1574\n",
      "       SCONJ       0.40      0.19      0.26        85\n",
      "        VERB       0.31      0.46      0.37       652\n",
      "           X       0.00      0.00      0.00        60\n",
      "\n",
      "    accuracy                           0.88     31150\n",
      "   macro avg       0.45      0.36      0.37     31150\n",
      "weighted avg       0.87      0.88      0.87     31150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(sentence_level_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(sentence_level_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "TEST_UNIQUE_TAGS = np.array(UNIQUE_TAGS)[np.unique(test_labels)].tolist()   # классов в тесте может быть меньше\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=TEST_UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_pred, train_loss, test_pred, test_loss    # больше не надо\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение полученных теггеров и сравнение\n",
    "\n",
    "Сделан отдельный класс для расстановки тегов части речи `POSTagger`:\n",
    "- принимает на вход обученную модель\n",
    "- отображение символов в ИД\n",
    "- отображение номеров тегов в строковое представление\n",
    "- статистику корпуса\n",
    "- единственный метод `__call__`\n",
    "\n",
    "Чтобы определить части речи токенов, сначала мы токенизируем переданное нам предложение, затем делаем то, что мы делали при обучении — а именно, перекладываем информацию о символах в тензор, затем, в цикле, применяем нашу модель для каждого предложения. В результате применения нашей модели к каждому предложению мы получаем трёхмерный тензор, первая размерность этого тензора соответствует количеству предложений, вторая — это количество меток, а третья — это максимальная длина предложения.\n",
    "\n",
    "Таким образом, для каждого предложения и для каждого токена у нас есть распределение вероятностей по меткам. И здесь мы просто выбираем для каждого токена наиболее вероятную метку. \n",
    "\n",
    "Ну, и напоследок — мы преобразовываем полученную информацию в формат, удобный для человека, то есть мы преобразовываем номера тэгов в их строковое название."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.105418Z",
     "start_time": "2019-08-29T13:56:42.093744Z"
    }
   },
   "outputs": [],
   "source": [
    "single_token_pos_tagger = POSTagger(single_token_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "sentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.125540Z",
     "start_time": "2019-08-29T13:56:42.106771Z"
    }
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'Мама мыла раму.',\n",
    "    'Косил косой косой косой.',\n",
    "    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n",
    "    'Сяпала Калуша с Калушатами по напушке.',\n",
    "    'Пирожки поставлены в печь, мама любит печь.',\n",
    "    'Ведро дало течь, вода стала течь.',\n",
    "    'Три да три, будет дырка.',\n",
    "    'Три да три, будет шесть.',\n",
    "    'Сорок сорок',\n",
    "    'Он видел их семью своими глазами',\n",
    "    'Эти типы стали есть в цехе',\n",
    "]\n",
    "test_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без контекста получилось вот:\n",
    "- а вот вообще то нормально получилось то, че вы сразу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.148124Z",
     "start_time": "2019-08-29T13:56:42.126930Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 91.86it/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама-NOUN мыла-VERB раму-NOUN\n",
      "косил-VERB косой-ADJ косой-ADJ косой-ADJ\n",
      "глокая-VERB куздра-NOUN штеко-ADJ будланула-VERB бокра-NOUN и-CCONJ куздрячит-VERB бокрёнка-NOUN\n",
      "сяпала-VERB калуша-VERB с-ADP калушатами-NOUN по-ADP напушке-ADV\n",
      "пирожки-NOUN поставлены-VERB в-ADP печь-VERB мама-NOUN любит-VERB печь-VERB\n",
      "ведро-NOUN дало-VERB течь-VERB вода-ADP стала-VERB течь-VERB\n",
      "три-NOUN да-ADP три-NOUN будет-NOUN дырка-NOUN\n",
      "три-NOUN да-ADP три-NOUN будет-NOUN шесть-VERB\n",
      "сорок-ADJ сорок-ADJ\n",
      "он-PRON видел-NOUN их-CCONJ семью-ADJ своими-NOUN глазами-NOUN\n",
      "эти-NOUN типы-NOUN стали-VERB есть-VERB в-ADP цехе-ADJ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С контекстом получилось вот:\n",
    "- не везде, но кое где сработало\n",
    "- сильно хуже вышло, чем было у авторов\n",
    "\n",
    "Причины не срабатывания помимо \"просто недоучили сеть\" (\"мама мыла раму\"):\n",
    "- \"три/три\" - определяющее контекстное слово \"дырка/шесть\" стоит дальше чем размер ядра и сеть его не видит, нужно более крупное ядро или больше слоев сверток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.168810Z",
     "start_time": "2019-08-29T13:56:42.149698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 86.07it/s]                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама-NOUN мыла-VERB раму-NOUN\n",
      "косил-VERB косой-ADJ косой-ADJ косой-NOUN\n",
      "глокая-NOUN куздра-NOUN штеко-NOUN будланула-VERB бокра-NOUN и-CCONJ куздрячит-NOUN бокрёнка-NOUN\n",
      "сяпала-VERB калуша-VERB с-ADP калушатами-NOUN по-ADP напушке-NOUN\n",
      "пирожки-NOUN поставлены-NOUN в-ADP печь-NOUN мама-VERB любит-VERB печь-NOUN\n",
      "ведро-NOUN дало-VERB течь-NOUN вода-NOUN стала-VERB течь-NOUN\n",
      "три-NOUN да-VERB три-NOUN будет-NOUN дырка-NOUN\n",
      "три-NOUN да-VERB три-NOUN будет-NOUN шесть-NOUN\n",
      "сорок-NOUN сорок-NOUN\n",
      "он-PRON видел-NOUN их-CCONJ семью-NOUN своими-NOUN глазами-NOUN\n",
      "эти-NOUN типы-NOUN стали-VERB есть-NOUN в-ADP цехе-NOUN\n"
     ]
    }
   ],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свёрточный модуль своими руками\n",
    "\n",
    "Все pytorch-модули наследуются от базового класса `nn.Module`.\n",
    "\n",
    "Реализуем не весь, но основной функционал стандартного модуля `nn.Conv1d` (так, чтоб он мог подключиться в конвеер вместо оригинального):\n",
    "- чтобы детально посмотреть механизм работы, как учитываются входные данные, как обрабатываются отдельные каналы (признаки) и т.п.\n",
    "\n",
    "Одномерные свертки **принимают на вход** 3д тензора (размер батча, количество входных каналов, длина одномерной последовательности)\n",
    "\n",
    "Одномерные свертки **возвращают** также 3д тензор размерности [\"количество элементов в батче\", \"количество выходных каналов\", \"новая длина последовательности\"]. Длина последовательности может либо остаться прежней, либо уменьшится, либо увеличиться — это зависит от размеров ядра и от паддинга.\n",
    "\n",
    "`__init__`\n",
    "\n",
    "- сохраняем входные параметры\n",
    "- создаем начальный тензор **весов**, он же **ядро свертки** (инициируем нормальным (почему?) шумом с малой дисперсией) \n",
    "  - количество строк равно количеству входных каналов умноженному на размер свёртки (на размер ядра), а количество столбцов этой матрицы равно количеству выходных каналов\n",
    "- создаем начальный тензор смещений (инициируем нулями)\n",
    "  - размерость по количеству выходных каналов\n",
    "\n",
    "`forward`\n",
    "\n",
    "1. Делаем паддинг\n",
    "- увеличиваем длину 3-го измерения на паддинг\n",
    "- заполняем их нулями\n",
    "- обновляем переменную с размером 3-го измерения (используется дальше)\n",
    "\n",
    "2. Подготовка признаков\n",
    "- это матрица окон, по которым проходи ядро слепленных вертикально (torch.cat(chunks, dim=1))\n",
    "\n",
    "3. Транспонируем под требования торча и применяем ядро\n",
    "\n",
    "- `torch.bmm` (batch matrix multiplication) - на вход идут\n",
    "  - матрица признаков, которая берется построчно (по окнам)\n",
    "  - ядро свертки (веса) - которые надо дополнить измерениями для совместности матричного умножения\n",
    "    - expand изменяет размер тензора, но, при этом, она делает это без выделения дополнительной памяти, то есть снаружи выглядит, что тензор большой, а на самом деле это — просто плоская матрица. Похоже просто переиндексация\n",
    "- добавляются смещения self.bias тоже дополненные измерениями для тензорной совместности \n",
    "- транспонируем под сигнатуру входного тензора\n",
    "\n",
    "Все просто, если четко соблюдать размерности и порядки измерений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.193140Z",
     "start_time": "2019-08-29T13:56:42.170233Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n",
    "                                   requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n",
    "        \n",
    "        # тут делается паддинг\n",
    "        batch_size, src_channels, sequence_len = x.shape        \n",
    "        if self.padding > 0:\n",
    "            pad = x.new_zeros(batch_size, src_channels, self.padding)\n",
    "            x = torch.cat((pad, x, pad), dim=-1)    # concat в торче\n",
    "            sequence_len = x.shape[-1]\n",
    "        \n",
    "        # тут подготовка признаков\n",
    "        chunks = []\n",
    "        chunk_size = sequence_len - self.kernel_size + 1\n",
    "        for offset in range(self.kernel_size):\n",
    "            chunks.append(x[:, :, offset:offset + chunk_size])\n",
    "\n",
    "        in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n",
    "        in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n",
    "        out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n",
    "        out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n",
    "        return out_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество параметров такое же"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.210013Z",
     "start_time": "2019-08-29T13:56:42.194620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 80338\n"
     ]
    }
   ],
   "source": [
    "sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                                      single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d),\n",
    "                                                      context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У авторов эта реализация требовала 50 сек. для расчета эпохи.\n",
    "\n",
    "Эта разница имеет значение только для той версии pytorch, которую используем мы сейчас, в другой версии pytorch разница может быть совершенно другая. \n",
    "\n",
    "Наиболее вероятно, что это ускорение вызвано тем, что наша реализация свёрток узкоспециализирована, в ней нету кучи разных вариантов паддинга, в ней нет механизма прореживания ядра, а также нет страйдов — в ней нельзя задавать шаг, с которым нужно идти скользящим окном по исходной последовательности. Другими словами, она проще гораздо, чем стандартная реализация свёрток. \n",
    "\n",
    "Таким образом, иногда имеет смысл что-то реализовать самому, но надо помнить, что в машинном обучении, часто, гораздо важнее быстро проверять разные гипотезы, а для этого лучше использовать стандартные модули, которые проверены, работают надёжно в разных ситуациях, они универсальны и вам не нужно тратить время на написание своих модулей. \n",
    "\n",
    "Свои модули имеет смысл писать только тогда, когда вы точно знаете ту архитектуру, которая вам нужна для решения вашей прикладной задачи, и вы хотите её ускорить. Например — для того, чтобы уметь запускать её на мобильном телефоне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T14:06:00.233326Z",
     "start_time": "2019-08-29T13:56:42.211456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0\n",
      "Эпоха: 52 итераций, 4.83 сек\n",
      "Среднее значение функции потерь на обучении 0.6565444871353415\n",
      "Среднее значение функции потерь на валидации 0.2849828514613603\n",
      "Новая лучшая модель!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if LOAD_LAST_MODEL:\n",
    "    # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "    try:\n",
    "        msg = sentence_level_model_my_conv.load_state_dict(torch.load('./models/sentence_level_model_my_conv.pth'))\n",
    "        print(\"Loading pretrained: \", msg)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Конфигурация сети поменялась, сохраненная модель не подходит\")\n",
    "        raise e\n",
    "else:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    (val_loss,\n",
    "    sentence_level_model_my_conv) = train_eval_loop(sentence_level_model_my_conv,\n",
    "                                                    train_dataset,\n",
    "                                                    test_dataset,\n",
    "                                                    F.cross_entropy,\n",
    "                                                    lr=5e-3,\n",
    "                                                    epoch_n=NUM_TRAIN_EPOCHS,\n",
    "                                                    batch_size=24, # 64 не дает\n",
    "                                                    device='cuda',\n",
    "                                                    early_stopping_patience=5,\n",
    "                                                    max_batches_per_epoch_train=500,\n",
    "                                                    max_batches_per_epoch_val=100,\n",
    "                                                    lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                                factor=0.5,\n",
    "                                                                                                                                verbose=True))\n",
    "    if not QUICK_RUN:\n",
    "        torch.save(sentence_level_model_my_conv.state_dict(), './models/sentence_level_model_my_conv.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество не хуже, даже получше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T14:06:39.145214Z",
     "start_time": "2019-08-29T14:06:00.234936Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [00:01, 23.33it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.13216552138328552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00     68557\n",
      "         ADJ       0.70      0.56      0.63      1349\n",
      "         ADP       0.89      0.92      0.90      1638\n",
      "         ADV       0.63      0.16      0.26      1021\n",
      "         AUX       0.80      0.77      0.78       125\n",
      "       CCONJ       0.90      0.84      0.87       641\n",
      "         DET       0.58      0.45      0.51       404\n",
      "        INTJ       0.00      0.00      0.00         7\n",
      "        NOUN       0.64      0.89      0.74      3925\n",
      "         NUM       0.82      0.24      0.37       174\n",
      "        PART       0.75      0.63      0.68       451\n",
      "        PRON       0.85      0.52      0.65       912\n",
      "       PROPN       0.86      0.61      0.71       414\n",
      "       PUNCT       1.00      1.00      1.00      3538\n",
      "       SCONJ       0.69      0.70      0.70       257\n",
      "         SYM       0.00      0.00      0.00         2\n",
      "        VERB       0.76      0.83      0.79      2403\n",
      "           X       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.96     85820\n",
      "   macro avg       0.66      0.56      0.59     85820\n",
      "weighted avg       0.96      0.96      0.95     85820\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101%|██████████| 14/13.90625 [00:00<00:00, 23.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.2861635386943817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00     22593\n",
      "         ADJ       0.72      0.58      0.64       895\n",
      "         ADP       0.84      0.90      0.87       875\n",
      "         ADV       0.19      0.05      0.09       275\n",
      "         AUX       0.86      0.88      0.87        58\n",
      "       CCONJ       0.80      0.77      0.78       224\n",
      "         DET       0.53      0.46      0.49       145\n",
      "        NOUN       0.66      0.83      0.74      2513\n",
      "         NUM       0.94      0.55      0.70       299\n",
      "        PART       0.73      0.46      0.57       142\n",
      "        PRON       0.60      0.42      0.49       170\n",
      "       PROPN       0.68      0.16      0.26       590\n",
      "       PUNCT       0.92      0.98      0.95      1574\n",
      "       SCONJ       0.63      0.47      0.54        85\n",
      "        VERB       0.49      0.73      0.59       652\n",
      "           X       0.00      0.00      0.00        60\n",
      "\n",
      "    accuracy                           0.92     31150\n",
      "   macro avg       0.66      0.58      0.60     31150\n",
      "weighted avg       0.92      0.92      0.92     31150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(sentence_level_model_my_conv, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(sentence_level_model_my_conv, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "TEST_UNIQUE_TAGS = np.array(UNIQUE_TAGS)[np.unique(test_labels)].tolist()   # классов в тесте может быть меньше\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=TEST_UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_pred, train_loss, test_pred, test_loss    # больше не нужны\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |     802 KB |  292221 KB |  142576 MB |  142575 MB |\n",
      "|       from large pool |       0 KB |  289280 KB |  139169 MB |  139169 MB |\n",
      "|       from small pool |     802 KB |    8723 KB |    3406 MB |    3406 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |     802 KB |  292221 KB |  142576 MB |  142575 MB |\n",
      "|       from large pool |       0 KB |  289280 KB |  139169 MB |  139169 MB |\n",
      "|       from small pool |     802 KB |    8723 KB |    3406 MB |    3406 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    4096 KB |  315392 KB |     902 MB |     898 MB |\n",
      "|       from large pool |       0 KB |  307200 KB |     884 MB |     884 MB |\n",
      "|       from small pool |    4096 KB |   10240 KB |      18 MB |      14 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    3293 KB |   81753 KB |   79093 MB |   79090 MB |\n",
      "|       from large pool |       0 KB |   78880 KB |   74914 MB |   74914 MB |\n",
      "|       from small pool |    3293 KB |    5334 KB |    4178 MB |    4175 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      39    |     129    |   24483    |   24444    |\n",
      "|       from large pool |       0    |      22    |   10228    |   10228    |\n",
      "|       from small pool |      39    |     126    |   14255    |   14216    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      39    |     129    |   24483    |   24444    |\n",
      "|       from large pool |       0    |      22    |   10228    |   10228    |\n",
      "|       from small pool |      39    |     126    |   14255    |   14216    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       2    |      15    |      41    |      39    |\n",
      "|       from large pool |       0    |      11    |      32    |      32    |\n",
      "|       from small pool |       2    |       5    |       9    |       7    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      16    |      23    |   14279    |   14263    |\n",
      "|       from large pool |       0    |      10    |    8060    |    8060    |\n",
      "|       from small pool |      16    |      20    |    6219    |    6203    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unreachable objects collected by GC: 112\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unreachable objects collected by GC:\", gc.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# До.за.\n",
    " \n",
    "В качестве домашнего задания мы предлагаем Вам поэкспериментировать с кодом этого семинара, чтобы лучше понять особенности свёрточных нейросетей и попробовать улучшить качество определения частей речи. Что можно попробовать сделать:\n",
    "\n",
    "- поиграться с параметрами и архитектурой - количеством каналов (размерностью эмбеддинга), глубиной нейросети, силой Dropout, добавить BatchNorm или другую нормализацию\n",
    "- подключить прореженные (dilated) свёртки, чтобы увеличить рецептивное поле без увеличения числа параметров\n",
    "- добавить взвешивание классов\n",
    "- использовать в качестве обозначения начала и конца слова не 0, а какой-нибудь другой токен (для 0 nn.Embedding всегда выдаёт - нулевой вектор, а в этом случае для начала а конца слова будут учиться специальные вектора)\n",
    "\n",
    "Также мы предлагаем Вам не ограничиваться этим списком, а придумать свои способы улучшить качество определения частей речи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Заново соберем все данные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepate_data():    \n",
    "    full_train = pyconll.load_from_file('./datasets/ru_syntagrus-ud-train.conllu')\n",
    "    full_test = pyconll.load_from_file('./datasets/ru_syntagrus-ud-dev.conllu')\n",
    "\n",
    "    MAX_SENT_LEN = max(max(len(sent) for sent in full_train), max(len(sent) for sent in full_test)) \n",
    "    MAX_ORIG_TOKEN_LEN = max(max(len(token.form) for sent in full_train for token in sent), \n",
    "                            max(len(token.form or [0]) for sent in full_test for token in sent))\n",
    "\n",
    "    all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "\n",
    "    train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n",
    "    char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\n",
    "\n",
    "    UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n",
    "    label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n",
    "\n",
    "    train_inputs, train_labels, err1 = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "    train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "\n",
    "    test_inputs, test_labels, err2 = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "    test_dataset = TensorDataset(test_inputs, test_labels)\n",
    "\n",
    "    del train_char_tokenized, full_train, full_test, train_inputs, test_inputs\n",
    "    gc.collect()\n",
    "\n",
    "    return char_vocab, label2id, train_dataset, test_dataset\n",
    "\n",
    "# char_vocab, label2id, train_dataset, test_dataset = prepate_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Добавили разреженное ядро (`dilation = 2`)**\n",
    "- остальное не меняем\n",
    "- время эпохи увеличилось на 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 80338\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.deterministic=False        # пока неясно зачем, но так рекомендуют для dilation сверток\n",
    "\n",
    "sentence_level_model_homework = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                                      single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.1, dilation=2),\n",
    "                                                      context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.1))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_homework.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0\n",
      "Эпоха: 20 итераций, 2.45 сек\n",
      "Среднее значение функции потерь на обучении 1.0558781012892724\n",
      "Среднее значение функции потерь на валидации 0.4848137540476663\n",
      "Новая лучшая модель!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if LOAD_LAST_MODEL:\n",
    "    try:\n",
    "        msg = sentence_level_model_homework.load_state_dict(torch.load('./models/sentence_level_model_homework.pth'))\n",
    "        print(\"Loading pretrained: \", msg)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Конфигурация сети поменялась, сохраненная модель не подходит\")\n",
    "        raise e\n",
    "else:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    (val_loss,\n",
    "    sentence_level_model_homework) = train_eval_loop(sentence_level_model_homework,\n",
    "                                                    train_dataset,\n",
    "                                                    test_dataset,\n",
    "                                                    F.cross_entropy,\n",
    "                                                    lr=5e-3,\n",
    "                                                    epoch_n=NUM_TRAIN_EPOCHS,\n",
    "                                                    batch_size=64,\n",
    "                                                    device='cuda',\n",
    "                                                    early_stopping_patience=5,\n",
    "                                                    max_batches_per_epoch_train=500,\n",
    "                                                    max_batches_per_epoch_val=100,\n",
    "                                                    lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                                factor=0.5,\n",
    "                                                                                                                                verbose=True))\n",
    "    if not QUICK_RUN:\n",
    "        torch.save(sentence_level_model_homework.state_dict(), './models/sentence_level_model_homework.pth')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метрики:**\n",
    "- на трейне улучшились 0,93\n",
    "- на тесте не поменялись 0,89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [00:01, 36.49it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.2614191770553589\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00     68557\n",
      "         ADJ       0.58      0.34      0.43      1349\n",
      "         ADP       0.89      0.64      0.75      1638\n",
      "         ADV       0.27      0.15      0.20      1021\n",
      "         AUX       0.00      0.00      0.00       125\n",
      "       CCONJ       0.67      0.67      0.67       641\n",
      "         DET       0.00      0.00      0.00       404\n",
      "        INTJ       0.00      0.00      0.00         7\n",
      "        NOUN       0.47      0.83      0.60      3925\n",
      "         NUM       0.00      0.00      0.00       174\n",
      "        PART       0.82      0.48      0.60       451\n",
      "        PRON       0.54      0.32      0.40       912\n",
      "       PROPN       0.00      0.00      0.00       414\n",
      "       PUNCT       0.96      1.00      0.98      3538\n",
      "       SCONJ       0.00      0.00      0.00       257\n",
      "         SYM       0.00      0.00      0.00         2\n",
      "        VERB       0.56      0.59      0.57      2403\n",
      "           X       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.92     85820\n",
      "   macro avg       0.37      0.33      0.34     85820\n",
      "weighted avg       0.92      0.92      0.92     85820\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101%|██████████| 14/13.90625 [00:00<00:00, 36.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.485891193151474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       0.99      1.00      1.00     22593\n",
      "         ADJ       0.59      0.35      0.44       895\n",
      "         ADP       0.87      0.62      0.73       875\n",
      "         ADV       0.11      0.07      0.09       275\n",
      "         AUX       0.00      0.00      0.00        58\n",
      "       CCONJ       0.58      0.73      0.64       224\n",
      "         DET       0.00      0.00      0.00       145\n",
      "        NOUN       0.54      0.79      0.64      2513\n",
      "         NUM       0.00      0.00      0.00       299\n",
      "        PART       0.84      0.37      0.52       142\n",
      "        PRON       0.42      0.35      0.38       170\n",
      "       PROPN       0.00      0.00      0.00       590\n",
      "       PUNCT       0.81      0.93      0.87      1574\n",
      "       SCONJ       0.00      0.00      0.00        85\n",
      "        VERB       0.31      0.51      0.39       652\n",
      "           X       0.00      0.00      0.00        60\n",
      "\n",
      "    accuracy                           0.88     31150\n",
      "   macro avg       0.38      0.36      0.36     31150\n",
      "weighted avg       0.86      0.88      0.87     31150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(sentence_level_model_homework, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(sentence_level_model_homework, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "TEST_UNIQUE_TAGS = np.array(UNIQUE_TAGS)[np.unique(test_labels)].tolist()   # классов в тесте может быть меньше\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=TEST_UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Заковыристые тестовые фразы:**\n",
    "- распознавание не улучшилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 83.72it/s]                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама-NOUN мыла-VERB раму-NOUN\n",
      "\n",
      "косил-VERB косой-ADJ косой-NOUN косой-NOUN\n",
      "\n",
      "глокая-NOUN куздра-NOUN штеко-NOUN будланула-VERB бокра-NOUN и-CCONJ куздрячит-NOUN бокрёнка-NOUN\n",
      "\n",
      "сяпала-VERB калуша-NOUN с-ADP калушатами-NOUN по-ADP напушке-NOUN\n",
      "\n",
      "пирожки-NOUN поставлены-NOUN в-ADP печь-NOUN мама-NOUN любит-NOUN печь-NOUN\n",
      "\n",
      "ведро-VERB дало-VERB течь-NOUN вода-VERB стала-VERB течь-NOUN\n",
      "\n",
      "три-NOUN да-NOUN три-NOUN будет-NOUN дырка-NOUN\n",
      "\n",
      "три-NOUN да-NOUN три-NOUN будет-NOUN шесть-NOUN\n",
      "\n",
      "сорок-NOUN сорок-NOUN\n",
      "\n",
      "он-PRON видел-VERB их-CCONJ семью-NOUN своими-NOUN глазами-NOUN\n",
      "\n",
      "эти-VERB типы-VERB стали-VERB есть-NOUN в-ADP цехе-NOUN\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_level_pos_tagger_dilation = POSTagger(sentence_level_model_homework, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "\n",
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger_dilation(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
