{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация кода по вопросам со StackOverflow\n",
    "\n",
    "[Оригинал](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)\n",
    "\n",
    "Обучим seq2seq модель, которая, по последовательности токенов одного вида (а именно — последовательности слов естественного языка, из которых состоит вопрос), генерирует последовательность токенов другого вида — а именно, команды, операторы, имена переменных, пунктуация... которую мы используем при написании кода. Для простоты ограничимся примерами из одного языка — python.\n",
    "\n",
    "Самые популярные sequence to sequence модели — это модели вида \"**encoder-decorder**\", которые разбирались в лекции:\n",
    "- они используют RNN для того, чтобы закодировать входную последовательность в некоторый вектор (представление входного предложения в некоторым заранее зафиксированном формате)\n",
    "- этот вектор декодируется второй RNN (декодером), который учится предсказывать выходную последовательность, генерируя последовательно токен за токеном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUICK_RUN = True    # использовать уже готовую модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Еще раз теория\n",
    "\n",
    "<img src=\"./img/seq2seq.png\" width=\"500\">\n",
    "\n",
    "**RNN энкодер**\n",
    "- на вход мы подаём токенизированную фразу последовательно по 1 токену\n",
    "  - в каждый момент времени, входом энкодера является текущей токен — назовём его \"x\", а также некоторое скрытое состояние — назовём его \"h\" (от слова \"hidden\")\n",
    "- на каждый следующий скрытый слой подается скрытое состояние предыдущего шага\n",
    "  - на выходе новое скрытое состояние\n",
    "  - скрытое состояние содержит в себе информацию обо всём предложении, которую сеть видела к текущему моменту\n",
    "- нулевое скрытое состояние можно инициализировать нулями или использовать, например, равномерное распределение\n",
    "- как только последнее слово было передано в RNN, будем использовать последнее скрытое состояние как вектор, содержащий в себе информацию обо всём предложении\n",
    "\n",
    "**RNN декодер**\n",
    "- имея выход энкодера, можно начинать декодировать его — генерировать выходную последовательность\n",
    "- нулевое скрытое состояние декодера - это последнее скрытое состояние энкодера\n",
    "- на каждом шаге нам нужно предсказывать следующее слово\n",
    "  - для этого будем на каждом шаге пропускать текущее скрытое состояние через линейный слой и предсказывать следующее слово\n",
    "- после того как мы сгенерировали всю выходную последовательность, мы можем сравнить её с переводом из нашей обучающей выборки\n",
    "\n",
    "**Обновление весов**\n",
    "- считаем функцию потерь\n",
    "- делаем backward-шаг, посчитав градиент функции потерь\n",
    "- обновляем веса сети, в направлении минимизации функции потерь\n",
    "\n",
    "**Дополнительные фишки**\n",
    "\n",
    "- \"**teacher forcing**\"[1,2] - метод для обучения RNN, который в некотором (заранее зафиксированном) проценте случаев использует в качестве входа \"[ground truth](https://en.wikipedia.org/wiki/Ground_truth#Statistics_and_machine_learning)\" (короче, таргет) с предыдущего шага, а не предсказанное сетью значение\n",
    "- подход, который касается **длины генерируемой последовательности**:\n",
    "  - не обязательно ждать, пока модель сгенерирует end-of-sequence токен\n",
    "  - вместо этого, прекратить генерацию, когда мы выдали достаточное количество слов\n",
    "    - например, когда длина выходной последовательности стала примерно равна длине и входной последовательности\n",
    "  - это позволит нам избежать слишком долгого обучения, либо генерирования излишнего количества символов в конце последовательности (которые скорее всего будут = `<PAD>`)\n",
    "\n",
    "[1] Williams R. J., Zipser D. A learning algorithm for continually running fully recurrent neural networks //Neural computation. – 1989. – Т. 1. – №. 2. – С. 270-280.  \n",
    "[2] Lamb A. M. et al. Professor forcing: A new algorithm for training recurrent networks //Advances In Neural Information Processing Systems. – 2016. – С. 4601-4609."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle,\n",
    "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
    "\n",
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "# import sys; sys.path.append('./stepik-dl-nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(0)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Еще полезные штуки\n",
    "\n",
    "- Таблица параметров модели. Типа подробный `get_parameters()` \n",
    "- Все тензоры ноутбука\n",
    "- Очистить память от ненужных тензоров/моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.summary import count_parameters, dump_tensors, free_mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим, что представляют собой данные\n",
    "- intent - вопрос, заданные стаковерфлоу\n",
    "- snippet - кусок кода в самом популярном ответе на вопрос"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to convert a list of multiple integers int...</td>\n",
       "      <td>sum(d * 10 ** i for i, d in enumerate(x[::-1]))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to convert a list of multiple integers int...</td>\n",
       "      <td>r = int(''.join(map(str, x)))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how to convert a datetime string back to datet...</td>\n",
       "      <td>datetime.strptime('2010-11-13 10:33:54.227806'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Averaging the values in a dictionary based on ...</td>\n",
       "      <td>[(i, sum(j) / len(j)) for i, j in list(d.items...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zip lists in python</td>\n",
       "      <td>zip([1, 2], [3, 4])</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              intent  \\\n",
       "0  How to convert a list of multiple integers int...   \n",
       "1  How to convert a list of multiple integers int...   \n",
       "2  how to convert a datetime string back to datet...   \n",
       "3  Averaging the values in a dictionary based on ...   \n",
       "4                                zip lists in python   \n",
       "\n",
       "                                             snippet  \n",
       "0    sum(d * 10 ** i for i, d in enumerate(x[::-1]))  \n",
       "1                      r = int(''.join(map(str, x)))  \n",
       "2  datetime.strptime('2010-11-13 10:33:54.227806'...  \n",
       "3  [(i, sum(j) / len(j)) for i, j in list(d.items...  \n",
       "4                                zip([1, 2], [3, 4])  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./datasets/stackoverflow_code_generation/conala/conala-train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужно написать токенизаторы, которые помогут нам поделить на токены вопросы со StackOverflow и кусочек кода\n",
    "\n",
    "- `tokenize_question` токенизирует наш вопрос — делает это с помощью простой регулярки, убирает слишком длинные слова, которые, скорее всего, являются названиями веб-сайтов, либо слишком длинными названиями каких-то текстовых полей и т.п. Возвращает токены в обратном порядке: `[::-1]`. Зачем в обратном порядке:\n",
    "  - значительно (?) улучшает работу LSTM и подобных сетей, причем точно не ясно почему (работает только в однонаправленном варианте, тут конкретно это ничего не даст, но и не поломает ничего, т.к. RNN инвариантна к развороту последовательности (но не к порядку как CNN)). Тут это просто чтоб показать, как обычно делают:\n",
    "    - предполагается, что когда сеть обучается и сравнивают данные и таргет, то их \"как бы конкатенируют последовательно друг за другом\", т.е. развернутые первые токены входа оказываются рядом с первыми токенами выхода и сеть лучше улавливает их корреляцию (звучит как-то неубедительно, но объясняют [так](https://arxiv.org/abs/1409.3215))\n",
    "\n",
    "- `tokenize_snippet` токенизирует кусок кода похожим образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_question(text):\n",
    "    \"\"\"\n",
    "    Tokenizes question from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return list(filter(lambda x: len(x) < 16, re.findall(r\"[\\w']+\", text)[::-1]))\n",
    "\n",
    "def tokenize_snippet(text):\n",
    "    \"\"\"\n",
    "    Tokenizes code snippet into a list of operands\n",
    "    \"\"\"\n",
    "    return list(filter(lambda x: len(x) < 10, re.findall(r\"[\\w']+|[.,!?;:@~(){}\\[\\]+-/=\\\\\\'\\\"\\`]\", text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработчик входныx данных на `torchtext`\n",
    "\n",
    "- `Field` позволяет нам определить, как должны обрабатываться данные (т.е. переводится текст в тензоры)\n",
    "  - создаем поля SRC и TRG, они будут образованы функциями токенизации с добавлением токенов начала/конца последовательности и переводом всего текста в нижний регистр\n",
    "  - для вопросов будет также учитываться их длина (пригодится для обучения модели)\n",
    "- `TabularDataset` класс для создания объекта датасета из файлам с данным (cvs, tsv, json)\n",
    "  - проходит по 3 файлам, где выборка уже поделена авторами курса на обучающую (2000 вопросов), валидационную (379) и тестовую (500)\n",
    "  - если надо, может сам делить на трейн/тест (т.к. потомок `Dataset`)\n",
    "- `BucketIterator` итератор для получения пакетов (батчей) из датасета (далее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "SRC = Field(\n",
    "    tokenize = tokenize_question, \n",
    "    init_token = '<sos>', \n",
    "    eos_token = '<eos>', \n",
    "    lower = True,\n",
    "    include_lengths = True\n",
    ")\n",
    "\n",
    "TRG = Field(\n",
    "    tokenize = tokenize_snippet, \n",
    "    init_token = '<sos>', \n",
    "    eos_token = '<eos>', \n",
    "    lower = True\n",
    ")\n",
    "\n",
    "fields = {\n",
    "    'intent': ('src', SRC),\n",
    "    'snippet': ('trg', TRG)\n",
    "}\n",
    "\n",
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "train_data, valid_data, test_data = TabularDataset.splits(\n",
    "                            path = 'datasets/stackoverflow_code_generation/conala/',\n",
    "                            train = 'conala-train.csv',\n",
    "                            validation = 'conala-valid.csv',\n",
    "                            test = 'conala-test.csv',\n",
    "                            format = 'csv',\n",
    "                            fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, данные с похожей тематикой можно найти, загуглив следующее сочетание слов: \"StackOverflow question code dataset\". Это датасет, намайненный автоматически со StackOverflow, с помощью \"Bi-View Hierarchical Neural Network\". Там порядка 150 тыс. вопросов. Cтатья про \"bi view hierarchical neural network\" была представлена в 2018 году[1].\n",
    "\n",
    "[1] Yao Z. et al. Staqc: A systematically mined question-code dataset from stack overflow //Proceedings of the 2018 World Wide Web Conference. – 2018. – С. 1693-1703."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cформируем словарь\n",
    "\n",
    "Задаём максимально возможный размер слоя и минимальную встречаемость слова для того, чтобы попасть в словарь (3 для вопросов, 5 для кода).\n",
    "\n",
    "Чтобы использовать паддинг, `torchtext` требует, чтобы все элементы в батче были отсортированы по их длине до применения паддинга, в убывающем порядке. То есть, первая последовательность должна быть самой длинной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 1285), ('in', 949), ('python', 922), ('to', 851), ('how', 633), ('of', 602), ('list', 558), ('string', 397), ('the', 328), ('from', 275), ('with', 228), ('pandas', 192), ('i', 191), ('dictionary', 162), ('get', 151), ('convert', 134), ('values', 131), ('do', 125), ('dataframe', 111), ('into', 110)]\n",
      "[(')', 3480), ('(', 3475), ('.', 2595), (',', 1899), ('[', 1122), (']', 1121), ('=', 927), (\"'\", 885), ('\\\\', 697), (':', 587), ('in', 504), ('x', 498), ('\"', 496), ('for', 450), ('1', 377), ('-', 279), ('a', 265), ('0', 259), ('/', 257), ('df', 234)]\n",
      "Уникальные токены в словаре интентов: 612\n",
      "Уникальные токены в словаре сниппетов: 395\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab([train_data.src], max_size=25000, min_freq=3)\n",
    "print(SRC.vocab.freqs.most_common(20))\n",
    "\n",
    "\n",
    "TRG.build_vocab([train_data.trg], min_freq=5)\n",
    "print(TRG.vocab.freqs.most_common(20))\n",
    "\n",
    "print(f\"Уникальные токены в словаре интентов: {len(SRC.vocab)}\")\n",
    "print(f\"Уникальные токены в словаре сниппетов: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: 2000\n",
      "Размер валидационной выборки: 379\n",
      "Размер тестовой выборки: 500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер обучающей выборки: {len(train_data.examples)}\")\n",
    "print(f\"Размер валидационной выборки: {len(valid_data.examples)}\")\n",
    "print(f\"Размер тестовой выборки: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итератор для батчей\n",
    "\n",
    "Экземпляр `BucketIterator` возвращает батчи с данными, у которых есть атрибут \n",
    "- \"src\" — это тензоры, кодирующие входные вопросы\n",
    "- \"trg\" (target) — это тензоры, кодирующие сниппеты с кодом. \n",
    "\n",
    "Удобно, что torchtext-итераторы умеют \n",
    "- автоматически добавлять паддинг\n",
    "-  `BucketIterator` умеет минимизировать количество паддинга для входов и выходов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     sort_within_batch = True,\n",
    "     sort_key = lambda x : len(x.src),\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс энкодера\n",
    "\n",
    "Двухслойный LSTM. *И мы используем модификацию **GRU** (Управляемые рекуррентные нейроны, Gated recurrent units), что почти тоже самое, но чуть меньше параметров, чуть меньше операций обновления весов, чуть меньше переобучается, при этом на практике показатели качества практически от LSTM не отличаются* \n",
    "\n",
    "Для многослойной LSTM/GRU входная последовательность идёт в первый слой сети, а скрытое состояние первого слоя используется как входная последовательность следующего слоя. Скрытое состояние первого слоя можно представить формулой, зависящей от входных токенов и от предыдущего скрытого состояния. Напомню что, в отличие от RNN, LSTM, кроме того, что берёт на вход предыдущее скрытые состояние и возвращает следующее, ещё и принимает на вход так называемое \"cell state\". Его обычно обозначают буквой \"c\". Можно воспринимать его как другой вид скрытого состояния. В итоге, конечное представление входной последовательности в виде вектора будет конкатенацией скрытого состояния и нашего cell state, которое будем обозначать буквой \"с\".\n",
    "\n",
    "`__init__` \n",
    "- входная размерность данных — это размерности наших \"one-hot\" векторов\n",
    "- размерность эмбеддингов, размерность слоя с эмбеддингами — например, можно сделать его равным 100 или 200, любому другому числу, которое кажется вам наиболее подходящим (зашибись теория)\n",
    "-  \"encoding hidden dimension\" (вот он) — это размерность скрытого состояния (количество ячеек памяти, максимальная длина взаимосвязанной последовательности)\n",
    "-  \"dec_hid_dim\" - размерность, которую нужно выдавать на декодер\n",
    "-   \"dropout\" — это количество дропаута. Он будет включаться между ембеддингом и рекурренткой. Дропаут в рекуррентных сетях - это отдельная тема, мы не можем нарушать рекуррентные связи внутри RNN, иначе мы фактически лишаем сеть памяти\n",
    "\n",
    "`forward`\n",
    "- ембеддинг -> дропаут -> упаковка -> RNN -> распаковка -> tanh -> скрытое состояние\n",
    "- все размерности подписаны, можно поразбираться\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #src_len = [src sent len]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))    # dropout(!)\n",
    "        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.detach().cpu())   # тензор длины д.б. на хосте\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "                     \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "\n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [sent len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упаковка, распаковка** выровненных паддингом последовательностей\n",
    "- все RNN классы умеют принимать данные в таком виде\n",
    "- длины последовательностей must be on the CPU if provided as a tensor\n",
    "- справка ни о чем, походу, это просто `vstack` всех последовательностей в одну"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-1.0115,  0.2167],\n",
       "        [-0.6123,  0.5036],\n",
       "        [ 0.0461,  0.4024],\n",
       "        [ 0.0461,  0.4024],\n",
       "        [-1.0115,  0.2167],\n",
       "        [-0.6123,  0.5036],\n",
       "        [ 0.2310,  0.6931],\n",
       "        [-0.2669,  2.1785]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([4, 4, 4, 4]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 0, 0], \n",
    "                  [1, 2, 3, 4]])                # Batch X MaxIn\n",
    "emb = nn.Embedding(x.shape[-1] + 1, 2)(x)       # Batch X MaxIn X Emb\n",
    "packed = nn.utils.rnn.pack_padded_sequence(emb, [4, 4, 4, 4])\n",
    "packed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Механизм внутреннего внимания\n",
    "\n",
    "Будем использовать стандартный attention, который посчитает нам веса по нашей входной последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src sent len]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat encoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src sent len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src sent len, dec hid dim]\n",
    "                \n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        #energy = [batch size, dec hid dim, src sent len]\n",
    "        \n",
    "        #v = [dec hid dim]\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, dec hid dim]\n",
    "            \n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #attention = [batch size, src sent len]\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс декодера\n",
    "\n",
    "Ахритерктура аналогичная энкодеру.\n",
    "\n",
    "- первый слой получает пару с прошлого шага и прогоняет её через нашу сеть вместе с текущим токеном, чтобы предсказать новую пару\n",
    "- прогоняем наше скрытое состояние с верхнего слоя через линейный слой, чтобы сделать предсказание следующего токена в выходном генерируемом предложении\n",
    "\n",
    "Параметры:\n",
    "- \"output_dimension\" (это размер one-hot векторов, которые подаются на вход декодеру). Это число должно быть равно размеру словаря таргета.\n",
    "\n",
    "Форвард:\n",
    "- unsqueeze к входным токенам, чтобы добавить ещё одну размерность\n",
    "- аналогично энкодеру, применяем эмбеддинг-слой и дропаут.\n",
    "- применяем attention\n",
    "- батч с токенами передаём в RNN вместе с \"h\" и \"c\" векторами с предыдущего шага\n",
    "- получаем на выходе output (это скрытое состояние с верхнего слоя нашей сети), новое скрытое состояние и новое cell состояние, то есть новые вектора \"h\" и \"c\"\n",
    "- прогоняем output (после того, как избавились от лишней размерности) через линейный слой, чтобы получить предсказание следующего слова в нашей последовательности\n",
    "- возвращать здесь мы будем, собственно — output, скрытое состояние и вектор \"а\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src sent len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "                \n",
    "        #a = [batch size, src sent len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src sent len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [sent len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #output = [bsz, output dim]\n",
    "        \n",
    "        return output, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq модель\n",
    "\n",
    "Объединяет все это в кучу.\n",
    "\n",
    "Входные параметры seq2seq модели —это энкодер, декодер, device, а также токены, с помощью которых мы кодируем паддинг, начало последовательности и конец последовательности.\n",
    "\n",
    "Нужно убедиться, что количество слоёв и размерность скрытых состояний — одинакова для энкодера и декодера. Это не обязательное условие, но в противном случае (в случае разного количества слоёв) нам придётся придумывать некоторые способы это обойти\n",
    "- Например, если в энкодере два слоя, а в декоре один, то можно использовать оба вектора, либо можно их усреднить.\n",
    "\n",
    "\"forward\"-метод в классе seq2seq берёт на вход вопрос, сниппет с кодом и \"teacher forcing ratio\".  На выход он отдаёт нам output, а также веса attention, которые мы посчитали в модуле attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx, sos_idx, eos_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        if trg is None:\n",
    "            assert teacher_forcing_ratio == 0, \"Must be zero during inference\"\n",
    "            inference = True\n",
    "            trg = torch.zeros((100, src.shape[1])).long().fill_(self.sos_idx).to(src.device)\n",
    "        else:\n",
    "            inference = False\n",
    "            \n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #tensor to store attention\n",
    "        attentions = torch.zeros(max_len, batch_size, src.shape[0]).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "                \n",
    "        #mask = [batch size, src sent len]\n",
    "                \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attention = self.decoder(output, hidden, encoder_outputs, mask)\n",
    "            outputs[t] = output\n",
    "            attentions[t] = attention\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "            if inference and output.item() == self.eos_idx:\n",
    "                return outputs[:t], attentions[:t]\n",
    "            \n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициализация модели\n",
    "\n",
    "Сделаем так, чтобы количество слоёв у энкодера и декодера, а также размерности векторов скрытых состояний были одинаковыми. \n",
    "\n",
    "dropout будет достаточно большим (\"0.8\"). Если меньше, то плохо учится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "ENC_HID_DIM = 100\n",
    "DEC_HID_DIM = 100\n",
    "ENC_DROPOUT = 0.8\n",
    "DEC_DROPOUT = 0.8\n",
    "PAD_IDX = SRC.vocab.stoi['<pad>']\n",
    "SOS_IDX = TRG.vocab.stoi['<sos>']\n",
    "EOS_IDX = TRG.vocab.stoi['<eos>']\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, PAD_IDX, SOS_IDX, EOS_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так задаются в торче пользовательские функции иницализации:\n",
    "- параметры веса - нормальный шум\n",
    "- остальные параметры - 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(612, 128)\n",
       "    (rnn): GRU(128, 100, bidirectional=True)\n",
       "    (fc): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (dropout): Dropout(p=0.8, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=300, out_features=100, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(395, 128)\n",
       "    (rnn): GRU(328, 100)\n",
       "    (out): Linear(in_features=428, out_features=395, bias=True)\n",
       "    (dropout): Dropout(p=0.8, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель содержит 615,651 параметров\n"
     ]
    }
   ],
   "source": [
    "def cnt_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Модель содержит {cnt_parameters(model):,} параметров')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+-----------+------------+---------------+----------+\n",
      "|         Modules/Tensors          | GPU |   Shape   | Parameters |      Type     |  Memory  |\n",
      "+----------------------------------+-----+-----------+------------+---------------+----------+\n",
      "|     encoder.embedding.weight     |  +  | 612 x 128 |   78336    | torch.float32 | 6893568  |\n",
      "|     encoder.rnn.weight_ih_l0     |  +  | 300 x 128 |   38400    | torch.float32 | 3379200  |\n",
      "|     encoder.rnn.weight_hh_l0     |  +  | 300 x 100 |   30000    | torch.float32 | 2640000  |\n",
      "|      encoder.rnn.bias_ih_l0      |  +  |    300    |    300     | torch.float32 |  26400   |\n",
      "|      encoder.rnn.bias_hh_l0      |  +  |    300    |    300     | torch.float32 |  26400   |\n",
      "| encoder.rnn.weight_ih_l0_reverse |  +  | 300 x 128 |   38400    | torch.float32 | 3379200  |\n",
      "| encoder.rnn.weight_hh_l0_reverse |  +  | 300 x 100 |   30000    | torch.float32 | 2640000  |\n",
      "|  encoder.rnn.bias_ih_l0_reverse  |  +  |    300    |    300     | torch.float32 |  26400   |\n",
      "|  encoder.rnn.bias_hh_l0_reverse  |  +  |    300    |    300     | torch.float32 |  26400   |\n",
      "|        encoder.fc.weight         |  +  | 100 x 200 |   20000    | torch.float32 | 1760000  |\n",
      "|         encoder.fc.bias          |  +  |    100    |    100     | torch.float32 |   8800   |\n",
      "|       decoder.attention.v        |  +  |    100    |    100     | torch.float32 |   8800   |\n",
      "|  decoder.attention.attn.weight   |  +  | 100 x 300 |   30000    | torch.float32 | 2640000  |\n",
      "|   decoder.attention.attn.bias    |  +  |    100    |    100     | torch.float32 |   8800   |\n",
      "|     decoder.embedding.weight     |  +  | 395 x 128 |   50560    | torch.float32 | 4449280  |\n",
      "|     decoder.rnn.weight_ih_l0     |  +  | 300 x 328 |   98400    | torch.float32 | 8659200  |\n",
      "|     decoder.rnn.weight_hh_l0     |  +  | 300 x 100 |   30000    | torch.float32 | 2640000  |\n",
      "|      decoder.rnn.bias_ih_l0      |  +  |    300    |    300     | torch.float32 |  26400   |\n",
      "|      decoder.rnn.bias_hh_l0      |  +  |    300    |    300     | torch.float32 |  26400   |\n",
      "|        decoder.out.weight        |  +  | 395 x 428 |   169060   | torch.float32 | 14877280 |\n",
      "|         decoder.out.bias         |  +  |    395    |    395     | torch.float32 |  34760   |\n",
      "+----------------------------------+-----+-----------+------------+---------------+----------+\n",
      "Total Trainable Params: 615651\n",
      "Total memory: 51.67 Mb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "615651"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define our optimizer and criterion. We have already initialized `PAD_IDX` when initializing the model, so we don't need to do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- переводим модель в training mode (с помощью model.train). \n",
    "  - Это включит dropout и батч-нормализацию (если бы она была, но, в нашем случае, мы её не используем). \n",
    "- итерируемся через батч-итератор. Что мы делаем на каждой итерации? \n",
    "  - берём входное и выходное предложение из батча, вместо входного предложения мы получаем пару — \"входное предложение и его длина\" (как мы уже обсуждали ранее). \n",
    "  - делаем zero_grad — обнуляем градиенты, посчитанные на предыдущем шаге, \n",
    "  - передаём source и target в нашу модель и получаем некоторый выход и веса attention, \n",
    "  - считаем градиенты с помощью \"loss backward\", предварительно посчитав функцию потерь, \n",
    "  - клипаем (clip) градиенты (это популярный паттерн, и недопускает \"взрыва градиента\")\n",
    "  - делаем шаг нашим оптимизатором и считаем лосс. \n",
    "\n",
    "Замечательно, на выход наша функция будет возвращать нормализованный (по выборке) лосс по нашей эпохе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src, src_len = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, attetion = model(src, src_len, trg, 0.4)    #turn ON teacher forcing\n",
    "        \n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также у нас есть функция, которую мы будем использовать для оценивания качества модели (для evaluate). Здесь, вначале, мы переводим нашу модель в состояние оценивания качества (это означает \"выключить dropout\", \"выключить батч-нормализацию\") и дальше проделываем примерно такие же шаги, как и в функции \"train\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src, src_len = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, attention = model(src, src_len, trg, 0.0)   #turn OFF teacher forcing\n",
    "\n",
    "            #trg = [trg sent len, batch size]\n",
    "            #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg sent len - 1) * batch size]\n",
    "            #output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Кроме того, давайте напишем функцию, которая будет замерять время, потраченное на каждую эпоху, и назовём её \"epoch_time\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Перплексия** в теории информации — безразмерная величина, мера того, насколько хорошо распределение вероятностей предсказывает выборку. Низкий показатель перплексии указывает на то, что распределение вероятности хорошо предсказывает выборку.\n",
    "\n",
    "Формула перплексии:\n",
    "\n",
    "$$PP(W) = P(w_1, w_2, .., w_N)^{-\\frac{1}{N}} = \\sqrt[N]{\\dfrac{1}{P(w_1, w_2, .., w_N)}} = \\sqrt[N]{\\dfrac{1}{\\prod_{i=1}^N P(w_i | w_1, .., w_{i-1})}}$$\n",
    "​\n",
    "- Можно использовать перплексию для сравнения двух нейросетей с разными архитектурами на одной и той же задачи (обе нейросети обучены переводить текст с языка А на язык Б)\n",
    "- Перплексия связного текста ниже, чем перплексия произвольного набора слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минимум 100 эпох, чтоб не совсем мусор генерился, хоть и по памяти из трейна (датасет совсем небольшой)\n",
    "- около 10 мин."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "CLIP = 1        # более высокие градиенты обрезаются до этого значения\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not QUICK_RUN:  \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), './models/conala_model_attention_test.pt')\n",
    "        \n",
    "        print(f'Эпоха: {epoch+1:02} | Время: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'Перплексия (обучение): {math.exp(train_loss):7.3f}')\n",
    "        print(f'Перплексия (валидация): {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перплексия (валидация):  31.944\n"
     ]
    }
   ],
   "source": [
    "if QUICK_RUN:\n",
    "    model.load_state_dict(torch.load('./models/conala_model_attention_test.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'Перплексия (валидация): {math.exp(test_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание кода по вопросу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = tokenize_question(sentence) \n",
    "    tokenized = ['<sos>'] + [t.lower() for t in tokenized] + ['<eos>']\n",
    "    numericalized = [SRC.vocab.stoi[t] for t in tokenized] \n",
    "    sentence_length = torch.LongTensor([len(numericalized)])\n",
    "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device) \n",
    "    translation_tensor_logits, attention = model(tensor, sentence_length, None, 0) \n",
    "    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n",
    "    translation = [TRG.vocab.itos[t] for t in translation_tensor]\n",
    "    translation, attention = translation[1:], attention[1:]\n",
    "    return translation, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def display_attention(candidate, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    ax.matshow(attention, cmap='bone')    # 2д матрица -> тепловая карта\n",
    "        \n",
    "    question = ['<sos>'] + [t.lower() for t in tokenize_question(candidate)] + ['<eos>']\n",
    "    answer = translation\n",
    "    \n",
    "    ax.set(xticks=np.arange(attention.shape[-1]),\n",
    "           yticks=np.arange(attention.shape[0]),\n",
    "           xticklabels=question, \n",
    "           yticklabels=answer,\n",
    "           title=\"Карта внимания\",\n",
    "           xlabel='Запрос',\n",
    "           ylabel='Ответ')\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",rotation_mode=\"default\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = object datetime to back string datetime a convert to how\n",
      "trg = datetime . strptime ( '2010 - 11 - 13 10 : 33 : 54 . 227806' , ' y - m - d h : m : s . f' )\n"
     ]
    }
   ],
   "source": [
    "example_idx = 2\n",
    "\n",
    "src = ' '.join(vars(train_data.examples[example_idx])['src'])\n",
    "trg = ' '.join(vars(train_data.examples[example_idx])['trg'])\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg =  datetime . datetime . now ( ) )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 12])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation, attention = translate_sentence(model, src)\n",
    "\n",
    "print('predicted trg = ', ' '.join(translation))\n",
    "\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = string to tuple convert python\n",
      "trg = \" \" \" \" \" \" . join ( ( 'a' , 'b' , 'c' , 'd' , 'g' , 'x' , 'r' , 'e' ) )\n"
     ]
    }
   ],
   "source": [
    "example_idx = 8\n",
    "\n",
    "src = ' '.join(vars(valid_data.examples[example_idx])['src'])\n",
    "trg = ' '.join(vars(valid_data.examples[example_idx])['trg'])\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg =  <unk> ( ' ( ' , <unk> )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation, attention = translate_sentence(model, src)\n",
    "\n",
    "print('predicted trg = ', ' '.join(translation))\n",
    "\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = 8 utf to 1251 cp from string a convert to how\n",
      "trg = d . decode ( 'cp1251' ) . encode ( 'utf8' )\n"
     ]
    }
   ],
   "source": [
    "example_idx = 4\n",
    "\n",
    "src = ' '.join(vars(test_data.examples[example_idx])['src'])\n",
    "trg = ' '.join(vars(test_data.examples[example_idx])['trg'])\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg =  print(<unk>.<unk>(<unk>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation, attention = translate_sentence(model, src)\n",
    "\n",
    "print('predicted trg = ', ''.join(translation))\n",
    "\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# До.за.\n",
    "\n",
    "При обучении seq2seq модели из семинара качество работы модели оказалось не слишком хорошим. У этого есть несколько причин:\n",
    "\n",
    "- Слишком маленький датасет. На 2000 пар сложно обучить хорошую модель для решения такой сложной задачи.\n",
    "- Модель очень простая (чиво?:)), есть смысл попробовать усложнить ее (использовать другие архитектуры, например, Трансформер, разобранный в предыдущих лекциях и семинарах).\n",
    "- Стоит более аккуратно подбирать параметры модели. Обратите внимание на:\n",
    "  - Процесс построения словаря. Может быть, нужно поварьировать параметры min_freq, max_freq.\n",
    "  - Dropout. Есть ли смысл добавлять еще больше dropout в модель?\n",
    "  - Стратегия изменения learning rate в процессе обучения.\n",
    "  - \n",
    "В качестве домашнего задания мы предлагаем Вам поэкспериментировать с кодом этого семинара и улучшить качество работы модели."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
