{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNKaJz5j_ylj"
   },
   "source": [
    "# Определение эмоциональной окраски твитов с помощью BERT. Ужатая версия BERT\n",
    "\n",
    "Попробуем воспроизвести примерно то же, что и на семинаре, но с использованием **уменьшенной версии модели BERT** (ориг. [Маленький и быстрый BERT для русского языка](https://habr.com/ru/post/562064/))\n",
    "\n",
    "    \"В качестве основы для модели я взял классический bert-multilingual (веса), ибо хочу, чтобы модель понимала и русский, и английский, и его же использую на ранних стадиях дистилляции как учителя по распределению токенов. Словарь этой модели содержит 120К токенов, но я отобрал только те, которые часто встречаются в русском и английском языках, оставив 30К. Размер эмбеддинга я сократил с 768 до 312, число слоёв – с 12 до 3. Эмбеддинги я инициализировал из bert-multilingual, все остальные веса – случайным образом.\"\n",
    "\n",
    "И с учетом [туториала](https://huggingface.co/docs/transformers/training) (Fine-tune a pretrained model in native PyTorch)\n",
    "- вообще изменения минимальные, все и так подошло"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RX_ZDhicpHkV"
   },
   "source": [
    "## Установка библиотек\n",
    "\n",
    "Понадобится `transformers`, `sentencepiece`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "0NmMdkZO8R6q",
    "outputId": "1cc59bfa-1dbb-4540-cb22-196f399f62af"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers sentencepiece\n",
    "\n",
    "QUICK_RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ok002ceNB8E7",
    "outputId": "06ef90d2-7518-4209-da66-1dd45c357c78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 12:16:35.520406: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64\n",
      "2022-04-21 12:16:35.520443: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Таблица параметров модели. Типа подробный `get_parameters()` \n",
    "- Все тензоры ноутбука\n",
    "- Очистить память от ненужных тензоров/моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.summary import count_parameters, dump_tensors, free_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oYsV4H8fCpZ-",
    "outputId": "b8812c8e-3149-475f-b4c0-262160485c39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 760\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == 'cpu':\n",
    "    print('cpu')\n",
    "else:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guw6ZNtaswKc"
   },
   "source": [
    "## Загрузка данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pos_texts = pd.read_csv('datasets/bert_sentiment_analysis/positive.csv', encoding='utf8', sep=';', header=None)\n",
    "neg_texts = pd.read_csv('datasets/bert_sentiment_analysis/negative.csv', encoding='utf8', sep=';', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81869</th>\n",
       "      <td>410760809166688256</td>\n",
       "      <td>1386767983</td>\n",
       "      <td>zmeevolk</td>\n",
       "      <td>@kakabadze сейчас сумму лимита стоимости 1000 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2696</td>\n",
       "      <td>60</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50977</th>\n",
       "      <td>410042527689895936</td>\n",
       "      <td>1386596731</td>\n",
       "      <td>Shusharina96</td>\n",
       "      <td>Как я давно не ходила по магазинам. Скоро буде...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3974</td>\n",
       "      <td>85</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19686</th>\n",
       "      <td>409406838992814080</td>\n",
       "      <td>1386445171</td>\n",
       "      <td>_Noisy_Killjoy_</td>\n",
       "      <td>\"гспд,он же вылитый Крейг :)\" Фото ridiculousg...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35140</td>\n",
       "      <td>1111</td>\n",
       "      <td>719</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83272</th>\n",
       "      <td>410773983877791744</td>\n",
       "      <td>1386771124</td>\n",
       "      <td>Tor83V</td>\n",
       "      <td>@fantomrocker нетт конечно ) уважение надо зас...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58308</td>\n",
       "      <td>16740</td>\n",
       "      <td>12294</td>\n",
       "      <td>1507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86216</th>\n",
       "      <td>410802675115388928</td>\n",
       "      <td>1386777964</td>\n",
       "      <td>PashaSnowbell</td>\n",
       "      <td>чувствую,что зачеты я буду сдавать так же) htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1022</td>\n",
       "      <td>43</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0           1                2   \\\n",
       "81869  410760809166688256  1386767983         zmeevolk   \n",
       "50977  410042527689895936  1386596731     Shusharina96   \n",
       "19686  409406838992814080  1386445171  _Noisy_Killjoy_   \n",
       "83272  410773983877791744  1386771124           Tor83V   \n",
       "86216  410802675115388928  1386777964    PashaSnowbell   \n",
       "\n",
       "                                                      3   4   5   6   7   \\\n",
       "81869  @kakabadze сейчас сумму лимита стоимости 1000 ...   1   0   0   0   \n",
       "50977  Как я давно не ходила по магазинам. Скоро буде...   1   0   0   0   \n",
       "19686  \"гспд,он же вылитый Крейг :)\" Фото ridiculousg...   1   0   1   0   \n",
       "83272  @fantomrocker нетт конечно ) уважение надо зас...   1   0   0   0   \n",
       "86216  чувствую,что зачеты я буду сдавать так же) htt...   1   0   0   0   \n",
       "\n",
       "          8      9      10    11  \n",
       "81869   2696     60     77     0  \n",
       "50977   3974     85     57     0  \n",
       "19686  35140   1111    719    13  \n",
       "83272  58308  16740  12294  1507  \n",
       "86216   1022     43     29     0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_texts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean     85.0\n",
       "std      27.0\n",
       "min      21.0\n",
       "max     179.0\n",
       "Name: 3, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_texts[3].apply(lambda x: len(x)).describe()[[\"mean\", \"std\", \"min\", \"max\"]].round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guw6ZNtaswKc"
   },
   "source": [
    "## Подготовка датасета\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = np.concatenate([pos_texts[3].values, neg_texts[3].values])\n",
    "labels = np.array([[1] for _ in range(pos_texts.shape[0])] + [[0] for _ in range(neg_texts.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sentences) == len(labels) == pos_texts.shape[0] + neg_texts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дим, ты помогаешь мне, я тебе, все взаимно, все правильно)\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "if QUICK_RUN:\n",
    "    idx = np.random.choice(np.arange(len(sentences)), size = N)\n",
    "    sentences = sentences[idx]\n",
    "    labels = labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренировочная и тестовая выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, test_sentences, train_gt, test_gt = train_test_split(list(sentences), labels, test_size=0.3)\n",
    "train_sentences, val_sentences, train_gt, val_gt = train_test_split(train_sentences, train_gt, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630 300\n"
     ]
    }
   ],
   "source": [
    "print(len(train_gt), len(test_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_sentences, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_sentences, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_sentences, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch принято оборачивать обучающие данные в класс. Он является наследником класса Dataset и переопределяет метод __len__ числа примеров и метод __getitem__ получения примера по индексу idx\n",
    "\n",
    "Создав датасет, его можно передать объекту DataLoader, который будет выдавать батчи, перемешивать, разбивать на обучение и тест, нормализовать данные и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuTwitterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RuTwitterDataset(train_encodings, train_gt)\n",
    "val_dataset = RuTwitterDataset(val_encodings, val_gt)\n",
    "test_dataset = RuTwitterDataset(test_encodings, test_gt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "eval_dataloader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"cointegrated/rubert-tiny\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(name=\"linear\", \n",
    "                             optimizer=optimizer, \n",
    "                             num_warmup_steps=0, \n",
    "                             num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+-----+-------------+------------+---------------+----------+\n",
      "|                    Modules/Tensors                     | GPU |    Shape    | Parameters |      Type     | DataMem  |\n",
      "+--------------------------------------------------------+-----+-------------+------------+---------------+----------+\n",
      "|         bert.embeddings.word_embeddings.weight         |  +  | 29564 x 312 |  9223968   | torch.float32 | 36895872 |\n",
      "|       bert.embeddings.position_embeddings.weight       |  +  |  512 x 312  |   159744   | torch.float32 |  638976  |\n",
      "|      bert.embeddings.token_type_embeddings.weight      |  +  |   2 x 312   |    624     | torch.float32 |   2496   |\n",
      "|            bert.embeddings.LayerNorm.weight            |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|             bert.embeddings.LayerNorm.bias             |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|    bert.encoder.layer.0.attention.self.query.weight    |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|     bert.encoder.layer.0.attention.self.query.bias     |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|     bert.encoder.layer.0.attention.self.key.weight     |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|      bert.encoder.layer.0.attention.self.key.bias      |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|    bert.encoder.layer.0.attention.self.value.weight    |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|     bert.encoder.layer.0.attention.self.value.bias     |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|   bert.encoder.layer.0.attention.output.dense.weight   |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|    bert.encoder.layer.0.attention.output.dense.bias    |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "| bert.encoder.layer.0.attention.output.LayerNorm.weight |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|  bert.encoder.layer.0.attention.output.LayerNorm.bias  |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|     bert.encoder.layer.0.intermediate.dense.weight     |  +  |  600 x 312  |   187200   | torch.float32 |  748800  |\n",
      "|      bert.encoder.layer.0.intermediate.dense.bias      |  +  |     600     |    600     | torch.float32 |   2400   |\n",
      "|        bert.encoder.layer.0.output.dense.weight        |  +  |  312 x 600  |   187200   | torch.float32 |  748800  |\n",
      "|         bert.encoder.layer.0.output.dense.bias         |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|      bert.encoder.layer.0.output.LayerNorm.weight      |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|       bert.encoder.layer.0.output.LayerNorm.bias       |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|    bert.encoder.layer.1.attention.self.query.weight    |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|     bert.encoder.layer.1.attention.self.query.bias     |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|     bert.encoder.layer.1.attention.self.key.weight     |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|      bert.encoder.layer.1.attention.self.key.bias      |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|    bert.encoder.layer.1.attention.self.value.weight    |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|     bert.encoder.layer.1.attention.self.value.bias     |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|   bert.encoder.layer.1.attention.output.dense.weight   |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|    bert.encoder.layer.1.attention.output.dense.bias    |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "| bert.encoder.layer.1.attention.output.LayerNorm.weight |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|  bert.encoder.layer.1.attention.output.LayerNorm.bias  |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|     bert.encoder.layer.1.intermediate.dense.weight     |  +  |  600 x 312  |   187200   | torch.float32 |  748800  |\n",
      "|      bert.encoder.layer.1.intermediate.dense.bias      |  +  |     600     |    600     | torch.float32 |   2400   |\n",
      "|        bert.encoder.layer.1.output.dense.weight        |  +  |  312 x 600  |   187200   | torch.float32 |  748800  |\n",
      "|         bert.encoder.layer.1.output.dense.bias         |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|      bert.encoder.layer.1.output.LayerNorm.weight      |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|       bert.encoder.layer.1.output.LayerNorm.bias       |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|    bert.encoder.layer.2.attention.self.query.weight    |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|     bert.encoder.layer.2.attention.self.query.bias     |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|     bert.encoder.layer.2.attention.self.key.weight     |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|      bert.encoder.layer.2.attention.self.key.bias      |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|    bert.encoder.layer.2.attention.self.value.weight    |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|     bert.encoder.layer.2.attention.self.value.bias     |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|   bert.encoder.layer.2.attention.output.dense.weight   |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|    bert.encoder.layer.2.attention.output.dense.bias    |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "| bert.encoder.layer.2.attention.output.LayerNorm.weight |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|  bert.encoder.layer.2.attention.output.LayerNorm.bias  |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|     bert.encoder.layer.2.intermediate.dense.weight     |  +  |  600 x 312  |   187200   | torch.float32 |  748800  |\n",
      "|      bert.encoder.layer.2.intermediate.dense.bias      |  +  |     600     |    600     | torch.float32 |   2400   |\n",
      "|        bert.encoder.layer.2.output.dense.weight        |  +  |  312 x 600  |   187200   | torch.float32 |  748800  |\n",
      "|         bert.encoder.layer.2.output.dense.bias         |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|      bert.encoder.layer.2.output.LayerNorm.weight      |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|       bert.encoder.layer.2.output.LayerNorm.bias       |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|                bert.pooler.dense.weight                |  +  |  312 x 312  |   97344    | torch.float32 |  389376  |\n",
      "|                 bert.pooler.dense.bias                 |  +  |     312     |    312     | torch.float32 |   1248   |\n",
      "|                   classifier.weight                    |  +  |   2 x 312   |    624     | torch.float32 |   2496   |\n",
      "|                    classifier.bias                     |  +  |      2      |     2      | torch.float32 |    8     |\n",
      "+--------------------------------------------------------+-----+-------------+------------+---------------+----------+\n",
      "Total Trainable Params: 11784794\n",
      "Total memory (min): 46,034.35 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11784794"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNl8khAhPYju"
   },
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы статьи про BERT советуют выбирать: \n",
    "- learning rate из следующего списка: $5 \\cdot 10^{-5}, 3 \\cdot 10^{-5}, 2 \\cdot 10^{-5}$, \n",
    "- количество эпох дообучения: 2, 3, 4.\n",
    "- размер батча: 16, 32\n",
    "\n",
    "### Оптимизатор с \"регуляризацией\"\n",
    "\n",
    "`AdamW` - это реализация `Adam` с применением **Weight Decay**. \n",
    "\n",
    "Что такое Weight Decay? При каждом обновлении веса давайте кроме движения в сторону антиградиента еще и будем вычитать маленький кусочек веса, умноженный на некоторый гиперпараметр. Например, формула стохастического градиентного спуска с применением Weight Decay будет выглядеть так:\n",
    "\n",
    "$w = w - lr * \\dfrac{\\partial L}{\\partial w} - lr * wd * w$ \n",
    "\n",
    "Здесь lr - learning rate, wd - weight decay, гиперпараметр.\n",
    "\n",
    "Если Вы внимательно посмотрите на эту формулу, то заметите, что в случае стохастического градиентного спуска Weight Decay эквивалентен применению L2-регуляризации к loss-функции:\n",
    "\n",
    "$$L_{new} = L + \\dfrac{wd}{2} ||w||^2 \\\\\n",
    " \n",
    "\\dfrac{\\partial (L_{new})}{\\partial w} = \\dfrac{\\partial L}{\\partial w} + wd \\cdot w \\\\\n",
    "\n",
    "w = w - lr \\cdot \\dfrac{\\partial L_{new}}{\\partial w} = w - lr * \\dfrac{\\partial L}{\\partial w} - lr * wd * w $$\n",
    "\n",
    "Однако, L2-регуляризация (**меняем loss-функцию**) и Weight Decay (**не меняем loss-функцию**, меняем только способ обновления весов) работают одинаково только в простом случае стохастического градиентного спуска, в случае адаптивных оптимизаторов, например, Адама, эти два подхода различаются (причем эмпирически было показано, что часто Weight Decay работает лучше). Вся эта история породила некоторую путаницу в терминилогии, и во многих библиотеках Адам реализовали именно с применением L2-регуляризации, ошибочно называя такой подход Weight Decay.  Авторы статьи [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) решили разграничить Adam+L2 и Adam+Weight Decay, назвав последнее AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxSMw0FrptiL"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:11<00:00,  3.65it/s]"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "loss_set = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_set.append(loss.detach().cpu())  \n",
    "        # Рисуем график\n",
    "        clear_output(True)\n",
    "        plt.plot(loss_set)\n",
    "        plt.title(\"Training loss\")\n",
    "        plt.xlabel(\"Batch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "        # break\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цикл обучения\n",
    "\n",
    "На самом деле это 1 итерация цикла (1 эпоха), если надо, можно еще раз тыкнуть ичейку.\n",
    "\n",
    "- переводим модель в train mode, \n",
    "- распаковываем входные данные (вектор с индексами токенов и метки классов)\n",
    "- помещаем наши данные на GPU с помощью \n",
    "- не забываем очистить градиенты с прошлого шага с помощью `zero_grad()` \n",
    "- делаем forward pass, считаем loss, затем делаем backward pass, считаем градиенты, \n",
    "- дальше — стандартно — делаем optimizer.step(). \n",
    " \n",
    "Кроме того, в процессе обучения мы будем рисовать график и считать лосс. В конце каждой эпохи (в нашем случае — в конце единственной эпохи) посчитаем лосс на нашей обучающей выборке.\n",
    "\n",
    "- нет никакой необходимости проходить по всему тренировочному датасету (делать всю эпоху)\n",
    "  - где-то после третери лосс не отлипает от 0\n",
    "  - точность на валидации при этом получилась 99.96%\n",
    "  - на GPU на старой видеокарте: ~5 мин."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkyubuJSOzg3"
   },
   "source": [
    "# Оценка качества на отложенной выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По идее должно быть\n",
    "\n",
    "    from datasets import load_metric\n",
    "    metric = load_metric(\"accuracy\")\n",
    "\n",
    "Но такого чет нету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAN0LZBOOPVh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент правильных предсказаний на отложенной выборке: 90.33%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "valid_preds, valid_labels = [], []\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    valid_preds.extend(predictions.detach().cpu().tolist())\n",
    "    valid_labels.extend(batch[\"labels\"].detach().cpu().tolist())\n",
    "\n",
    "\n",
    "print(\"Процент правильных предсказаний на отложенной выборке: {0:.2f}%\".format(\n",
    "    accuracy_score(valid_labels, valid_preds) * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Неправильных предсказаний: 29/300\n"
     ]
    }
   ],
   "source": [
    "print('Неправильных предсказаний: {0}/{1}'.format(\n",
    "    sum([z[0][0] != z[1] for z in zip(valid_labels,  valid_preds)]),    # (!)\n",
    "    len(valid_labels)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если пройти всю эпоху, то точность 99,98% (11 ошибок на 68051).\n",
    "- на самом деле это с такой точностью сеть уловила алгоритм, которым делалась авторазметка текста по смайликам. \n",
    "- там была некоторая функция, она без всякого шума, отсюда такая точность \n",
    "- поэтому вот пожалуйста, копия этой \"неизвестной\" функции в виде нейросети"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Fine-Tuning Sentence Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
