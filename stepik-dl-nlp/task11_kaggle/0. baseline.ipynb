{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация заголовков научных статей: слабый baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Источник: https://github.com/bentrevett/pytorch-seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab,\n",
    "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
    "\n",
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git\n",
    "# import sys; sys.path.append('/content/stepik-dl-nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from helpers.utils import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возможно, Вам потребуется предварительно загрузить модели SpaCy для английского языка\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text) if not tok.text.isspace()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/envs/py39/lib/python3.9/site-packages/torchtext-0.11.0a0-py3.9-linux-x86_64.egg/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy import data, vocab\n",
    "\n",
    "tokenizer = data.get_tokenizer('spacy')\n",
    "\n",
    "TEXT = Field(tokenize=tokenize,\n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            include_lengths = True,\n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 2.78 s, total: 1min 22s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trn_data_fields = [(\"src\", TEXT),\n",
    "                   (\"trg\", TEXT)]\n",
    "\n",
    "dataset = data.TabularDataset(\n",
    "    path='datasets/train.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=trn_data_fields\n",
    ")\n",
    "\n",
    "train_data, valid_data, test_data = dataset.split(split_ratio=[0.98, 0.01, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in vocabulary: 32466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 0, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.build_vocab(train_data, min_freq = 10)\n",
    "print(f\"Unique tokens in vocabulary: {len(TEXT.vocab)}\")\n",
    "\n",
    "TEXT.vocab.stoi[\" \"], TEXT.vocab.stoi[\"  \"], TEXT.vocab.stoi[\"   \"], TEXT.vocab.stoi[\"    \"], TEXT.vocab.stoi[\"     \"]   # ??? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['building', 'classifiers', 'to', 'predict', 'the', 'start', 'of', 'glucose', '-', 'lowering', 'pharmacotherapy', 'using', 'belgian', 'health', 'expenditure', 'data']\n",
      "building classifiers to predict the start of glucose-lowering   pharmacotherapy using belgian health expenditure data\n",
      "[('TOKEN', 'building'), ('TOKEN', 'classifiers'), ('TOKEN', 'to'), ('TOKEN', 'predict'), ('TOKEN', 'the'), ('TOKEN', 'start'), ('TOKEN', 'of'), ('TOKEN', 'glucose'), ('INFIX', '-'), ('TOKEN', 'lowering'), ('TOKEN', 'pharmacotherapy'), ('TOKEN', 'using'), ('TOKEN', 'belgian'), ('TOKEN', 'health'), ('TOKEN', 'expenditure'), ('TOKEN', 'data')]\n",
      "['building', 'classifiers', 'to', 'predict', 'the', 'start', 'of', 'glucose', '-', 'lowering', '  ', 'pharmacotherapy', 'using', 'belgian', 'health', 'expenditure', 'data']\n",
      "[building, classifiers, to, predict, the, start, of, glucose, -, lowering,   , pharmacotherapy, using, belgian, health, expenditure, data]\n"
     ]
    }
   ],
   "source": [
    "s11 = \"building classifiers to predict the start of glucose-lowering   pharmacotherapy using belgian health expenditure data\"\n",
    "for t in [tokenize(s11), spacy_en.tokenizer(s11), spacy_en.tokenizer.explain(s11), [tok.text for tok in spacy_en.tokenizer(s11)], [tok for tok in spacy_en.tokenizer(s11)]]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size = BATCH_SIZE,\n",
    "     sort_within_batch = True,\n",
    "     sort_key = lambda x : len(x.src),\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #src_len = [src sent len]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "                \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
    "                \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "                                 \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "            \n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [sent len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src sent len]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat encoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src sent len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src sent len, dec hid dim]\n",
    "                \n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        #energy = [batch size, dec hid dim, src sent len]\n",
    "        \n",
    "        #v = [dec hid dim]\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, dec hid dim]\n",
    "            \n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #attention = [batch size, src sent len]\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src sent len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "                \n",
    "        #a = [batch size, src sent len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src sent len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [sent len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #output = [bsz, output dim]\n",
    "        \n",
    "        return output, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx, sos_idx, eos_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        if trg is None:\n",
    "            assert teacher_forcing_ratio == 0, \"Must be zero during inference\"\n",
    "            inference = True\n",
    "            trg = torch.zeros((100, src.shape[1])).long().fill_(self.sos_idx).to(src.device)\n",
    "        else:\n",
    "            inference = False\n",
    "            \n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #tensor to store attention\n",
    "        attentions = torch.zeros(max_len, batch_size, src.shape[0]).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "                \n",
    "        #mask = [batch size, src sent len]\n",
    "                \n",
    "        for t in range(1, max_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
    "            # and mask\n",
    "            #receive output tensor (predictions), new hidden state and attention tensor\n",
    "            output, hidden, attention = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #place attentions in a tensor holding attention value for each input token\n",
    "            attentions[t] = attention\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "            #if doing inference and next token/prediction is an eos token then stop\n",
    "            if inference and input.item() == self.eos_idx:\n",
    "                return outputs[:t], attentions[:t]\n",
    "            \n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+-------------+------------+---------------+----------+\n",
      "|         Modules/Tensors          | GPU |    Shape    | Parameters |      Type     | DataMem  |\n",
      "+----------------------------------+-----+-------------+------------+---------------+----------+\n",
      "|     encoder.embedding.weight     |  +  | 40353 x 128 |  5165184   | torch.float32 | 20660736 |\n",
      "|     encoder.rnn.weight_ih_l0     |  +  |  192 x 128  |   24576    | torch.float32 |  98304   |\n",
      "|     encoder.rnn.weight_hh_l0     |  +  |   192 x 64  |   12288    | torch.float32 |  49152   |\n",
      "|      encoder.rnn.bias_ih_l0      |  +  |     192     |    192     | torch.float32 |   768    |\n",
      "|      encoder.rnn.bias_hh_l0      |  +  |     192     |    192     | torch.float32 |   768    |\n",
      "| encoder.rnn.weight_ih_l0_reverse |  +  |  192 x 128  |   24576    | torch.float32 |  98304   |\n",
      "| encoder.rnn.weight_hh_l0_reverse |  +  |   192 x 64  |   12288    | torch.float32 |  49152   |\n",
      "|  encoder.rnn.bias_ih_l0_reverse  |  +  |     192     |    192     | torch.float32 |   768    |\n",
      "|  encoder.rnn.bias_hh_l0_reverse  |  +  |     192     |    192     | torch.float32 |   768    |\n",
      "|        encoder.fc.weight         |  +  |   64 x 128  |    8192    | torch.float32 |  32768   |\n",
      "|         encoder.fc.bias          |  +  |      64     |     64     | torch.float32 |   256    |\n",
      "|       decoder.attention.v        |  +  |      64     |     64     | torch.float32 |   256    |\n",
      "|  decoder.attention.attn.weight   |  +  |   64 x 192  |   12288    | torch.float32 |  49152   |\n",
      "|   decoder.attention.attn.bias    |  +  |      64     |     64     | torch.float32 |   256    |\n",
      "|     decoder.embedding.weight     |  +  | 40353 x 128 |  5165184   | torch.float32 | 20660736 |\n",
      "|     decoder.rnn.weight_ih_l0     |  +  |  192 x 256  |   49152    | torch.float32 |  196608  |\n",
      "|     decoder.rnn.weight_hh_l0     |  +  |   192 x 64  |   12288    | torch.float32 |  49152   |\n",
      "|      decoder.rnn.bias_ih_l0      |  +  |     192     |    192     | torch.float32 |   768    |\n",
      "|      decoder.rnn.bias_hh_l0      |  +  |     192     |    192     | torch.float32 |   768    |\n",
      "|        decoder.out.weight        |  +  | 40353 x 320 |  12912960  | torch.float32 | 51651840 |\n",
      "|         decoder.out.bias         |  +  |    40353    |   40353    | torch.float32 |  161412  |\n",
      "+----------------------------------+-----+-------------+------------+---------------+----------+\n",
      "Total Trainable Params: 23440673\n",
      "Total memory (min): 91,565.13 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23440673"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "OUTPUT_DIM = len(TEXT.vocab)\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ENC_DROPOUT = 0.8\n",
    "DEC_DROPOUT = 0.8\n",
    "PAD_IDX = TEXT.vocab.stoi['<pad>']\n",
    "SOS_IDX = TEXT.vocab.stoi['<sos>']\n",
    "EOS_IDX = TEXT.vocab.stoi['<eos>']\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, PAD_IDX, SOS_IDX, EOS_IDX, device).to(device)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(40353, 128)\n",
       "    (rnn): GRU(128, 64, bidirectional=True)\n",
       "    (fc): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (dropout): Dropout(p=0.8, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=192, out_features=64, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(40353, 128)\n",
       "    (rnn): GRU(256, 64)\n",
       "    (out): Linear(in_features=320, out_features=40353, bias=True)\n",
       "    (dropout): Dropout(p=0.8, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 23,440,673 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def cnt_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {cnt_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    history = []\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src, src_len = batch.src\n",
    "        trg, trg_len = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, attetion = model(src, src_len, trg)\n",
    "        \n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        history.append(loss.cpu().data.numpy())\n",
    "        if (i+1)%10==0:\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n",
    "\n",
    "            clear_output(True)\n",
    "            ax[0].plot(history, label='train loss')\n",
    "            ax[0].set_xlabel('Batch')\n",
    "            ax[0].set_title('Train loss')\n",
    "            if train_history is not None:\n",
    "                ax[1].plot(train_history, label='general train history')\n",
    "                ax[1].set_xlabel('Epoch')\n",
    "            if valid_history is not None:\n",
    "                ax[1].plot(valid_history, label='general valid history')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.show()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src, src_len = batch.src\n",
    "            trg, trg_len = batch.trg\n",
    "\n",
    "            output, attention = model(src, src_len, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg sent len, batch size]\n",
    "            #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg sent len - 1) * batch size]\n",
    "            #output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'models/my_baseline.pt'\n",
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "\n",
    "train_history = []\n",
    "valid_history = []\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, train_history, valid_history)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_NAME)\n",
    "        \n",
    "        \n",
    "    train_history.append(train_loss)\n",
    "    valid_history.append(valid_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load the parameters from our best validation loss and get our results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cpu usage\n",
    "model.load_state_dict(torch.load(MODEL_NAME, map_location=torch.device('cpu')))\n",
    "\n",
    "# for gpu usage\n",
    "# model.load_state_dict(torch.load(MODEL_NAME), map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация заголовков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, tokenized_sentence):\n",
    "    model.eval()\n",
    "    tokenized_sentence = ['<sos>'] + [t.lower() for t in tokenized_sentence] + ['<eos>']\n",
    "    numericalized = [TEXT.vocab.stoi[t] for t in tokenized_sentence] \n",
    "    sentence_length = torch.LongTensor([len(numericalized)]).to(device) \n",
    "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device) \n",
    "    translation_tensor_logits, attention = model(tensor, sentence_length, None, 0) \n",
    "    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n",
    "    translation = [TEXT.vocab.itos[t] for t in translation_tensor]\n",
    "    translation, attention = translation[1:], attention[1:]\n",
    "    return translation, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(30,50))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy().T\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=12)\n",
    "    ax.set_yticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'])\n",
    "    ax.set_xticklabels(['']+translation, rotation=80)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 100\n",
    "\n",
    "src = vars(train_data.examples[example_idx])['src']\n",
    "trg = vars(train_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, attention = translate_sentence(model, src)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_idx in range(100):\n",
    "    src = vars(test_data.examples[example_idx])['src']\n",
    "    trg = vars(test_data.examples[example_idx])['trg']\n",
    "    translation, attention = translate_sentence(model, src)\n",
    "\n",
    "    print('Оригинальный заголовок: ', ' '.join(trg))\n",
    "    print('Предсказанный заголовок: ', ' '.join(translation))\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 0\n",
    "\n",
    "src = vars(valid_data.examples[example_idx])['src']\n",
    "trg = vars(valid_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, attention = translate_sentence(model, src)\n",
    "\n",
    "print(f'predicted trg = {translation}')\n",
    "\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 510\n",
    "\n",
    "src = vars(test_data.examples[example_idx])['src']\n",
    "trg = vars(test_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, attention = translate_sentence(model, src)\n",
    "\n",
    "print(f'predicted trg = {translation}')\n",
    "\n",
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Считаем BLEU на train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "n_gram_weights = [0.3334, 0.3333, 0.3333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = []\n",
    "generated_texts = []\n",
    "macro_bleu = 0\n",
    "\n",
    "for example_idx in range(test_len):\n",
    "    src = vars(test_data.examples[example_idx])['src']\n",
    "    trg = vars(test_data.examples[example_idx])['trg']\n",
    "    translation, _ = translate_sentence(model, src)\n",
    "\n",
    "    original_texts.append(trg)\n",
    "    generated_texts.append(translation)\n",
    "\n",
    "    bleu_score = nltk.translate.bleu_score.sentence_bleu(\n",
    "        [trg],\n",
    "        translation,\n",
    "        weights = n_gram_weights\n",
    "    )    \n",
    "    macro_bleu += bleu_score\n",
    "\n",
    "macro_bleu /= test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaging sentence-level BLEU (i.e. macro-average precision)\n",
    "print('Macro-average BLEU (LSTM): {0:.5f}'.format(macro_bleu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем submission в Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission_data = pd.read_csv('datasets/test.csv')\n",
    "abstracts = submission_data['abstract'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация заголовков для тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "for abstract in abstracts:\n",
    "    title, _ = translate_sentence(model, abstract.split())\n",
    "    titles.append(' '.join(title).replace('<unk>', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записываем полученные заголовки в файл формата `<abstract>,<title>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'abstract': abstracts, 'title': titles})\n",
    "submission_df.to_csv('datasets/predicted_titles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью скрипта `generate_csv` приводим файл `submission_prediction.csv` в формат, необходимый для посылки в соревнование на Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.create_submission import generate_csv\n",
    "\n",
    "generate_csv('datasets/predicted_titles.csv', 'datasets/kaggle_pred.csv', 'datasets/vocs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l datasets/kaggle_pred.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head datasets/kaggle_pred.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
