{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- из данных убраны дубли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = f\"./datasets/train_clean.csv\"\n",
    "SMALL_CSV = f\"./cache/train.csv\"\n",
    "SCORING_CSV = f\"./datasets/test.csv\"\n",
    "\n",
    "USE_SMALL = True\n",
    "\n",
    "max_title = 36          # max 103, 3 сигмы 34 + 2\n",
    "max_abstract = 460      # max 1096, 3 сигмы 457 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-04a3546adcf83ebb\n",
      "Reusing dataset csv (/home/user1/.cache/huggingface/datasets/csv/default-04a3546adcf83ebb/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "arxiv_dataset = datasets.Dataset.from_csv(SMALL_CSV if USE_SMALL else TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2 if USE_SMALL else 0.02\n",
    "arxiv_dataset = arxiv_dataset.train_test_split(test_size=test_size)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200, dict_keys(['abstract', 'title']))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arxiv_dataset[\"train\"]), len(arxiv_dataset[\"test\"]), arxiv_dataset[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2e0a9ad90b647d2d\n",
      "Reusing dataset csv (/home/user1/.cache/huggingface/datasets/csv/default-2e0a9ad90b647d2d/0.0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, dict_keys(['abstract']))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_dataset = datasets.Dataset.from_csv(SCORING_CSV)\n",
    "len(scoring_dataset), scoring_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 383/383 [00:00<00:00, 265kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 456kB/s]  \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# google/bert_uncased_L-8_H-512_A-8     # medium\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-4_H-512_A-8\")   # small\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-8_H-512_A-8\")   # medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2070, 2146, 2146, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"some long long  long  long  long  long text\", max_length=5, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\" Длины последовательностей max = 1096 / 103. Берем какбе 3 сигмы:\n",
    "        - max_length=457 + 4\n",
    "        - max_length=34 + 2\n",
    "    \"\"\"\n",
    "\n",
    "    srcs = [prefix + doc for doc in examples[\"abstract\"]]\n",
    "    model_inputs = tokenizer(srcs, max_length=max_abstract, truncation=True) # max_length includes special tokens\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        trgs = tokenizer(examples[\"title\"], max_length=max_title, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = trgs[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.64ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.87ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_arxiv = arxiv_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['abstract', 'title', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']),\n",
       " 'asynchrony, overlaps and delays in sensory-motor signals introduce ambiguity as to which stimuli, actions, and rewards are causally related. only the repetition of reward episodes helps distinguish true cause-effect relationships from coincidental occurrences. in the model proposed here, a novel plasticity rule employs short and long-term changes to evaluate hypotheses on cause-effect relationships. transient weights represent hypotheses that are consolidated in long-term memory only when they consistently predict or cause future rewards. the main objective of the model is to preserve existing network topologies when learning with ambiguous information flows. learning is also improved by biasing the exploration of the stimulus-response space towards actions that in the past occurred before rewards. the model indicates under which conditions beliefs can be consolidated in long-term memory, it suggests a solution to the plasticity-stability dilemma, and proposes an interpretation of the role of short-term plasticity.',\n",
       " 'short-term plasticity as cause-effect hypothesis testing in distal   reward learning')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_arxiv[\"train\"][0].keys(), tokenized_arxiv[\"train\"][0][\"abstract\"], tokenized_arxiv[\"train\"][0][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 159M/159M [00:20<00:00, 8.08MB/s] \n",
      "Some weights of the model checkpoint at google/bert_uncased_L-8_H-512_A-8 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-8_H-512_A-8 were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at google/bert_uncased_L-8_H-512_A-8 and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoderModel(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 512)\n",
       "        (token_type_embeddings): Embedding(2, 512)\n",
       "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=512, out_features=30522, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "# google bert-small https://huggingface.co/google/bert_uncased_L-4_H-512_A-8\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('google/bert_uncased_L-8_H-512_A-8', 'google/bert_uncased_L-8_H-512_A-8') # medium\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обучении тагрет в модель подается кроме последнего токена, при валидации/генрации - кроме первого.\n",
    "\n",
    "Поэтому для генерации сид-фраза должна начинаться с того токена, на котором модель обучалась, в данном случае это первый токен берта `[CLS]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+-----+-------------+------------+---------------+----------+\n",
      "|                           Modules/Tensors                           | GPU |    Shape    | Parameters |      Type     | DataMem  |\n",
      "+---------------------------------------------------------------------+-----+-------------+------------+---------------+----------+\n",
      "|              encoder.embeddings.word_embeddings.weight              |  +  | 30522 x 512 |  15627264  | torch.float32 | 62509056 |\n",
      "|            encoder.embeddings.position_embeddings.weight            |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|           encoder.embeddings.token_type_embeddings.weight           |  +  |   2 x 512   |    1024    | torch.float32 |   4096   |\n",
      "|                 encoder.embeddings.LayerNorm.weight                 |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|                  encoder.embeddings.LayerNorm.bias                  |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.0.attention.self.query.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.0.attention.self.query.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.0.attention.self.key.weight          |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|           encoder.encoder.layer.0.attention.self.key.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.0.attention.self.value.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.0.attention.self.value.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        encoder.encoder.layer.0.attention.output.dense.weight        |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         encoder.encoder.layer.0.attention.output.dense.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      encoder.encoder.layer.0.attention.output.LayerNorm.weight      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       encoder.encoder.layer.0.attention.output.LayerNorm.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.0.intermediate.dense.weight          |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|           encoder.encoder.layer.0.intermediate.dense.bias           |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|             encoder.encoder.layer.0.output.dense.weight             |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|              encoder.encoder.layer.0.output.dense.bias              |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|           encoder.encoder.layer.0.output.LayerNorm.weight           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|            encoder.encoder.layer.0.output.LayerNorm.bias            |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.1.attention.self.query.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.1.attention.self.query.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.1.attention.self.key.weight          |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|           encoder.encoder.layer.1.attention.self.key.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.1.attention.self.value.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.1.attention.self.value.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        encoder.encoder.layer.1.attention.output.dense.weight        |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         encoder.encoder.layer.1.attention.output.dense.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      encoder.encoder.layer.1.attention.output.LayerNorm.weight      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       encoder.encoder.layer.1.attention.output.LayerNorm.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.1.intermediate.dense.weight          |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|           encoder.encoder.layer.1.intermediate.dense.bias           |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|             encoder.encoder.layer.1.output.dense.weight             |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|              encoder.encoder.layer.1.output.dense.bias              |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|           encoder.encoder.layer.1.output.LayerNorm.weight           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|            encoder.encoder.layer.1.output.LayerNorm.bias            |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.2.attention.self.query.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.2.attention.self.query.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.2.attention.self.key.weight          |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|           encoder.encoder.layer.2.attention.self.key.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.2.attention.self.value.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.2.attention.self.value.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        encoder.encoder.layer.2.attention.output.dense.weight        |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         encoder.encoder.layer.2.attention.output.dense.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      encoder.encoder.layer.2.attention.output.LayerNorm.weight      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       encoder.encoder.layer.2.attention.output.LayerNorm.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.2.intermediate.dense.weight          |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|           encoder.encoder.layer.2.intermediate.dense.bias           |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|             encoder.encoder.layer.2.output.dense.weight             |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|              encoder.encoder.layer.2.output.dense.bias              |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|           encoder.encoder.layer.2.output.LayerNorm.weight           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|            encoder.encoder.layer.2.output.LayerNorm.bias            |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.3.attention.self.query.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.3.attention.self.query.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.3.attention.self.key.weight          |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|           encoder.encoder.layer.3.attention.self.key.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.3.attention.self.value.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.3.attention.self.value.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        encoder.encoder.layer.3.attention.output.dense.weight        |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         encoder.encoder.layer.3.attention.output.dense.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      encoder.encoder.layer.3.attention.output.LayerNorm.weight      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       encoder.encoder.layer.3.attention.output.LayerNorm.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.3.intermediate.dense.weight          |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|           encoder.encoder.layer.3.intermediate.dense.bias           |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|             encoder.encoder.layer.3.output.dense.weight             |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|              encoder.encoder.layer.3.output.dense.bias              |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|           encoder.encoder.layer.3.output.LayerNorm.weight           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|            encoder.encoder.layer.3.output.LayerNorm.bias            |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.4.attention.self.query.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.4.attention.self.query.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.4.attention.self.key.weight          |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|           encoder.encoder.layer.4.attention.self.key.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.4.attention.self.value.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.4.attention.self.value.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        encoder.encoder.layer.4.attention.output.dense.weight        |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         encoder.encoder.layer.4.attention.output.dense.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      encoder.encoder.layer.4.attention.output.LayerNorm.weight      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       encoder.encoder.layer.4.attention.output.LayerNorm.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.4.intermediate.dense.weight          |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|           encoder.encoder.layer.4.intermediate.dense.bias           |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|             encoder.encoder.layer.4.output.dense.weight             |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|              encoder.encoder.layer.4.output.dense.bias              |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|           encoder.encoder.layer.4.output.LayerNorm.weight           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|            encoder.encoder.layer.4.output.LayerNorm.bias            |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.5.attention.self.query.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.5.attention.self.query.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.5.attention.self.key.weight          |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|           encoder.encoder.layer.5.attention.self.key.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.5.attention.self.value.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.5.attention.self.value.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        encoder.encoder.layer.5.attention.output.dense.weight        |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         encoder.encoder.layer.5.attention.output.dense.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      encoder.encoder.layer.5.attention.output.LayerNorm.weight      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       encoder.encoder.layer.5.attention.output.LayerNorm.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.5.intermediate.dense.weight          |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|           encoder.encoder.layer.5.intermediate.dense.bias           |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|             encoder.encoder.layer.5.output.dense.weight             |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|              encoder.encoder.layer.5.output.dense.bias              |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|           encoder.encoder.layer.5.output.LayerNorm.weight           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|            encoder.encoder.layer.5.output.LayerNorm.bias            |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.6.attention.self.query.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.6.attention.self.query.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.6.attention.self.key.weight          |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|           encoder.encoder.layer.6.attention.self.key.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.6.attention.self.value.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.6.attention.self.value.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        encoder.encoder.layer.6.attention.output.dense.weight        |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         encoder.encoder.layer.6.attention.output.dense.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      encoder.encoder.layer.6.attention.output.LayerNorm.weight      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       encoder.encoder.layer.6.attention.output.LayerNorm.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.6.intermediate.dense.weight          |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|           encoder.encoder.layer.6.intermediate.dense.bias           |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|             encoder.encoder.layer.6.output.dense.weight             |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|              encoder.encoder.layer.6.output.dense.bias              |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|           encoder.encoder.layer.6.output.LayerNorm.weight           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|            encoder.encoder.layer.6.output.LayerNorm.bias            |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.7.attention.self.query.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.7.attention.self.query.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.7.attention.self.key.weight          |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|           encoder.encoder.layer.7.attention.self.key.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         encoder.encoder.layer.7.attention.self.value.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|          encoder.encoder.layer.7.attention.self.value.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        encoder.encoder.layer.7.attention.output.dense.weight        |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         encoder.encoder.layer.7.attention.output.dense.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      encoder.encoder.layer.7.attention.output.LayerNorm.weight      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       encoder.encoder.layer.7.attention.output.LayerNorm.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          encoder.encoder.layer.7.intermediate.dense.weight          |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|           encoder.encoder.layer.7.intermediate.dense.bias           |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|             encoder.encoder.layer.7.output.dense.weight             |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|              encoder.encoder.layer.7.output.dense.bias              |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|           encoder.encoder.layer.7.output.LayerNorm.weight           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|            encoder.encoder.layer.7.output.LayerNorm.bias            |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|                     encoder.pooler.dense.weight                     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|                      encoder.pooler.dense.bias                      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|            decoder.bert.embeddings.word_embeddings.weight           |  +  | 30522 x 512 |  15627264  | torch.float32 | 62509056 |\n",
      "|          decoder.bert.embeddings.position_embeddings.weight         |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         decoder.bert.embeddings.token_type_embeddings.weight        |  +  |   2 x 512   |    1024    | torch.float32 |   4096   |\n",
      "|               decoder.bert.embeddings.LayerNorm.weight              |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|                decoder.bert.embeddings.LayerNorm.bias               |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.0.attention.self.query.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.0.attention.self.query.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.0.attention.self.key.weight       |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         decoder.bert.encoder.layer.0.attention.self.key.bias        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.0.attention.self.value.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.0.attention.self.value.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      decoder.bert.encoder.layer.0.attention.output.dense.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|       decoder.bert.encoder.layer.0.attention.output.dense.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight   |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.0.crossattention.self.query.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.0.crossattention.self.query.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.0.crossattention.self.key.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|      decoder.bert.encoder.layer.0.crossattention.self.key.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.0.crossattention.self.value.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.0.crossattention.self.value.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|   decoder.bert.encoder.layer.0.crossattention.output.dense.weight   |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|    decoder.bert.encoder.layer.0.crossattention.output.dense.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "| decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|  decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias  |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.0.intermediate.dense.weight       |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|         decoder.bert.encoder.layer.0.intermediate.dense.bias        |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|           decoder.bert.encoder.layer.0.output.dense.weight          |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|            decoder.bert.encoder.layer.0.output.dense.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         decoder.bert.encoder.layer.0.output.LayerNorm.weight        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          decoder.bert.encoder.layer.0.output.LayerNorm.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.1.attention.self.query.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.1.attention.self.query.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.1.attention.self.key.weight       |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         decoder.bert.encoder.layer.1.attention.self.key.bias        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.1.attention.self.value.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.1.attention.self.value.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      decoder.bert.encoder.layer.1.attention.output.dense.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|       decoder.bert.encoder.layer.1.attention.output.dense.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight   |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.1.crossattention.self.query.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.1.crossattention.self.query.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.1.crossattention.self.key.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|      decoder.bert.encoder.layer.1.crossattention.self.key.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.1.crossattention.self.value.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.1.crossattention.self.value.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|   decoder.bert.encoder.layer.1.crossattention.output.dense.weight   |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|    decoder.bert.encoder.layer.1.crossattention.output.dense.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "| decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|  decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias  |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.1.intermediate.dense.weight       |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|         decoder.bert.encoder.layer.1.intermediate.dense.bias        |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|           decoder.bert.encoder.layer.1.output.dense.weight          |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|            decoder.bert.encoder.layer.1.output.dense.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         decoder.bert.encoder.layer.1.output.LayerNorm.weight        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          decoder.bert.encoder.layer.1.output.LayerNorm.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.2.attention.self.query.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.2.attention.self.query.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.2.attention.self.key.weight       |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         decoder.bert.encoder.layer.2.attention.self.key.bias        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.2.attention.self.value.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.2.attention.self.value.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      decoder.bert.encoder.layer.2.attention.output.dense.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|       decoder.bert.encoder.layer.2.attention.output.dense.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight   |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.2.crossattention.self.query.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.2.crossattention.self.query.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.2.crossattention.self.key.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|      decoder.bert.encoder.layer.2.crossattention.self.key.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.2.crossattention.self.value.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.2.crossattention.self.value.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|   decoder.bert.encoder.layer.2.crossattention.output.dense.weight   |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|    decoder.bert.encoder.layer.2.crossattention.output.dense.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "| decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|  decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias  |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.2.intermediate.dense.weight       |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|         decoder.bert.encoder.layer.2.intermediate.dense.bias        |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|           decoder.bert.encoder.layer.2.output.dense.weight          |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|            decoder.bert.encoder.layer.2.output.dense.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         decoder.bert.encoder.layer.2.output.LayerNorm.weight        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          decoder.bert.encoder.layer.2.output.LayerNorm.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.3.attention.self.query.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.3.attention.self.query.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.3.attention.self.key.weight       |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         decoder.bert.encoder.layer.3.attention.self.key.bias        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.3.attention.self.value.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.3.attention.self.value.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      decoder.bert.encoder.layer.3.attention.output.dense.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|       decoder.bert.encoder.layer.3.attention.output.dense.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight   |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.3.crossattention.self.query.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.3.crossattention.self.query.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.3.crossattention.self.key.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|      decoder.bert.encoder.layer.3.crossattention.self.key.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.3.crossattention.self.value.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.3.crossattention.self.value.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|   decoder.bert.encoder.layer.3.crossattention.output.dense.weight   |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|    decoder.bert.encoder.layer.3.crossattention.output.dense.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "| decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|  decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias  |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.3.intermediate.dense.weight       |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|         decoder.bert.encoder.layer.3.intermediate.dense.bias        |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|           decoder.bert.encoder.layer.3.output.dense.weight          |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|            decoder.bert.encoder.layer.3.output.dense.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         decoder.bert.encoder.layer.3.output.LayerNorm.weight        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          decoder.bert.encoder.layer.3.output.LayerNorm.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.4.attention.self.query.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.4.attention.self.query.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.4.attention.self.key.weight       |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         decoder.bert.encoder.layer.4.attention.self.key.bias        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.4.attention.self.value.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.4.attention.self.value.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      decoder.bert.encoder.layer.4.attention.output.dense.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|       decoder.bert.encoder.layer.4.attention.output.dense.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight   |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.4.crossattention.self.query.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.4.crossattention.self.query.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.4.crossattention.self.key.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|      decoder.bert.encoder.layer.4.crossattention.self.key.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.4.crossattention.self.value.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.4.crossattention.self.value.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|   decoder.bert.encoder.layer.4.crossattention.output.dense.weight   |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|    decoder.bert.encoder.layer.4.crossattention.output.dense.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "| decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|  decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias  |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.4.intermediate.dense.weight       |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|         decoder.bert.encoder.layer.4.intermediate.dense.bias        |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|           decoder.bert.encoder.layer.4.output.dense.weight          |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|            decoder.bert.encoder.layer.4.output.dense.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         decoder.bert.encoder.layer.4.output.LayerNorm.weight        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          decoder.bert.encoder.layer.4.output.LayerNorm.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.5.attention.self.query.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.5.attention.self.query.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.5.attention.self.key.weight       |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         decoder.bert.encoder.layer.5.attention.self.key.bias        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.5.attention.self.value.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.5.attention.self.value.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      decoder.bert.encoder.layer.5.attention.output.dense.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|       decoder.bert.encoder.layer.5.attention.output.dense.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight   |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.5.crossattention.self.query.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.5.crossattention.self.query.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.5.crossattention.self.key.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|      decoder.bert.encoder.layer.5.crossattention.self.key.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.5.crossattention.self.value.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.5.crossattention.self.value.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|   decoder.bert.encoder.layer.5.crossattention.output.dense.weight   |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|    decoder.bert.encoder.layer.5.crossattention.output.dense.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "| decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|  decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias  |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.5.intermediate.dense.weight       |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|         decoder.bert.encoder.layer.5.intermediate.dense.bias        |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|           decoder.bert.encoder.layer.5.output.dense.weight          |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|            decoder.bert.encoder.layer.5.output.dense.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         decoder.bert.encoder.layer.5.output.LayerNorm.weight        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          decoder.bert.encoder.layer.5.output.LayerNorm.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.6.attention.self.query.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.6.attention.self.query.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.6.attention.self.key.weight       |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         decoder.bert.encoder.layer.6.attention.self.key.bias        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.6.attention.self.value.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.6.attention.self.value.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      decoder.bert.encoder.layer.6.attention.output.dense.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|       decoder.bert.encoder.layer.6.attention.output.dense.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight   |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.6.crossattention.self.query.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.6.crossattention.self.query.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.6.crossattention.self.key.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|      decoder.bert.encoder.layer.6.crossattention.self.key.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.6.crossattention.self.value.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.6.crossattention.self.value.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|   decoder.bert.encoder.layer.6.crossattention.output.dense.weight   |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|    decoder.bert.encoder.layer.6.crossattention.output.dense.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "| decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|  decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias  |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.6.intermediate.dense.weight       |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|         decoder.bert.encoder.layer.6.intermediate.dense.bias        |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|           decoder.bert.encoder.layer.6.output.dense.weight          |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|            decoder.bert.encoder.layer.6.output.dense.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         decoder.bert.encoder.layer.6.output.LayerNorm.weight        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          decoder.bert.encoder.layer.6.output.LayerNorm.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.7.attention.self.query.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.7.attention.self.query.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.7.attention.self.key.weight       |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|         decoder.bert.encoder.layer.7.attention.self.key.bias        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|       decoder.bert.encoder.layer.7.attention.self.value.weight      |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|        decoder.bert.encoder.layer.7.attention.self.value.bias       |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|      decoder.bert.encoder.layer.7.attention.output.dense.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|       decoder.bert.encoder.layer.7.attention.output.dense.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight   |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.7.crossattention.self.query.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.7.crossattention.self.query.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|     decoder.bert.encoder.layer.7.crossattention.self.key.weight     |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|      decoder.bert.encoder.layer.7.crossattention.self.key.bias      |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|    decoder.bert.encoder.layer.7.crossattention.self.value.weight    |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|     decoder.bert.encoder.layer.7.crossattention.self.value.bias     |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|   decoder.bert.encoder.layer.7.crossattention.output.dense.weight   |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|    decoder.bert.encoder.layer.7.crossattention.output.dense.bias    |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "| decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|  decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias  |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|        decoder.bert.encoder.layer.7.intermediate.dense.weight       |  +  |  2048 x 512 |  1048576   | torch.float32 | 4194304  |\n",
      "|         decoder.bert.encoder.layer.7.intermediate.dense.bias        |  +  |     2048    |    2048    | torch.float32 |   8192   |\n",
      "|           decoder.bert.encoder.layer.7.output.dense.weight          |  +  |  512 x 2048 |  1048576   | torch.float32 | 4194304  |\n",
      "|            decoder.bert.encoder.layer.7.output.dense.bias           |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|         decoder.bert.encoder.layer.7.output.LayerNorm.weight        |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          decoder.bert.encoder.layer.7.output.LayerNorm.bias         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|                     decoder.cls.predictions.bias                    |  +  |    30522    |   30522    | torch.float32 |  122088  |\n",
      "|            decoder.cls.predictions.transform.dense.weight           |  +  |  512 x 512  |   262144   | torch.float32 | 1048576  |\n",
      "|             decoder.cls.predictions.transform.dense.bias            |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|          decoder.cls.predictions.transform.LayerNorm.weight         |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "|           decoder.cls.predictions.transform.LayerNorm.bias          |  +  |     512     |    512     | torch.float32 |   2048   |\n",
      "+---------------------------------------------------------------------+-----+-------------+------------+---------------+----------+\n",
      "Total Trainable Params: 91191098\n",
      "Total memory (min): 356,215.23 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91191098"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers.utils import count_parameters\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(   # gpu 6 GB\n",
    "    output_dir=\"./bert2bert-base-results\",\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    ignore_data_skip=True,                  # disable ignore unused field in data warnings\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,          # eff.batch = grd_acc * batch = 32\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2*8,                   # eff.epochs = epoch / grd_acc = 2\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_arxiv[\"train\"],\n",
    "    eval_dataset=tokenized_arxiv[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 800\n",
      "  Num Epochs = 16\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 400\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]/home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "  6%|▋         | 25/400 [00:27<06:47,  1.09s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                \n",
      "  6%|▋         | 25/400 [00:29<06:47,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.713404655456543, 'eval_runtime': 2.4418, 'eval_samples_per_second': 81.906, 'eval_steps_per_second': 20.476, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 50/400 [00:55<05:56,  1.02s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                \n",
      " 12%|█▎        | 50/400 [00:58<05:56,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.164654731750488, 'eval_runtime': 2.3511, 'eval_samples_per_second': 85.065, 'eval_steps_per_second': 21.266, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 75/400 [01:24<05:39,  1.05s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                \n",
      " 19%|█▉        | 75/400 [01:27<05:39,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.90974235534668, 'eval_runtime': 2.3386, 'eval_samples_per_second': 85.521, 'eval_steps_per_second': 21.38, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 100/400 [01:55<06:02,  1.21s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      " 25%|██▌       | 100/400 [01:58<06:02,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.8107171058654785, 'eval_runtime': 2.6813, 'eval_samples_per_second': 74.59, 'eval_steps_per_second': 18.648, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 125/400 [02:28<05:01,  1.10s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      " 31%|███▏      | 125/400 [02:30<05:01,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.702428817749023, 'eval_runtime': 2.4794, 'eval_samples_per_second': 80.666, 'eval_steps_per_second': 20.166, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 150/400 [02:57<04:33,  1.09s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      " 38%|███▊      | 150/400 [03:00<04:33,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.663840293884277, 'eval_runtime': 2.4499, 'eval_samples_per_second': 81.635, 'eval_steps_per_second': 20.409, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 175/400 [03:27<03:56,  1.05s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      " 44%|████▍     | 175/400 [03:29<03:56,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.597473621368408, 'eval_runtime': 2.6777, 'eval_samples_per_second': 74.691, 'eval_steps_per_second': 18.673, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 200/400 [04:00<03:49,  1.15s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      " 50%|█████     | 200/400 [04:03<03:49,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.59846305847168, 'eval_runtime': 2.5076, 'eval_samples_per_second': 79.758, 'eval_steps_per_second': 19.94, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 225/400 [04:32<03:10,  1.09s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      " 56%|█████▋    | 225/400 [04:35<03:10,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.5791802406311035, 'eval_runtime': 2.3903, 'eval_samples_per_second': 83.672, 'eval_steps_per_second': 20.918, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 250/400 [05:02<02:41,  1.08s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      " 62%|██████▎   | 250/400 [05:05<02:41,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.547262191772461, 'eval_runtime': 2.59, 'eval_samples_per_second': 77.221, 'eval_steps_per_second': 19.305, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 275/400 [05:32<02:25,  1.17s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      " 69%|██████▉   | 275/400 [05:35<02:25,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.557379245758057, 'eval_runtime': 2.534, 'eval_samples_per_second': 78.927, 'eval_steps_per_second': 19.732, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 300/400 [06:01<01:45,  1.05s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      " 75%|███████▌  | 300/400 [06:04<01:45,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.554116725921631, 'eval_runtime': 2.4232, 'eval_samples_per_second': 82.535, 'eval_steps_per_second': 20.634, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 325/400 [06:31<01:21,  1.09s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      " 81%|████████▏ | 325/400 [06:34<01:21,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.545895576477051, 'eval_runtime': 2.6332, 'eval_samples_per_second': 75.954, 'eval_steps_per_second': 18.989, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 350/400 [07:04<01:03,  1.28s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      " 88%|████████▊ | 350/400 [07:07<01:03,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.5565714836120605, 'eval_runtime': 3.0192, 'eval_samples_per_second': 66.242, 'eval_steps_per_second': 16.561, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 375/400 [07:36<00:27,  1.09s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      " 94%|█████████▍| 375/400 [07:39<00:27,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.547601699829102, 'eval_runtime': 2.6015, 'eval_samples_per_second': 76.878, 'eval_steps_per_second': 19.219, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [08:06<00:00,  1.06s/it]The following columns in the evaluation set  don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: token_type_ids, abstract, title. If token_type_ids, abstract, title are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 4\n",
      "                                                 \n",
      "100%|██████████| 400/400 [08:08<00:00,  1.06s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 400/400 [08:08<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.549394607543945, 'eval_runtime': 2.4135, 'eval_samples_per_second': 82.866, 'eval_steps_per_second': 20.717, 'epoch': 16.0}\n",
      "{'train_runtime': 488.8834, 'train_samples_per_second': 26.182, 'train_steps_per_second': 0.818, 'train_loss': 4.19384521484375, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=4.19384521484375, metrics={'train_runtime': 488.8834, 'train_samples_per_second': 26.182, 'train_steps_per_second': 0.818, 'train_loss': 4.19384521484375, 'epoch': 16.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqdm._instances.clear()\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(example):\n",
    "    input_ids = tokenizer(prefix + example[\"abstract\"], \n",
    "                        max_length=max_abstract, \n",
    "                        truncation=True, \n",
    "                        return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "    outputs = model.generate(input_ids.to(device))\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('we prove that the 3-state potts antiferromagnet on the diced lattice (dual of the kagome lattice) has entropically-driven long-range order at low temperatures (including zero). we then present monte carlo simulations, using a cluster algorithm, of the 3-state and 4-state models. the 3-state model has a phase transition to the high-temperature disordered phase at v = e^j - 1 = -0.860599 +- 0.000004 that appears to be in the universality class of the 3-state potts ferromagnet. the 4-state model is disordered throughout the physical region, including at zero temperature.',\n",
       " 'phase transition in the 3-state potts antiferromagnet on the diced   lattice',\n",
       " 'a non - non - non - - covariance model of the non - covar')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "arxiv_dataset[\"test\"][n][\"abstract\"], arxiv_dataset[\"test\"][n][\"title\"], generate(arxiv_dataset[\"test\"][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('we employ conditional tsallis q entropies to study the separability of symmetric one parameter w and ghz multiqubit mixed states. the strongest limitation on separability is realized in the limit q-->infinity, and is found to be much superior to the condition obtained using the von neumann conditional entropy (q=1 case). except for the example of two qubit and three qubit symmetric states of ghz family, the $q$-conditional entropy method leads to sufficient - but not necessary - conditions on separability.',\n",
       " 'separability of a family of one parameter w and ghz multiqubit states   using abe-rajagopal q-conditional entropy approach',\n",
       " 'a multi - semiparable multivariate multiplebility and the multiplebility of')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 42\n",
    "arxiv_dataset[\"test\"][n][\"abstract\"], arxiv_dataset[\"test\"][n][\"title\"], generate(arxiv_dataset[\"test\"][n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU-score\n",
    "\n",
    "Самоделки:\n",
    "- 0.02457 (словарь 6152, по 5 эпох по 5r-4, 1e-3, min.val.loss = 3.875) \n",
    "- **0.19204** (словарь 60 тыс. ~15 эпох с шагом 5e-4 -> 5e-5, min.val.loss = 2.289)\n",
    "- 0.12601 (словарь 84 тыс. много разных эпох, сходится плохо, min.val.loss = 3.305)\n",
    "- 0.10644 (BPE, словарь 16 тыс., много разных эпох, сходится плохо, min.val.loss = 3.8)\n",
    "\n",
    "T5-small\n",
    "- BLEU-score: **0.044...** 1% тюнинг\n",
    "- BLEU-score: **0.16563** (3 эпохи - 2,5 часа RTX2060 6Gb)\n",
    "\n",
    "T5-base\n",
    "- BLEU-score: **0.07422** (без обучения)\n",
    "- обучение не тянет...\n",
    "\n",
    "BART-base\n",
    "- BLEU-score: **0.17743** 1% тюнинг\n",
    "- BLEU-score: **0.17984** (1.43 эпохи - 2,5 часа RTX2060 6Gb)\n",
    "- BLEU-score: **0.19266** (2 эпохи)\n",
    "\n",
    "bert2bert-small (4 слоя, 8 голов внимания, 512 эмбеддинг)\n",
    "- BLEU-score: **0.00524** 1% тюнинг\n",
    "- BLEU-score: **0.04807** (2 эпохи) чето не то....\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 72/200 [00:23<00:41,  3.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user1/projects/stepik.org/DataScience/Нейронные сети и обработка текста/stepik-dl-nlp/task11_kaggle/7. BERT_seq2seq.ipynb Cell 30'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/7.%20BERT_seq2seq.ipynb#ch0000029?line=5'>6</a>\u001b[0m references \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/7.%20BERT_seq2seq.ipynb#ch0000029?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m tqdm(tokenized_arxiv[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/7.%20BERT_seq2seq.ipynb#ch0000029?line=7'>8</a>\u001b[0m     candidates\u001b[39m.\u001b[39mappend(generate(example)\u001b[39m.\u001b[39msplit())\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/7.%20BERT_seq2seq.ipynb#ch0000029?line=8'>9</a>\u001b[0m     references\u001b[39m.\u001b[39mappend([example[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msplit()])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/7.%20BERT_seq2seq.ipynb#ch0000029?line=10'>11</a>\u001b[0m score \u001b[39m=\u001b[39m bleu_score(candidates, references, max_n\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, weights\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m)\n",
      "\u001b[1;32m/home/user1/projects/stepik.org/DataScience/Нейронные сети и обработка текста/stepik-dl-nlp/task11_kaggle/7. BERT_seq2seq.ipynb Cell 26'\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/7.%20BERT_seq2seq.ipynb#ch0000025?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(example):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/7.%20BERT_seq2seq.ipynb#ch0000025?line=1'>2</a>\u001b[0m     input_ids \u001b[39m=\u001b[39m tokenizer(prefix \u001b[39m+\u001b[39m example[\u001b[39m\"\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m\"\u001b[39m], \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/7.%20BERT_seq2seq.ipynb#ch0000025?line=2'>3</a>\u001b[0m                         max_length\u001b[39m=\u001b[39mmax_abstract, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/7.%20BERT_seq2seq.ipynb#ch0000025?line=3'>4</a>\u001b[0m                         truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/7.%20BERT_seq2seq.ipynb#ch0000025?line=4'>5</a>\u001b[0m                         return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids  \u001b[39m# Batch size 1\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/7.%20BERT_seq2seq.ipynb#ch0000025?line=5'>6</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/7.%20BERT_seq2seq.ipynb#ch0000025?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mdecode(outputs[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py:1254\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1248'>1249</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1249'>1250</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1, but is \u001b[39m\u001b[39m{\u001b[39;00mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m when doing greedy search.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1250'>1251</a>\u001b[0m         )\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1252'>1253</a>\u001b[0m     \u001b[39m# 10. run greedy search\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1253'>1254</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1254'>1255</a>\u001b[0m         input_ids,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1255'>1256</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1256'>1257</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1257'>1258</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1258'>1259</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1259'>1260</a>\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1260'>1261</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1261'>1262</a>\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1262'>1263</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1263'>1264</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1265'>1266</a>\u001b[0m \u001b[39melif\u001b[39;00m is_sample_gen_mode:\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1266'>1267</a>\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1267'>1268</a>\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1268'>1269</a>\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, typical_p\u001b[39m=\u001b[39mtypical_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1269'>1270</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py:1638\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1634'>1635</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1636'>1637</a>\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1637'>1638</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1638'>1639</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1639'>1640</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1640'>1641</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1641'>1642</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1642'>1643</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1644'>1645</a>\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/generation_utils.py?line=1645'>1646</a>\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:513\u001b[0m, in \u001b[0;36mEncoderDecoderModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=507'>508</a>\u001b[0m     decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=508'>509</a>\u001b[0m         labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=509'>510</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=511'>512</a>\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=512'>513</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=513'>514</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=514'>515</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=515'>516</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=516'>517</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=517'>518</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=518'>519</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=519'>520</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=520'>521</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=521'>522</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=522'>523</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=523'>524</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_decoder,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=524'>525</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=526'>527</a>\u001b[0m \u001b[39m# Compute loss independent from decoder (as some shift the logits inside them)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=527'>528</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py:1226\u001b[0m, in \u001b[0;36mBertLMHeadModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1222'>1223</a>\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1223'>1224</a>\u001b[0m     use_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1225'>1226</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1226'>1227</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1227'>1228</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1228'>1229</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1229'>1230</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1230'>1231</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1231'>1232</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1232'>1233</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1233'>1234</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1234'>1235</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1235'>1236</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1236'>1237</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1237'>1238</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1238'>1239</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1239'>1240</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1241'>1242</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1242'>1243</a>\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=986'>987</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=988'>989</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=989'>990</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=990'>991</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=993'>994</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=994'>995</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=995'>996</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=996'>997</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=997'>998</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=998'>999</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=999'>1000</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1000'>1001</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1001'>1002</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1002'>1003</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1003'>1004</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1004'>1005</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1005'>1006</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1006'>1007</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1007'>1008</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=1008'>1009</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=575'>576</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=576'>577</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=577'>578</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=581'>582</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=582'>583</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=583'>584</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=584'>585</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=585'>586</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=586'>587</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=587'>588</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=588'>589</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=589'>590</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=590'>591</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=591'>592</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=592'>593</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=594'>595</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=595'>596</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py:472\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=459'>460</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=460'>461</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=461'>462</a>\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=468'>469</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=469'>470</a>\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=470'>471</a>\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=471'>472</a>\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=472'>473</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=473'>474</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=474'>475</a>\u001b[0m         head_mask,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=475'>476</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=476'>477</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=477'>478</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=478'>479</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=480'>481</a>\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py:411\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=391'>392</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=392'>393</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=393'>394</a>\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=399'>400</a>\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=400'>401</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=401'>402</a>\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=402'>403</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=403'>404</a>\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=408'>409</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=409'>410</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=410'>411</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(self_outputs[\u001b[39m0\u001b[39;49m], hidden_states)\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=411'>412</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=412'>413</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py:363\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=360'>361</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=361'>362</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=362'>363</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLayerNorm(hidden_states \u001b[39m+\u001b[39;49m input_tensor)\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/transformers/models/bert/modeling_bert.py?line=363'>364</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/torch/nn/modules/normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/normalization.py?line=187'>188</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/normalization.py?line=188'>189</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/modules/normalization.py?line=189'>190</a>\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/envs/py310/lib64/python3/site-packages/torch/nn/functional.py:2486\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/functional.py?line=2481'>2482</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/functional.py?line=2482'>2483</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/functional.py?line=2483'>2484</a>\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/functional.py?line=2484'>2485</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///home/user1/envs/py310/lib64/python3/site-packages/torch/nn/functional.py?line=2485'>2486</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "tqdm._instances.clear()\n",
    "\n",
    "candidates = []\n",
    "references = []\n",
    "for example in tqdm(tokenized_arxiv[\"test\"]):\n",
    "    candidates.append(generate(example).split())\n",
    "    references.append([example[\"title\"].split()])\n",
    "\n",
    "score = bleu_score(candidates, references, max_n=3, weights=[1/3]*3)\n",
    "\n",
    "print('BLEU-score: {0:.5f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepik score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NAME = \"bert2bert-small\" if USE_SMALL else \"bert2bert-small-tune\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация заголовков для тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm._instances.clear()\n",
    "\n",
    "abstracts = []\n",
    "titles = []\n",
    "\n",
    "for example in tqdm(scoring_dataset):\n",
    "    abstracts.append(example[\"abstract\"])\n",
    "    titles.append(generate(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось, например"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts[1], titles[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записываем полученные заголовки в файл формата `<abstract>,<title>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission_df = pd.DataFrame({'abstract': abstracts, 'title': titles})\n",
    "submission_df.to_csv(f\"./submission/predicted_titles_{SUBMISSION_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"title\"].apply(lambda x: len(str(x).split())).describe()[[\"mean\",\"std\", \"max\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью скрипта `generate_csv` приводим файл `submission_prediction.csv` в формат, необходимый для отправки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.create_submission import generate_csv\n",
    "\n",
    "generate_csv(input_file=f\"./submission/predicted_titles_{SUBMISSION_NAME}.csv\", \n",
    "             output_file=f'./submission/submission_{SUBMISSION_NAME}.csv', \n",
    "             voc_file=f'./datasets/vocs.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# С учетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv(\"./datasets/train.csv\")\n",
    "submission_df = pd.read_csv(f\"./submission/predicted_titles_{SUBMISSION_NAME}.csv\")\n",
    "\n",
    "intersect_idx = np.intersect1d(submission_df[\"abstract\"].str.lower(), train_df[\"abstract\"].str.lower(), return_indices=True)\n",
    "\n",
    "submission_df.loc[intersect_idx[1], 'title'] = train_df.loc[intersect_idx[2], 'title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.create_submission import generate_csv\n",
    "\n",
    "submission_df.to_csv(f\"./submission/predicted_titles_{SUBMISSION_NAME}_fake.csv\", index=False)\n",
    "\n",
    "generate_csv(input_file=f\"./submission/predicted_titles_{SUBMISSION_NAME}_fake.csv\", \n",
    "             output_file=f'./submission/submission_{SUBMISSION_NAME}_fake.csv', \n",
    "             voc_file=f'./datasets/vocs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'./submission/submission_{SUBMISSION_NAME}_fake.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5-small:\n",
    "- **Score: 0.26174** 1% tuning\n",
    "- **Score: 0.34497** tuning 3 эпохи\n",
    "- **Score: 0.51810** + добавление правильных меток из трейна\n",
    "\n",
    "T5-base:\n",
    "- **Score: 0.20510** w/o tuning\n",
    "- для обучения с имеющейся длиной последовательности не хватает памяти GPU\n",
    "\n",
    "BART-base\n",
    "- **Score: 0.33851** 1% tuning\n",
    "- **Score: 0.39536** tuning 1,5 эпохи\n",
    "- **Score: 0.54804** + добавление правильных меток из трейна\n",
    "- **Score: 0.56782** 2 эпохи с накопление градиента (вот и в топ-10)\n",
    "- ... дальше не интересно"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d00ba0b92f737b23b3e678e3a3ceb3fe4e948ad1ab95d9c6fdcbb4b4ec65f3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
