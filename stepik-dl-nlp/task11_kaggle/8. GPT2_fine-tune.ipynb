{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По мотивам топового решения на Каггле, [туториала](https://towardsdatascience.com/conditional-text-generation-by-fine-tuning-gpt-2-11c1a9fc639d)\n",
    "\n",
    "- из данных убраны дубли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = f\"./datasets/train_clean.csv\"\n",
    "SMALL_CSV = f\"./cache/train.csv\"\n",
    "SCORING_CSV = f\"./datasets/test.csv\"\n",
    "\n",
    "USE_SMALL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_title = 34          # max 103, \"типа 3 сигмы\"\n",
    "max_abstract = 457      # max 1096, \"типа 3 сигмы\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2 124M parameters - 523 MB\n",
    "# distilgpt2 82M parameters - 336 MB\n",
    "MODEL = \"gpt2\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасеты\n",
    "\n",
    "- пакет huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-044aa5b2537bb910\n",
      "Reusing dataset csv (/home/user1/.cache/huggingface/datasets/csv/default-044aa5b2537bb910/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "arxiv_dataset = datasets.Dataset.from_csv(SMALL_CSV if USE_SMALL else TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2 if USE_SMALL else 0.02\n",
    "arxiv_dataset = arxiv_dataset.train_test_split(test_size=test_size)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103490, 2113, dict_keys(['abstract', 'title']))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arxiv_dataset[\"train\"]), len(arxiv_dataset[\"test\"]), arxiv_dataset[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2e0a9ad90b647d2d\n",
      "Reusing dataset csv (/home/user1/.cache/huggingface/datasets/csv/default-2e0a9ad90b647d2d/0.0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, dict_keys(['abstract']))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_dataset = datasets.Dataset.from_csv(SCORING_CSV)\n",
    "len(scoring_dataset), scoring_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "SPECIAL_TOKENS  = { \"bos_token\": \"[BOS]\",\n",
    "                    \"eos_token\": \"[EOS]\",\n",
    "                    \"unk_token\": \"[UNK]\",                    \n",
    "                    \"pad_token\": \"[PAD]\",\n",
    "                    \"sep_token\": \"[SEP]\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)   # 1.3 MB\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 построена на декодерах. Работает в режиме языковой модели - т.е. на вход последовательность, на выход прогноз ее продолжения. Поэтому для обучения и генерации данные и таргет ей подаются одной последовательностью, разделенной специальными токенами, которые она и запоминает.\n",
    "- в пакете `transformers` тут также надо подавать `label` == `input_ids` для единоообразия конструкций для трейнера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_gpt2(examples):\n",
    "    \"\"\"Без оптимизаций, сразу паддинг по максимальному\"\"\"\n",
    "\n",
    "    inputs = [tokenizer.bos_token + abstract + \\\n",
    "             tokenizer.sep_token + title + \\\n",
    "             tokenizer.eos_token for (abstract, title) in zip(examples[\"abstract\"], examples[\"title\"])]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=max_abstract + max_title + 3, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"label\"] = model_inputs[\"input_ids\"]   # вот такое API\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:55<00:00,  1.88ba/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.82ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_arxiv = arxiv_dataset.map(preprocess_function_gpt2, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['abstract', 'title', 'input_ids', 'attention_mask', 'label']),\n",
       " 'as in many other areas of science, systems biology makes extensive use of statistical association and significance estimates in contingency tables, a type of categorical data analysis known in this field as enrichment (also over-representation or enhancement) analysis. in spite of efforts to create probabilistic annotations, especially in the gene ontology context, or to deal with uncertainty in high throughput-based datasets, current enrichment methods largely ignore this probabilistic information since they are mainly based on variants of the fisher exact test. we developed an open-source r package to deal with probabilistic categorical data analysis, probcd, that does not require a static contingency table. the contingency table for the enrichment problem is built using the expectation of a bernoulli scheme stochastic process given the categorization probabilities. an on-line interface was created to allow usage by non-programmers and is available at: http://xerad.systemsbiology.net/probcd/ . we present an analysis framework and software tools to address the issue of uncertainty in categorical data analysis. in particular, concerning the enrichment analysis, probcd can accommodate: (i) the stochastic nature of the high-throughput experimental techniques and (ii) probabilistic gene annotation.',\n",
       " 'probcd: enrichment analysis accounting for categorization uncertainty')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_arxiv[\"train\"][0].keys(), tokenized_arxiv[\"train\"][0][\"abstract\"], tokenized_arxiv[\"train\"][0][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Специальные токены работают"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[BOS]as in many other areas of science, systems biology makes extensive use of statistical association and significance estimates in contingency tables, a type of categorical data analysis known in this field as enrichment (also over-representation or enhancement) analysis. in spite of efforts to create probabilistic annotations, especially in the gene ontology context, or to deal with uncertainty in high throughput-based datasets, current enrichment methods largely ignore this probabilistic information since they are mainly based on variants of the fisher exact test. we developed an open-source r package to deal with probabilistic categorical data analysis, probcd, that does not require a static contingency table. the contingency table for the enrichment problem is built using the expectation of a bernoulli scheme stochastic process given the categorization probabilities. an on-line interface was created to allow usage by non-programmers and is available at: http://xerad.systemsbiology.net/probcd/. we present an analysis framework and software tools to address the issue of uncertainty in categorical data analysis. in particular, concerning the enrichment analysis, probcd can accommodate: (i) the stochastic nature of the high-throughput experimental techniques and (ii) probabilistic gene annotation.[SEP]probcd: enrichment analysis accounting for categorization uncertainty[EOS][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_arxiv[\"train\"][0][\"input_ids\"], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50262, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, TrainingArguments, Trainer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL,\n",
    "                                        bos_token_id=tokenizer.bos_token_id,\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        sep_token_id=tokenizer.sep_token_id,\n",
    "                                        pad_token_id=tokenizer.pad_token_id, \n",
    "                                        output_hidden_states=False)\n",
    "\n",
    "# как вариант - западить все [EOS] и в конструктор pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заморозка слоев (чем дальше слой от входа, тем более общие зависомсти в исходных последовательностях он акуумулирует (токен -> предложение -> текст -> дискурс)).\n",
    "\n",
    "По-идее, призвано ускорить сходимость при сохранениии генерализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False                         # везде отключено построение дерева autograd\n",
    "\n",
    "for i, m in enumerate(model.transformer.h):        \n",
    "    #Only un-freeze the last n transformer blocks\n",
    "    if i >= 6:\n",
    "        for parameter in m.parameters():\n",
    "            parameter.requires_grad = True \n",
    "\n",
    "for parameter in model.transformer.ln_f.parameters():        \n",
    "    parameter.requires_grad = True\n",
    "\n",
    "for parameter in model.lm_head.parameters():        \n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)    # if no train else trainer sends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# не работает с GPT-2 (отпаддили сразу все заранее, это вроде как некрасиво, но можно, и по памяти не отвалится неожиданно)\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./GPT2-results\",\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_accumulation_steps=12,         # eff.batch = grd_acc * batch = 36\n",
    "    weight_decay=0.01,\n",
    "    save_steps=100, \n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=0.05*12,               # eff.epochs = epoch / grd_acc = 0.05\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_arxiv[\"train\"],\n",
    "    eval_dataset=tokenized_arxiv[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: title, abstract. If title, abstract are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 103490\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 36\n",
      "  Gradient Accumulation steps = 12\n",
      "  Total optimization steps = 1725\n",
      " 29%|██▉       | 500/1725 [27:45<1:07:59,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8089, 'learning_rate': 1.4249275362318842e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1000/1725 [55:29<40:04,  3.32s/it] Saving model checkpoint to ./GPT2-results/checkpoint-1000\n",
      "Configuration saved in ./GPT2-results/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5474, 'learning_rate': 8.452173913043478e-06, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./GPT2-results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./GPT2-results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./GPT2-results/checkpoint-1000/special_tokens_map.json\n",
      " 87%|████████▋ | 1500/1725 [1:23:27<12:19,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5306, 'learning_rate': 2.6550724637681165e-06, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1725/1725 [1:35:43<00:00,  3.30s/it]The following columns in the evaluation set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: title, abstract. If title, abstract are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2113\n",
      "  Batch size = 3\n",
      "                                                     \n",
      "100%|██████████| 1725/1725 [1:36:50<00:00,  3.30s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1725/1725 [1:36:50<00:00,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.433777928352356, 'eval_runtime': 66.8537, 'eval_samples_per_second': 31.606, 'eval_steps_per_second': 10.545, 'epoch': 0.6}\n",
      "{'train_runtime': 5810.734, 'train_samples_per_second': 10.686, 'train_steps_per_second': 0.297, 'train_loss': 1.9055420275701993, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1725, training_loss=1.9055420275701993, metrics={'train_runtime': 5810.734, 'train_samples_per_second': 10.686, 'train_steps_per_second': 0.297, 'train_loss': 1.9055420275701993, 'epoch': 0.6})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tqdm._instances.clear()\n",
    "\n",
    "trainer.train()#(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./GPT2-results\n",
      "Configuration saved in ./GPT2-results/config.json\n",
      "Model weights saved in ./GPT2-results/pytorch_model.bin\n",
      "tokenizer config file saved in ./GPT2-results/tokenizer_config.json\n",
      "Special tokens file saved in ./GPT2-results/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация\n",
    "\n",
    "Модель помнит, что после текста, обрамленного `[BOS]` `[SEP]`, идет другой текст, заканчивающийся `[EOS]`, и пытается его максимально правдоподобно воспроизвести.\n",
    "\n",
    "Готового seq2seq метода генерации в пакете нету, поэтому:\n",
    "- считаем ответом все что модель нагенерила после `[SEP]` и по длине не более `max_title` (ибо недоученная она генерит до упора) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = model.config.eos_token_id       # ?? не надо, когда все спец.токены в модели\n",
    "model.config.max_length = max_abstract + max_title + 3\n",
    "\n",
    "def generate(example, num_beams=3, num_return_sequences=1):\n",
    "    \"\"\"Метод генерации - только лучевой поиск\"\"\"\n",
    "\n",
    "    text = tokenizer.bos_token + example[\"abstract\"] + tokenizer.sep_token\n",
    "    input_ids = tokenizer(text, \n",
    "                        max_length=max_abstract + 2, \n",
    "                        truncation=True, \n",
    "                        return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "    outputs = model.generate(input_ids.to(device), do_sample=True, \n",
    "                                                   num_beams=num_beams, \n",
    "                                                   max_length = max_abstract + max_title + 3,\n",
    "                                                   repetition_penalty=5.0, \n",
    "                                                   no_repeat_ngram_size  = 2, \n",
    "                                                   early_stopping=True, \n",
    "                                                   num_return_sequences=num_return_sequences)\n",
    "    \n",
    "    res = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    res = res[res.find(tokenizer.sep_token)+len(tokenizer.sep_token):res.find(tokenizer.eos_token)]\n",
    "    \n",
    "    return \" \".join(res.split()[:max_title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('we provide a jacobian criterion that applies to arbitrary chemical reaction networks taken with mass-action kinetics to preclude the existence of multiple positive steady states within any stoichiometric class for any choice of rate constants. we are concerned with the characterization of injective networks, that is, networks for which the species formation rate function is injective in the interior of the positive orthant within each stoichiometric class. we show that a network is injective if and only if the determinant of the jacobian of a certain function does not vanish. the function consists of components of the species formation rate function and a maximal set of independent conservation laws. the determinant of the function is a polynomial in the species concentrations and the rate constants (linear in the latter) and its coefficients are fully determined. the criterion also precludes the existence of degenerate steady states. further, we relate injectivity of a chemical reaction network to that of the chemical reaction network obtained by adding outflow, or degradation, reactions for all species.',\n",
       " 'preclusion of switch behavior in reaction networks with mass-action   kinetics',\n",
       " 'dynamic injection of peptides')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "arxiv_dataset[\"test\"][n][\"abstract\"], arxiv_dataset[\"test\"][n][\"title\"], generate(arxiv_dataset[\"test\"][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('compact pseudo-riemannian manifolds that have parallel weyl tensor without being conformally flat or locally symmetric are known to exist in infinitely many dimensions greater than 4. we prove some general topological properties of such manifolds, namely, vanishing of the euler characteristic and real pontryagin classes, and infiniteness of the fundamental group. we also show that, in the lorentzian case, each of them is at least 5-dimensional and admits a two-fold cover which is a bundle over the circle.',\n",
       " 'on compact manifolds admitting indefinite metrics with parallel weyl   tensor',\n",
       " 'compacts with hyperbolic polynomials')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 42\n",
    "arxiv_dataset[\"test\"][n][\"abstract\"], arxiv_dataset[\"test\"][n][\"title\"], generate(arxiv_dataset[\"test\"][n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU-score\n",
    "\n",
    "Самоделки:\n",
    "- 0.02457 (словарь 6152, по 5 эпох по 5r-4, 1e-3, min.val.loss = 3.875) \n",
    "- **0.19204** (словарь 60 тыс. ~15 эпох с шагом 5e-4 -> 5e-5, min.val.loss = 2.289)\n",
    "- 0.12601 (словарь 84 тыс. много разных эпох, сходится плохо, min.val.loss = 3.305)\n",
    "- 0.10644 (BPE, словарь 16 тыс., много разных эпох, сходится плохо, min.val.loss = 3.8)\n",
    "\n",
    "T5-small\n",
    "- BLEU-score: **0.044...** 1% тюнинг\n",
    "- BLEU-score: **0.16563** (3 эпохи - 2,5 часа RTX2060 6Gb)\n",
    "\n",
    "T5-base\n",
    "- BLEU-score: **0.07422** (без обучения)\n",
    "- обучение не тянет...\n",
    "\n",
    "BART-base\n",
    "- BLEU-score: **0.17743** 1% тюнинг\n",
    "- BLEU-score: **0.17984** (1.43 эпохи - 2,5 часа RTX2060 6Gb)\n",
    "- BLEU-score: **0.19266** (2 эпохи)\n",
    "\n",
    "GPT-2\n",
    "- BLEU-score: **0.00318** 5% тюниг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2113/2113 [32:27<00:00,  1.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-score: 0.00318\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "tqdm._instances.clear()\n",
    "\n",
    "candidates = []\n",
    "references = []\n",
    "for example in tqdm(tokenized_arxiv[\"test\"]):\n",
    "    candidates.append(generate(example).split())\n",
    "    references.append([example[\"title\"].split()])\n",
    "\n",
    "score = bleu_score(candidates, references, max_n=3, weights=[1/3]*3)\n",
    "\n",
    "print('BLEU-score: {0:.5f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepik score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NAME = \"GPT-base\" if USE_SMALL else \"GPT-base-tune\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация заголовков для тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [15:11<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm._instances.clear()\n",
    "\n",
    "abstracts = []\n",
    "titles = []\n",
    "\n",
    "for example in tqdm(scoring_dataset):\n",
    "    abstracts.append(example[\"abstract\"])\n",
    "    titles.append(generate(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось, например"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The doc2vec approach was introduced as an extension to word2vec (Le and Mikolov, 2014), to generate embeddings at the level of entire documents, with interesting results, followed by mixed success at reproducing results from the initial paper. This paper presents a rigorous empirical evaluation of doc2vec over two tasks. We compare doc2vec to two baselines and 2 advanced embedding-generating methodologies for documents. We found that doc2vec performs robustly when using models trained on large external corpora, and can be further improved by using pre-trained word embeddings. We also provide recommendations on hyper-parameter settings for general purpose applications, and release source code to induce document embeddings using our trained doc2vec models.',\n",
       " 'doc2vars: training methods in embedded documents')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[1], titles[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записываем полученные заголовки в файл формата `<abstract>,<title>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission_df = pd.DataFrame({'abstract': abstracts, 'title': titles})\n",
    "submission_df.to_csv(f\"./submission/predicted_titles_{SUBMISSION_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean    12.066000\n",
       "std     10.477663\n",
       "max     34.000000\n",
       "Name: title, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df[\"title\"].apply(lambda x: len(str(x).split())).describe()[[\"mean\",\"std\", \"max\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью скрипта `generate_csv` приводим файл `submission_prediction.csv` в формат, необходимый для отправки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.create_submission import generate_csv\n",
    "\n",
    "generate_csv(input_file=f\"./submission/predicted_titles_{SUBMISSION_NAME}.csv\", \n",
    "             output_file=f'./submission/submission_{SUBMISSION_NAME}.csv', \n",
    "             voc_file=f'./datasets/vocs.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# С учетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv(\"./datasets/train.csv\")\n",
    "submission_df = pd.read_csv(f\"./submission/predicted_titles_{SUBMISSION_NAME}.csv\")\n",
    "\n",
    "intersect_idx = np.intersect1d(submission_df[\"abstract\"].str.lower(), train_df[\"abstract\"].str.lower(), return_indices=True)\n",
    "\n",
    "submission_df.loc[intersect_idx[1], 'title'] = train_df.loc[intersect_idx[2], 'title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.create_submission import generate_csv\n",
    "\n",
    "submission_df.to_csv(f\"./submission/predicted_titles_{SUBMISSION_NAME}_fake.csv\", index=False)\n",
    "\n",
    "generate_csv(input_file=f\"./submission/predicted_titles_{SUBMISSION_NAME}_fake.csv\", \n",
    "             output_file=f'./submission/submission_{SUBMISSION_NAME}_fake.csv', \n",
    "             voc_file=f'./datasets/vocs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./submission/submission_GPT-base-tune_fake.csv'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'./submission/submission_{SUBMISSION_NAME}_fake.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5-small:\n",
    "- **Score: 0.26174** 1% tuning\n",
    "- **Score: 0.34497** tuning 3 эпохи\n",
    "- **Score: 0.51810** + добавление правильных меток из трейна\n",
    "\n",
    "T5-base:\n",
    "- **Score: 0.20510** w/o tuning\n",
    "- для обучения с имеющейся длиной последовательности не хватает памяти GPU\n",
    "\n",
    "BART-base\n",
    "- **Score: 0.33851** 1% tuning\n",
    "- **Score: 0.39536** tuning 1,5 эпохи\n",
    "- **Score: 0.54804** + добавление правильных меток из трейна\n",
    "- **Score: 0.56782** + 2 эпохи с накопление градиента (вот и в топ-10)\n",
    "- ... дальше не интересно\n",
    "\n",
    "GPT-2\n",
    "- **Score:**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d00ba0b92f737b23b3e678e3a3ceb3fe4e948ad1ab95d9c6fdcbb4b4ec65f3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
