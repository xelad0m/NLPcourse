{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Даже с батчем размера 1 не влезает...\n",
    "- 391 321 600 параметров\n",
    "\n",
    "<img src=\"./img/prophet.jpg\" width=\"300\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- из данных убраны дубли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = f\"./datasets/train_clean.csv\"\n",
    "SMALL_CSV = f\"./cache/train.csv\"\n",
    "SCORING_CSV = f\"./datasets/test.csv\"\n",
    "\n",
    "USE_SMALL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасеты\n",
    "\n",
    "- пакет huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-04a3546adcf83ebb\n",
      "Reusing dataset csv (/home/user1/.cache/huggingface/datasets/csv/default-04a3546adcf83ebb/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "arxiv_dataset = datasets.Dataset.from_csv(SMALL_CSV if USE_SMALL else TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2 if USE_SMALL else 0.02\n",
    "arxiv_dataset = arxiv_dataset.train_test_split(test_size=test_size)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200, dict_keys(['abstract', 'title']))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arxiv_dataset[\"train\"]), len(arxiv_dataset[\"test\"]), arxiv_dataset[\"train\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2e0a9ad90b647d2d\n",
      "Reusing dataset csv (/home/user1/.cache/huggingface/datasets/csv/default-2e0a9ad90b647d2d/0.0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, dict_keys(['abstract']))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_dataset = datasets.Dataset.from_csv(SCORING_CSV)\n",
    "len(scoring_dataset), scoring_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/envs/py310/lib64/python3/site-packages/transformers/configuration_utils.py:358: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/prophetnet-large-uncased\")   # 0.2 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\" Длины последовательностей max = 1096 / 103. Берем какбе 3 сигмы:\n",
    "        - max_length=457 + 3\n",
    "        - max_length=34 + 2\n",
    "    \"\"\"\n",
    "\n",
    "    srcs = [prefix + doc for doc in examples[\"abstract\"]]\n",
    "    model_inputs = tokenizer(srcs, max_length=460, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        trgs = tokenizer(examples[\"title\"], max_length=36, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = trgs[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.50s/ba]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.43ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_arxiv = arxiv_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['abstract', 'title', 'input_ids', 'attention_mask', 'labels']),\n",
       " \"exploiting relationships between visual regions and question words have achieved great success in learning multi-modality features for visual question answering (vqa). however, we argue that existing methods mostly model relations between individual visual regions and words, which are not enough to correctly answer the question. from humans' perspective, answering a visual question requires understanding the summarizations of visual and language information. in this paper, we proposed the multi-modality latent interaction module (mli) to tackle this problem. the proposed module learns the cross-modality relationships between latent visual and language summarizations, which summarize visual regions and question into a small number of latent representations to avoid modeling uninformative individual region-word relations. the cross-modality information between the latent summarizations are propagated to fuse valuable information from both modalities and are used to update the visual and word features. such mli modules can be stacked for several stages to model complex and latent relations between the two modalities and achieves highly competitive performance on public vqa benchmarks, vqa v2.0 and tdiuc . in addition, we show that the performance of our methods could be significantly improved by combining with pre-trained language model bert.\",\n",
       " 'multi-modality latent interaction network for visual question answering')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_arxiv[\"train\"][0].keys(), tokenized_arxiv[\"train\"][0][\"abstract\"], tokenized_arxiv[\"train\"][0][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/envs/py310/lib64/python3/site-packages/transformers/configuration_utils.py:358: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/prophetnet-large-uncased\") # 1460MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+-----+--------------+------------+---------------+-----------+\n",
      "|                            Modules/Tensors                            | GPU |    Shape     | Parameters |      Type     |  DataMem  |\n",
      "+-----------------------------------------------------------------------+-----+--------------+------------+---------------+-----------+\n",
      "|                   prophetnet.word_embeddings.weight                   |     | 30522 x 1024 |  31254528  | torch.float32 | 125018112 |\n",
      "|             prophetnet.encoder.position_embeddings.weight             |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|            prophetnet.encoder.embeddings_layer_norm.weight            |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|             prophetnet.encoder.embeddings_layer_norm.bias             |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.0.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.0.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.0.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.0.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.0.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.0.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.0.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.0.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.0.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.0.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.encoder.layers.0.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.encoder.layers.0.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.encoder.layers.0.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.encoder.layers.0.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.encoder.layers.0.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.0.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.1.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.1.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.1.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.1.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.1.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.1.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.1.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.1.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.1.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.1.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.encoder.layers.1.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.encoder.layers.1.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.encoder.layers.1.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.encoder.layers.1.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.encoder.layers.1.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.1.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.2.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.2.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.2.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.2.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.2.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.2.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.2.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.2.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.2.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.2.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.encoder.layers.2.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.encoder.layers.2.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.encoder.layers.2.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.encoder.layers.2.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.encoder.layers.2.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.2.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.3.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.3.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.3.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.3.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.3.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.3.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.3.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.3.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.3.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.3.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.encoder.layers.3.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.encoder.layers.3.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.encoder.layers.3.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.encoder.layers.3.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.encoder.layers.3.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.3.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.4.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.4.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.4.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.4.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.4.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.4.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.4.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.4.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.4.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.4.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.encoder.layers.4.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.encoder.layers.4.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.encoder.layers.4.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.encoder.layers.4.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.encoder.layers.4.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.4.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.5.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.5.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.5.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.5.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.5.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.5.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.5.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.5.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.5.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.5.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.encoder.layers.5.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.encoder.layers.5.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.encoder.layers.5.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.encoder.layers.5.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.encoder.layers.5.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.5.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.6.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.6.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.6.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.6.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.6.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.6.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.6.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.6.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.6.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.6.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.encoder.layers.6.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.encoder.layers.6.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.encoder.layers.6.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.encoder.layers.6.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.encoder.layers.6.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.6.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.7.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.7.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.7.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.7.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.7.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.7.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.7.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.7.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.7.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.7.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.encoder.layers.7.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.encoder.layers.7.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.encoder.layers.7.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.encoder.layers.7.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.encoder.layers.7.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.7.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.8.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.8.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.8.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.8.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.8.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.8.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.8.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.8.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.8.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.8.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.encoder.layers.8.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.encoder.layers.8.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.encoder.layers.8.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.encoder.layers.8.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.encoder.layers.8.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.8.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.9.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.9.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.9.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.9.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.9.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.9.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.9.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.9.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.9.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.9.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.encoder.layers.9.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.encoder.layers.9.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.encoder.layers.9.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.encoder.layers.9.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.encoder.layers.9.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.9.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.10.self_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.10.self_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.10.self_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.10.self_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.10.self_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.10.self_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.10.self_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.10.self_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.10.self_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.10.self_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|     prophetnet.encoder.layers.10.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|      prophetnet.encoder.layers.10.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|        prophetnet.encoder.layers.10.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|         prophetnet.encoder.layers.10.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.encoder.layers.10.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.encoder.layers.10.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.11.self_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.11.self_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.11.self_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.11.self_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.11.self_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.encoder.layers.11.self_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.11.self_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.encoder.layers.11.self_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.encoder.layers.11.self_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.encoder.layers.11.self_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|     prophetnet.encoder.layers.11.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|      prophetnet.encoder.layers.11.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|        prophetnet.encoder.layers.11.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|         prophetnet.encoder.layers.11.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.encoder.layers.11.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.encoder.layers.11.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|             prophetnet.decoder.position_embeddings.weight             |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|               prophetnet.decoder.ngram_embeddings.weight              |     |   2 x 1024   |    2048    | torch.float32 |    8192   |\n",
      "|         prophetnet.decoder.layers.0.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.0.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.0.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.0.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.0.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.0.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.0.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.0.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|  prophetnet.decoder.layers.0.self_attn.relative_pos_embeddings.weight |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|   prophetnet.decoder.layers.0.self_attn.relative_pos_embeddings.bias  |     |     512      |    512     | torch.float32 |    2048   |\n",
      "|        prophetnet.decoder.layers.0.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.0.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.0.cross_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.0.cross_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.0.cross_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.0.cross_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.0.cross_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.0.cross_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.0.cross_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.0.cross_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.0.cross_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.0.cross_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.decoder.layers.0.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.decoder.layers.0.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.decoder.layers.0.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.decoder.layers.0.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.0.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.0.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.1.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.1.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.1.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.1.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.1.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.1.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.1.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.1.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|  prophetnet.decoder.layers.1.self_attn.relative_pos_embeddings.weight |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|   prophetnet.decoder.layers.1.self_attn.relative_pos_embeddings.bias  |     |     512      |    512     | torch.float32 |    2048   |\n",
      "|        prophetnet.decoder.layers.1.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.1.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.1.cross_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.1.cross_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.1.cross_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.1.cross_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.1.cross_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.1.cross_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.1.cross_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.1.cross_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.1.cross_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.1.cross_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.decoder.layers.1.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.decoder.layers.1.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.decoder.layers.1.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.decoder.layers.1.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.1.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.1.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.2.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.2.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.2.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.2.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.2.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.2.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.2.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.2.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|  prophetnet.decoder.layers.2.self_attn.relative_pos_embeddings.weight |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|   prophetnet.decoder.layers.2.self_attn.relative_pos_embeddings.bias  |     |     512      |    512     | torch.float32 |    2048   |\n",
      "|        prophetnet.decoder.layers.2.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.2.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.2.cross_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.2.cross_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.2.cross_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.2.cross_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.2.cross_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.2.cross_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.2.cross_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.2.cross_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.2.cross_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.2.cross_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.decoder.layers.2.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.decoder.layers.2.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.decoder.layers.2.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.decoder.layers.2.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.2.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.2.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.3.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.3.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.3.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.3.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.3.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.3.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.3.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.3.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|  prophetnet.decoder.layers.3.self_attn.relative_pos_embeddings.weight |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|   prophetnet.decoder.layers.3.self_attn.relative_pos_embeddings.bias  |     |     512      |    512     | torch.float32 |    2048   |\n",
      "|        prophetnet.decoder.layers.3.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.3.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.3.cross_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.3.cross_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.3.cross_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.3.cross_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.3.cross_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.3.cross_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.3.cross_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.3.cross_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.3.cross_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.3.cross_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.decoder.layers.3.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.decoder.layers.3.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.decoder.layers.3.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.decoder.layers.3.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.3.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.3.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.4.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.4.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.4.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.4.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.4.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.4.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.4.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.4.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|  prophetnet.decoder.layers.4.self_attn.relative_pos_embeddings.weight |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|   prophetnet.decoder.layers.4.self_attn.relative_pos_embeddings.bias  |     |     512      |    512     | torch.float32 |    2048   |\n",
      "|        prophetnet.decoder.layers.4.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.4.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.4.cross_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.4.cross_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.4.cross_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.4.cross_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.4.cross_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.4.cross_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.4.cross_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.4.cross_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.4.cross_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.4.cross_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.decoder.layers.4.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.decoder.layers.4.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.decoder.layers.4.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.decoder.layers.4.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.4.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.4.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.5.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.5.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.5.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.5.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.5.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.5.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.5.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.5.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|  prophetnet.decoder.layers.5.self_attn.relative_pos_embeddings.weight |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|   prophetnet.decoder.layers.5.self_attn.relative_pos_embeddings.bias  |     |     512      |    512     | torch.float32 |    2048   |\n",
      "|        prophetnet.decoder.layers.5.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.5.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.5.cross_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.5.cross_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.5.cross_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.5.cross_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.5.cross_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.5.cross_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.5.cross_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.5.cross_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.5.cross_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.5.cross_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.decoder.layers.5.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.decoder.layers.5.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.decoder.layers.5.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.decoder.layers.5.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.5.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.5.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.6.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.6.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.6.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.6.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.6.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.6.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.6.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.6.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|  prophetnet.decoder.layers.6.self_attn.relative_pos_embeddings.weight |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|   prophetnet.decoder.layers.6.self_attn.relative_pos_embeddings.bias  |     |     512      |    512     | torch.float32 |    2048   |\n",
      "|        prophetnet.decoder.layers.6.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.6.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.6.cross_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.6.cross_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.6.cross_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.6.cross_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.6.cross_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.6.cross_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.6.cross_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.6.cross_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.6.cross_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.6.cross_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.decoder.layers.6.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.decoder.layers.6.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.decoder.layers.6.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.decoder.layers.6.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.6.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.6.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.7.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.7.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.7.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.7.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.7.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.7.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.7.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.7.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|  prophetnet.decoder.layers.7.self_attn.relative_pos_embeddings.weight |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|   prophetnet.decoder.layers.7.self_attn.relative_pos_embeddings.bias  |     |     512      |    512     | torch.float32 |    2048   |\n",
      "|        prophetnet.decoder.layers.7.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.7.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.7.cross_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.7.cross_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.7.cross_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.7.cross_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.7.cross_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.7.cross_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.7.cross_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.7.cross_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.7.cross_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.7.cross_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.decoder.layers.7.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.decoder.layers.7.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.decoder.layers.7.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.decoder.layers.7.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.7.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.7.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.8.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.8.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.8.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.8.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.8.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.8.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.8.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.8.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|  prophetnet.decoder.layers.8.self_attn.relative_pos_embeddings.weight |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|   prophetnet.decoder.layers.8.self_attn.relative_pos_embeddings.bias  |     |     512      |    512     | torch.float32 |    2048   |\n",
      "|        prophetnet.decoder.layers.8.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.8.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.8.cross_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.8.cross_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.8.cross_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.8.cross_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.8.cross_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.8.cross_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.8.cross_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.8.cross_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.8.cross_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.8.cross_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.decoder.layers.8.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.decoder.layers.8.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.decoder.layers.8.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.decoder.layers.8.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.8.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.8.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.9.self_attn.key_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.9.self_attn.key_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.9.self_attn.value_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.9.self_attn.value_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.9.self_attn.query_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.9.self_attn.query_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.9.self_attn.out_proj.weight         |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.9.self_attn.out_proj.bias          |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|  prophetnet.decoder.layers.9.self_attn.relative_pos_embeddings.weight |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|   prophetnet.decoder.layers.9.self_attn.relative_pos_embeddings.bias  |     |     512      |    512     | torch.float32 |    2048   |\n",
      "|        prophetnet.decoder.layers.9.self_attn_layer_norm.weight        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.9.self_attn_layer_norm.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.9.cross_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.9.cross_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.9.cross_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.9.cross_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.9.cross_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.9.cross_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.9.cross_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.9.cross_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.9.cross_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.9.cross_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.decoder.layers.9.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|       prophetnet.decoder.layers.9.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|         prophetnet.decoder.layers.9.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|          prophetnet.decoder.layers.9.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.9.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.9.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.10.self_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.10.self_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.10.self_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.10.self_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.10.self_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.10.self_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.10.self_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.10.self_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "| prophetnet.decoder.layers.10.self_attn.relative_pos_embeddings.weight |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|  prophetnet.decoder.layers.10.self_attn.relative_pos_embeddings.bias  |     |     512      |    512     | torch.float32 |    2048   |\n",
      "|        prophetnet.decoder.layers.10.self_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.10.self_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.10.cross_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.10.cross_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.10.cross_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|        prophetnet.decoder.layers.10.cross_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.10.cross_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|        prophetnet.decoder.layers.10.cross_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.10.cross_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.10.cross_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.10.cross_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.10.cross_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|     prophetnet.decoder.layers.10.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|      prophetnet.decoder.layers.10.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|        prophetnet.decoder.layers.10.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|         prophetnet.decoder.layers.10.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.decoder.layers.10.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.10.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.11.self_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.11.self_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.11.self_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.11.self_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.11.self_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.11.self_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.11.self_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|          prophetnet.decoder.layers.11.self_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "| prophetnet.decoder.layers.11.self_attn.relative_pos_embeddings.weight |     |  512 x 1024  |   524288   | torch.float32 |  2097152  |\n",
      "|  prophetnet.decoder.layers.11.self_attn.relative_pos_embeddings.bias  |     |     512      |    512     | torch.float32 |    2048   |\n",
      "|        prophetnet.decoder.layers.11.self_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|         prophetnet.decoder.layers.11.self_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.11.cross_attn.key_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.11.cross_attn.key_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.11.cross_attn.value_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|        prophetnet.decoder.layers.11.cross_attn.value_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.11.cross_attn.query_proj.weight       |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|        prophetnet.decoder.layers.11.cross_attn.query_proj.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.11.cross_attn.out_proj.weight        |     | 1024 x 1024  |  1048576   | torch.float32 |  4194304  |\n",
      "|         prophetnet.decoder.layers.11.cross_attn.out_proj.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.11.cross_attn_layer_norm.weight       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|        prophetnet.decoder.layers.11.cross_attn_layer_norm.bias        |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|     prophetnet.decoder.layers.11.feed_forward.intermediate.weight     |     | 4096 x 1024  |  4194304   | torch.float32 |  16777216 |\n",
      "|      prophetnet.decoder.layers.11.feed_forward.intermediate.bias      |     |     4096     |    4096    | torch.float32 |   16384   |\n",
      "|        prophetnet.decoder.layers.11.feed_forward.output.weight        |     | 1024 x 4096  |  4194304   | torch.float32 |  16777216 |\n",
      "|         prophetnet.decoder.layers.11.feed_forward.output.bias         |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|      prophetnet.decoder.layers.11.feed_forward_layer_norm.weight      |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|       prophetnet.decoder.layers.11.feed_forward_layer_norm.bias       |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|            prophetnet.decoder.embeddings_layer_norm.weight            |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "|             prophetnet.decoder.embeddings_layer_norm.bias             |     |     1024     |    1024    | torch.float32 |    4096   |\n",
      "+-----------------------------------------------------------------------+-----+--------------+------------+---------------+-----------+\n",
      "Total Trainable Params: 391321600\n",
      "Total memory (min): 1,528,600.00 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "391321600"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers.utils import count_parameters\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/user1/projects/stepik.org/DataScience/Нейронные сети и обработка текста/stepik-dl-nlp/task11_kaggle/9. Prophet_fine-tune.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user1/projects/stepik.org/DataScience/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%20%D0%B8%20%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0/stepik-dl-nlp/task11_kaggle/9.%20Prophet_fine-tune.ipynb#ch0000017?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.to(device)    # if no train else trainer sends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./Prophet-large-results\",\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    weight_decay=0.01,\n",
    "    # logging_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3*16,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_arxiv[\"train\"],\n",
    "    eval_dataset=tokenized_arxiv[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm._instances.clear()\n",
    "\n",
    "# trainer.train()#(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(example):\n",
    "    input_ids = tokenizer(prefix + example[\"abstract\"], \n",
    "                        max_length=460, \n",
    "                        truncation=True, \n",
    "                        return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "    outputs = model.generate(input_ids.to(device))\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "arxiv_dataset[\"test\"][n][\"abstract\"], arxiv_dataset[\"test\"][n][\"title\"], generate(arxiv_dataset[\"test\"][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 42\n",
    "arxiv_dataset[\"test\"][n][\"abstract\"], arxiv_dataset[\"test\"][n][\"title\"], generate(arxiv_dataset[\"test\"][n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLEU-score\n",
    "\n",
    "Самоделки:\n",
    "- 0.02457 (словарь 6152, по 5 эпох по 5r-4, 1e-3, min.val.loss = 3.875) \n",
    "- **0.19204** (словарь 60 тыс. ~15 эпох с шагом 5e-4 -> 5e-5, min.val.loss = 2.289)\n",
    "- 0.12601 (словарь 84 тыс. много разных эпох, сходится плохо, min.val.loss = 3.305)\n",
    "- 0.10644 (BPE, словарь 16 тыс., много разных эпох, сходится плохо, min.val.loss = 3.8)\n",
    "\n",
    "T5-small\n",
    "- BLEU-score: **0.044...** 1% тюнинг\n",
    "- BLEU-score: **0.16563** (3 эпохи - 2,5 часа RTX2060 6Gb)\n",
    "\n",
    "T5-base\n",
    "- BLEU-score: **0.07422** (без обучения)\n",
    "- обучение не тянет...\n",
    "\n",
    "BART-base\n",
    "- BLEU-score: **0.17743** 1% тюнинг\n",
    "- BLEU-score: **0.17984** (1.43 эпохи - 2,5 часа RTX2060 6Gb)\n",
    "- BLEU-score: **0.19266** (2 эпохи)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "tqdm._instances.clear()\n",
    "\n",
    "candidates = []\n",
    "references = []\n",
    "for example in tqdm(tokenized_arxiv[\"test\"]):\n",
    "    candidates.append(generate(example).split())\n",
    "    references.append([example[\"title\"].split()])\n",
    "\n",
    "score = bleu_score(candidates, references, max_n=3, weights=[1/3]*3)\n",
    "\n",
    "print('BLEU-score: {0:.5f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepik score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NAME = \"Prophet-large\" if USE_SMALL else \"Prophet-large-tune\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация заголовков для тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm._instances.clear()\n",
    "\n",
    "abstracts = []\n",
    "titles = []\n",
    "\n",
    "for example in tqdm(scoring_dataset):\n",
    "    abstracts.append(example[\"abstract\"])\n",
    "    titles.append(generate(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось, например"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts[1], titles[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записываем полученные заголовки в файл формата `<abstract>,<title>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission_df = pd.DataFrame({'abstract': abstracts, 'title': titles})\n",
    "submission_df.to_csv(f\"./submission/predicted_titles_{SUBMISSION_NAME}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"title\"].apply(lambda x: len(str(x).split())).describe()[[\"mean\",\"std\", \"max\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью скрипта `generate_csv` приводим файл `submission_prediction.csv` в формат, необходимый для отправки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.create_submission import generate_csv\n",
    "\n",
    "generate_csv(input_file=f\"./submission/predicted_titles_{SUBMISSION_NAME}.csv\", \n",
    "             output_file=f'./submission/submission_{SUBMISSION_NAME}.csv', \n",
    "             voc_file=f'./datasets/vocs.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# С учетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv(\"./datasets/train.csv\")\n",
    "submission_df = pd.read_csv(f\"./submission/predicted_titles_{SUBMISSION_NAME}.csv\")\n",
    "\n",
    "intersect_idx = np.intersect1d(submission_df[\"abstract\"].str.lower(), train_df[\"abstract\"].str.lower(), return_indices=True)\n",
    "\n",
    "submission_df.loc[intersect_idx[1], 'title'] = train_df.loc[intersect_idx[2], 'title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.create_submission import generate_csv\n",
    "\n",
    "submission_df.to_csv(f\"./submission/predicted_titles_{SUBMISSION_NAME}_fake.csv\", index=False)\n",
    "\n",
    "generate_csv(input_file=f\"./submission/predicted_titles_{SUBMISSION_NAME}_fake.csv\", \n",
    "             output_file=f'./submission/submission_{SUBMISSION_NAME}_fake.csv', \n",
    "             voc_file=f'./datasets/vocs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'./submission/submission_{SUBMISSION_NAME}_fake.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5-small:\n",
    "- **Score: 0.26174** 1% tuning\n",
    "- **Score: 0.34497** tuning 3 эпохи\n",
    "- **Score: 0.51810** + добавление правильных меток из трейна\n",
    "\n",
    "T5-base:\n",
    "- **Score: 0.20510** w/o tuning\n",
    "- для обучения с имеющейся длиной последовательности не хватает памяти GPU\n",
    "\n",
    "BART-base\n",
    "- **Score: 0.33851** 1% tuning\n",
    "- **Score: 0.39536** tuning 1,5 эпохи\n",
    "- **Score: 0.54804** + добавление правильных меток из трейна\n",
    "- **Score: 0.56782** 2 эпохи с накопление градиента (вот и в топ-10)\n",
    "- ... дальше не интересно"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53d00ba0b92f737b23b3e678e3a3ceb3fe4e948ad1ab95d9c6fdcbb4b4ec65f3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
